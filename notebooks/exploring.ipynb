{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training one period's MLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100585 articles\n"
     ]
    }
   ],
   "source": [
    "with open('../input/articles/2015_articles.txt', 'r') as f:\n",
    "    articles = f.readlines()\n",
    "    print(f'Found {len(articles)} articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"''american success,'' which like ''winter kills'' is a supremely playful and visually witty movie, finds the same handsome stars - jeff bridges and belinda bauer - in even more farfetched circumstances. in ''winter kills,'' mr. bridges played the heir to a wicked old entrepreneur's colossal fortune, and miss bauer played a spy and temptress with a part-time job at a women's magazine. this time, they're cast as a married couple, harry and sarah flowers, who have problems. harry's problem is sarah, and sarah's problem is that she spends her days dressed as a ballerina with butterfly wings. she is meant to be a fairy-tale figure, an enchanted princess who can't possibly meet the demands of everyday life. when harry interrupts her reveries to try to make love to her, sarah complains bitterly that he's breaking their conjugal rules. he can't try this. it isn't friday. the story follows harry's efforts to transform himself into sarah's idea of a real man, and to rob her financier father in the process. harry changes his identity, rents a red sports car with black-andwhite checked seats and is taught the mysteries of sex by a backstreet prostitute played by bianca jagger, who sports more diamonds than any backstreet prostitute might even dream of. mrs. jagger, who brings a don't-rumple-my-hair demeanor to her scenes of seductive abandon, is only one of the many anomalies that keep the film perversely interesting. another is the disheveled detective, played by steven keats, who's so down at the heels that the only way he can follow a man is by hopping into the front seat of his prey's taxi. he's too broke to afford a taxi of his own. mr. richert, who wrote the screenplay with larry cohen, doesn't have much of a knack for presenting the authentically satirical aspects of his story in a manner that might allow them to be taken seriously. his film is much too facetious for that, and when its whimsy isn't working, there's not much else going on. however, though it periodically wears thin, ''american success'' is a buoyant and enterprising movie more often than not, and what it lacks in coherence it makes up in dash. mr. richert may well make a wonderfully funny and stylish movie some day. on the other hand, even if he continues to produce relatively half-baked efforts like this one, they're sure to be energetic and merry. on the bill with ''american success'' - the title echoes the name of the well-known credit-card company mr. beatty is supposed to head - is ''couples and robbers,'' a short written and directed by clare peploe. this is a brief and ostensibly fanciful look at two newlyweds who launch their marriage by committing an ingenious robbery and by composing a very long list of the appliances and amenities they consider essential to their future happiness.              ''couples and robbers,'' though proficiently executed, is somewhat flat, particularly when juxtaposed with mr. richert's far more irreverent view of consumer culture and the dreams it engenders. janet maslin                                       only on fridays                                       american success, directed by william ri- chert; screenplay by mr. richert and larry cohen; story by mr. cohen; director of photography, anthony richmond; film editor, ralph e. winters; music by maurice jarre; produced by daniel h. blatt and edgar j. scherick; released by columbia pictures; presented by joseph papp in association with the fdm foundation for the arts. at the public theater, 425 lafayette street. running time: 88 minutes. this film is rated pg.                                       harry . . . . . jeff bridges                          sarah . . . . . belinda bauer                          mr. elliot . . . . . ned beatty                          rick duprez . . . . . steven keats                          corinne . . . . . bianca jagger                          ernst . . . . . john glover                          greta . . . . . mascha gonska                          herman . . . . . michael durrell                          mrs. heinemann . . . . . eva-maria meineke                          maire d'hotel . . . . . gunter meisner                          gunter . . . . . david brooks                          landlady . . . . . marie bardischewski                          young magician . . . . . sebastian baur                          amucco guard . . . . . peer brensing                          red-headed lady friend . . . . . judy brown                          rental agent . . . . . michael burger                          charles . . . . . andrew burleigh                          herman's secretary . . . . . claudia butenuth                          lichtenstein . . . . . peter capell\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7735"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(' office ' in article for article in articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5486"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(' head ' in article for article in articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2375"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(' success ' in article for article in articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(articles: list, key_words: list = [' office ', ' head ', ' success ']):\n",
    "    train = []\n",
    "    test = []\n",
    "\n",
    "    target_articles = []\n",
    "    other_articles = []\n",
    "\n",
    "    for article in articles:\n",
    "        if any(key_word in article for key_word in key_words):\n",
    "            target_articles.append(article)\n",
    "        else:\n",
    "            other_articles.append(article)\n",
    "\n",
    "    train.extend(target_articles[:int(len(target_articles) * 0.8)])\n",
    "    test.extend(target_articles[int(len(target_articles) * 0.8):])\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 11201 articles\n",
      "Test: 2801 articles\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test(articles)\n",
    "print(f'Train: {len(train)} articles')\n",
    "print(f'Test: {len(test)} articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/data_2015/train.txt', 'w') as f:\n",
    "    f.writelines(train)\n",
    "\n",
    "with open('../input/data_2015/test.txt', 'w') as f:\n",
    "    f.writelines(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_ENABLE_MPS_FALLBACK=1\n"
     ]
    }
   ],
   "source": [
    "%env PYTORCH_ENABLE_MPS_FALLBACK=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/data_2015 to /Users/imenekolli/.cache/huggingface/datasets/text/data_2015-5fda990804bfed91/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f155d26a22c2431b90493e008d3ca3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424ad65097b94af399bc7652e5d50d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30bf94c55a744e8bff6af6c27aefc80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c36927d71e245aaae37970ab056e8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /Users/imenekolli/.cache/huggingface/datasets/text/data_2015-5fda990804bfed91/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218f8a0a291a48b98f4c1d741644f20f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 11201\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2801\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = {\"train\": \"train.txt\", \"test\": \"test.txt\"}\n",
    "data = load_dataset('../input/data_2015', data_files=data_files)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"''american success,'' which like ''winter kills'' is a supremely playful and visually witty movie, finds the same handsome stars - jeff bridges and belinda bauer - in even more farfetched circumstances. in ''winter kills,'' mr. bridges played the heir to a wicked old entrepreneur's colossal fortune, and miss bauer played a spy and temptress with a part-time job at a women's magazine. this time, they're cast as a married couple, harry and sarah flowers, who have problems. harry's problem is sarah, and sarah's problem is that she spends her days dressed as a ballerina with butterfly wings. she is meant to be a fairy-tale figure, an enchanted princess who can't possibly meet the demands of everyday life. when harry interrupts her reveries to try to make love to her, sarah complains bitterly that he's breaking their conjugal rules. he can't try this. it isn't friday. the story follows harry's efforts to transform himself into sarah's idea of a real man, and to rob her financier father in the process. harry changes his identity, rents a red sports car with black-andwhite checked seats and is taught the mysteries of sex by a backstreet prostitute played by bianca jagger, who sports more diamonds than any backstreet prostitute might even dream of. mrs. jagger, who brings a don't-rumple-my-hair demeanor to her scenes of seductive abandon, is only one of the many anomalies that keep the film perversely interesting. another is the disheveled detective, played by steven keats, who's so down at the heels that the only way he can follow a man is by hopping into the front seat of his prey's taxi. he's too broke to afford a taxi of his own. mr. richert, who wrote the screenplay with larry cohen, doesn't have much of a knack for presenting the authentically satirical aspects of his story in a manner that might allow them to be taken seriously. his film is much too facetious for that, and when its whimsy isn't working, there's not much else going on. however, though it periodically wears thin, ''american success'' is a buoyant and enterprising movie more often than not, and what it lacks in coherence it makes up in dash. mr. richert may well make a wonderfully funny and stylish movie some day. on the other hand, even if he continues to produce relatively half-baked efforts like this one, they're sure to be energetic and merry. on the bill with ''american success'' - the title echoes the name of the well-known credit-card company mr. beatty is supposed to head - is ''couples and robbers,'' a short written and directed by clare peploe. this is a brief and ostensibly fanciful look at two newlyweds who launch their marriage by committing an ingenious robbery and by composing a very long list of the appliances and amenities they consider essential to their future happiness.              ''couples and robbers,'' though proficiently executed, is somewhat flat, particularly when juxtaposed with mr. richert's far more irreverent view of consumer culture and the dreams it engenders. janet maslin                                       only on fridays                                       american success, directed by william ri- chert; screenplay by mr. richert and larry cohen; story by mr. cohen; director of photography, anthony richmond; film editor, ralph e. winters; music by maurice jarre; produced by daniel h. blatt and edgar j. scherick; released by columbia pictures; presented by joseph papp in association with the fdm foundation for the arts. at the public theater, 425 lafayette street. running time: 88 minutes. this film is rated pg.                                       harry . . . . . jeff bridges                          sarah . . . . . belinda bauer                          mr. elliot . . . . . ned beatty                          rick duprez . . . . . steven keats                          corinne . . . . . bianca jagger                          ernst . . . . . john glover                          greta . . . . . mascha gonska                          herman . . . . . michael durrell                          mrs. heinemann . . . . . eva-maria meineke                          maire d'hotel . . . . . gunter meisner                          gunter . . . . . david brooks                          landlady . . . . . marie bardischewski                          young magician . . . . . sebastian baur                          amucco guard . . . . . peer brensing                          red-headed lady friend . . . . . judy brown                          rental agent . . . . . michael burger                          charles . . . . . andrew burleigh                          herman's secretary . . . . . claudia butenuth                          lichtenstein . . . . . peter capell\"}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 512  # Maximum sequence length allowed by the model\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Split the input sequences into smaller parts if they exceed the maximum length\n",
    "    tokenized_examples = tokenizer(examples[\"text\"], truncation=True, max_length=max_sequence_length)\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f6a983fa7a434a9e9d0cc55eded336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/11201 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b406f1a303044cbfb90a2df6666b4f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2801 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = data.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=[\"text\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0,\n",
       "  17809,\n",
       "  8015,\n",
       "  12657,\n",
       "  1282,\n",
       "  10559,\n",
       "  61,\n",
       "  101,\n",
       "  12801,\n",
       "  31421,\n",
       "  10469,\n",
       "  17809,\n",
       "  16,\n",
       "  10,\n",
       "  15835,\n",
       "  352,\n",
       "  23317,\n",
       "  8,\n",
       "  21545,\n",
       "  33228,\n",
       "  1569,\n",
       "  6,\n",
       "  5684,\n",
       "  5,\n",
       "  276,\n",
       "  19222,\n",
       "  2690,\n",
       "  111,\n",
       "  4112,\n",
       "  3145,\n",
       "  11879,\n",
       "  8,\n",
       "  12138,\n",
       "  8865,\n",
       "  741,\n",
       "  9994,\n",
       "  111,\n",
       "  11,\n",
       "  190,\n",
       "  55,\n",
       "  444,\n",
       "  37606,\n",
       "  4215,\n",
       "  4,\n",
       "  11,\n",
       "  12801,\n",
       "  31421,\n",
       "  10469,\n",
       "  10559,\n",
       "  475,\n",
       "  338,\n",
       "  4,\n",
       "  11879,\n",
       "  702,\n",
       "  5,\n",
       "  24482,\n",
       "  7,\n",
       "  10,\n",
       "  28418,\n",
       "  793,\n",
       "  11777,\n",
       "  18,\n",
       "  33568,\n",
       "  13016,\n",
       "  6,\n",
       "  8,\n",
       "  2649,\n",
       "  741,\n",
       "  9994,\n",
       "  702,\n",
       "  10,\n",
       "  10258,\n",
       "  8,\n",
       "  34919,\n",
       "  5224,\n",
       "  19,\n",
       "  10,\n",
       "  233,\n",
       "  12,\n",
       "  958,\n",
       "  633,\n",
       "  23,\n",
       "  10,\n",
       "  390,\n",
       "  18,\n",
       "  4320,\n",
       "  4,\n",
       "  42,\n",
       "  86,\n",
       "  6,\n",
       "  51,\n",
       "  214,\n",
       "  2471,\n",
       "  25,\n",
       "  10,\n",
       "  2997,\n",
       "  891,\n",
       "  6,\n",
       "  12280,\n",
       "  1506,\n",
       "  8,\n",
       "  579,\n",
       "  36000,\n",
       "  7716,\n",
       "  6,\n",
       "  54,\n",
       "  33,\n",
       "  1272,\n",
       "  4,\n",
       "  12280,\n",
       "  1506,\n",
       "  18,\n",
       "  936,\n",
       "  16,\n",
       "  579,\n",
       "  36000,\n",
       "  6,\n",
       "  8,\n",
       "  579,\n",
       "  36000,\n",
       "  18,\n",
       "  936,\n",
       "  16,\n",
       "  14,\n",
       "  79,\n",
       "  12500,\n",
       "  69,\n",
       "  360,\n",
       "  7001,\n",
       "  25,\n",
       "  10,\n",
       "  1011,\n",
       "  254,\n",
       "  1243,\n",
       "  19,\n",
       "  24317,\n",
       "  11954,\n",
       "  4,\n",
       "  79,\n",
       "  16,\n",
       "  2425,\n",
       "  7,\n",
       "  28,\n",
       "  10,\n",
       "  25310,\n",
       "  12,\n",
       "  31029,\n",
       "  1955,\n",
       "  6,\n",
       "  41,\n",
       "  44141,\n",
       "  19169,\n",
       "  54,\n",
       "  64,\n",
       "  75,\n",
       "  3544,\n",
       "  972,\n",
       "  5,\n",
       "  4501,\n",
       "  9,\n",
       "  7476,\n",
       "  301,\n",
       "  4,\n",
       "  77,\n",
       "  12280,\n",
       "  1506,\n",
       "  44388,\n",
       "  69,\n",
       "  41480,\n",
       "  918,\n",
       "  7,\n",
       "  860,\n",
       "  7,\n",
       "  146,\n",
       "  657,\n",
       "  7,\n",
       "  69,\n",
       "  6,\n",
       "  579,\n",
       "  36000,\n",
       "  37649,\n",
       "  31337,\n",
       "  14,\n",
       "  37,\n",
       "  18,\n",
       "  3433,\n",
       "  49,\n",
       "  21044,\n",
       "  39029,\n",
       "  1492,\n",
       "  4,\n",
       "  37,\n",
       "  64,\n",
       "  75,\n",
       "  860,\n",
       "  42,\n",
       "  4,\n",
       "  24,\n",
       "  965,\n",
       "  75,\n",
       "  6664,\n",
       "  21746,\n",
       "  4,\n",
       "  5,\n",
       "  527,\n",
       "  3905,\n",
       "  12280,\n",
       "  1506,\n",
       "  18,\n",
       "  1170,\n",
       "  7,\n",
       "  7891,\n",
       "  1003,\n",
       "  88,\n",
       "  579,\n",
       "  36000,\n",
       "  18,\n",
       "  1114,\n",
       "  9,\n",
       "  10,\n",
       "  588,\n",
       "  313,\n",
       "  6,\n",
       "  8,\n",
       "  7,\n",
       "  16785,\n",
       "  69,\n",
       "  8746,\n",
       "  3290,\n",
       "  906,\n",
       "  1150,\n",
       "  11,\n",
       "  5,\n",
       "  609,\n",
       "  4,\n",
       "  12280,\n",
       "  1506,\n",
       "  1022,\n",
       "  39,\n",
       "  3599,\n",
       "  6,\n",
       "  17845,\n",
       "  10,\n",
       "  1275,\n",
       "  1612,\n",
       "  512,\n",
       "  19,\n",
       "  909,\n",
       "  12,\n",
       "  463,\n",
       "  9830,\n",
       "  7869,\n",
       "  3202,\n",
       "  8,\n",
       "  16,\n",
       "  5850,\n",
       "  5,\n",
       "  30621,\n",
       "  9,\n",
       "  2099,\n",
       "  30,\n",
       "  10,\n",
       "  124,\n",
       "  20521,\n",
       "  36289,\n",
       "  702,\n",
       "  30,\n",
       "  741,\n",
       "  811,\n",
       "  3245,\n",
       "  1236,\n",
       "  17864,\n",
       "  6,\n",
       "  54,\n",
       "  1612,\n",
       "  55,\n",
       "  21019,\n",
       "  87,\n",
       "  143,\n",
       "  124,\n",
       "  20521,\n",
       "  36289,\n",
       "  429,\n",
       "  190,\n",
       "  3366,\n",
       "  9,\n",
       "  4,\n",
       "  475,\n",
       "  4926,\n",
       "  4,\n",
       "  1236,\n",
       "  17864,\n",
       "  6,\n",
       "  54,\n",
       "  3291,\n",
       "  10,\n",
       "  218,\n",
       "  75,\n",
       "  12,\n",
       "  338,\n",
       "  7198,\n",
       "  459,\n",
       "  12,\n",
       "  4783,\n",
       "  12,\n",
       "  34019,\n",
       "  31764,\n",
       "  7,\n",
       "  69,\n",
       "  5422,\n",
       "  9,\n",
       "  10195,\n",
       "  39433,\n",
       "  12506,\n",
       "  6,\n",
       "  16,\n",
       "  129,\n",
       "  65,\n",
       "  9,\n",
       "  5,\n",
       "  171,\n",
       "  36071,\n",
       "  14,\n",
       "  489,\n",
       "  5,\n",
       "  822,\n",
       "  228,\n",
       "  37794,\n",
       "  2679,\n",
       "  4,\n",
       "  277,\n",
       "  16,\n",
       "  5,\n",
       "  2982,\n",
       "  700,\n",
       "  5536,\n",
       "  196,\n",
       "  14104,\n",
       "  6,\n",
       "  702,\n",
       "  30,\n",
       "  11235,\n",
       "  2987,\n",
       "  7321,\n",
       "  2923,\n",
       "  6,\n",
       "  54,\n",
       "  18,\n",
       "  98,\n",
       "  159,\n",
       "  23,\n",
       "  5,\n",
       "  8872,\n",
       "  14,\n",
       "  5,\n",
       "  129,\n",
       "  169,\n",
       "  37,\n",
       "  64,\n",
       "  1407,\n",
       "  10,\n",
       "  313,\n",
       "  16,\n",
       "  30,\n",
       "  36390,\n",
       "  88,\n",
       "  5,\n",
       "  760,\n",
       "  2418,\n",
       "  9,\n",
       "  39,\n",
       "  18644,\n",
       "  18,\n",
       "  9955,\n",
       "  4,\n",
       "  37,\n",
       "  18,\n",
       "  350,\n",
       "  2263,\n",
       "  7,\n",
       "  4960,\n",
       "  10,\n",
       "  9955,\n",
       "  9,\n",
       "  39,\n",
       "  308,\n",
       "  4,\n",
       "  475,\n",
       "  338,\n",
       "  4,\n",
       "  24230,\n",
       "  90,\n",
       "  6,\n",
       "  54,\n",
       "  875,\n",
       "  5,\n",
       "  26735,\n",
       "  19,\n",
       "  784,\n",
       "  12795,\n",
       "  1029,\n",
       "  2457,\n",
       "  6,\n",
       "  630,\n",
       "  75,\n",
       "  33,\n",
       "  203,\n",
       "  9,\n",
       "  10,\n",
       "  31326,\n",
       "  13,\n",
       "  10864,\n",
       "  5,\n",
       "  36106,\n",
       "  3435,\n",
       "  33937,\n",
       "  5894,\n",
       "  9,\n",
       "  39,\n",
       "  527,\n",
       "  11,\n",
       "  10,\n",
       "  4737,\n",
       "  14,\n",
       "  429,\n",
       "  1157,\n",
       "  106,\n",
       "  7,\n",
       "  28,\n",
       "  551,\n",
       "  3640,\n",
       "  4,\n",
       "  39,\n",
       "  822,\n",
       "  16,\n",
       "  203,\n",
       "  350,\n",
       "  34407,\n",
       "  6514,\n",
       "  13,\n",
       "  14,\n",
       "  6,\n",
       "  8,\n",
       "  77,\n",
       "  63,\n",
       "  31754,\n",
       "  8628,\n",
       "  965,\n",
       "  75,\n",
       "  447,\n",
       "  6,\n",
       "  89,\n",
       "  18,\n",
       "  45,\n",
       "  203,\n",
       "  1493,\n",
       "  164,\n",
       "  15,\n",
       "  4,\n",
       "  959,\n",
       "  6,\n",
       "  600,\n",
       "  24,\n",
       "  27405,\n",
       "  15033,\n",
       "  7174,\n",
       "  6,\n",
       "  12801,\n",
       "  8015,\n",
       "  12657,\n",
       "  1282,\n",
       "  17809,\n",
       "  16,\n",
       "  10,\n",
       "  15980,\n",
       "  927,\n",
       "  8,\n",
       "  2914,\n",
       "  21434,\n",
       "  1569,\n",
       "  55,\n",
       "  747,\n",
       "  87,\n",
       "  45,\n",
       "  6,\n",
       "  8,\n",
       "  99,\n",
       "  24,\n",
       "  14026,\n",
       "  11,\n",
       "  1029,\n",
       "  40584,\n",
       "  24,\n",
       "  817,\n",
       "  62,\n",
       "  11,\n",
       "  12575,\n",
       "  4,\n",
       "  475,\n",
       "  338,\n",
       "  4,\n",
       "  24230,\n",
       "  90,\n",
       "  189,\n",
       "  157,\n",
       "  146,\n",
       "  10,\n",
       "  30980,\n",
       "  6269,\n",
       "  8,\n",
       "  14160,\n",
       "  2],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_dataset = tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d55ee3c3c054be7b0a2bd2ba4e9ffaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2443, 'learning_rate': 1.7620747085415183e-05, 'epoch': 0.36}\n",
      "{'loss': 0.2354, 'learning_rate': 1.524149417083036e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2b37a74d8e42e1b7f565ac34396476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2029215693473816, 'eval_runtime': 19489.7455, 'eval_samples_per_second': 0.144, 'eval_steps_per_second': 0.018, 'epoch': 1.0}\n",
      "{'loss': 0.2282, 'learning_rate': 1.286224125624554e-05, 'epoch': 1.07}\n",
      "{'loss': 0.2264, 'learning_rate': 1.048298834166072e-05, 'epoch': 1.43}\n",
      "{'loss': 0.2227, 'learning_rate': 8.103735427075898e-06, 'epoch': 1.78}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edad8b1d020c4d4e99e6547cd607104a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20161357522010803, 'eval_runtime': 2214.6406, 'eval_samples_per_second': 1.265, 'eval_steps_per_second': 0.158, 'epoch': 2.0}\n",
      "{'loss': 0.219, 'learning_rate': 5.7244825124910784e-06, 'epoch': 2.14}\n",
      "{'loss': 0.2204, 'learning_rate': 3.3452295979062578e-06, 'epoch': 2.5}\n",
      "{'loss': 0.2186, 'learning_rate': 9.659766833214373e-07, 'epoch': 2.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5139aa107d4c678002b2814457ef1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19864130020141602, 'eval_runtime': 36536.9869, 'eval_samples_per_second': 0.077, 'eval_steps_per_second': 0.01, 'epoch': 3.0}\n",
      "{'train_runtime': 209287.4986, 'train_samples_per_second': 0.161, 'train_steps_per_second': 0.02, 'train_loss': 0.2265650676370603, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4203, training_loss=0.2265650676370603, metrics={'train_runtime': 209287.4986, 'train_samples_per_second': 0.161, 'train_steps_per_second': 0.02, 'train_loss': 0.2265650676370603, 'epoch': 3.0})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../model/MLM_2015\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a9d9b138df46268f332f9d45b255d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 1.22\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer.push_to_hub('imene-kolli/my_awesome_eli5_mlm_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The president held the <mask> for 10 years.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.4038034975528717,\n",
       "  'token': 558,\n",
       "  'token_str': ' office',\n",
       "  'sequence': 'The president held the office for 10 years.'},\n",
       " {'score': 0.34403306245803833,\n",
       "  'token': 737,\n",
       "  'token_str': ' position',\n",
       "  'sequence': 'The president held the position for 10 years.'},\n",
       " {'score': 0.0841364786028862,\n",
       "  'token': 633,\n",
       "  'token_str': ' job',\n",
       "  'sequence': 'The president held the job for 10 years.'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\"fill-mask\", \"my_awesome_eli5_mlm_model\")\n",
    "mask_filler(text, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.09414557367563248,\n",
       "  'token': 558,\n",
       "  'token_str': ' office',\n",
       "  'sequence': 'welcome to my office, I am the manager here.'},\n",
       " {'score': 0.053980667144060135,\n",
       "  'token': 165,\n",
       "  'token_str': ' team',\n",
       "  'sequence': 'welcome to my team, I am the manager here.'},\n",
       " {'score': 0.03739987686276436,\n",
       "  'token': 1082,\n",
       "  'token_str': ' site',\n",
       "  'sequence': 'welcome to my site, I am the manager here.'}]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\"fill-mask\", \"my_awesome_eli5_mlm_model\")\n",
    "mask_filler('welcome to my <mask>, I am the manager here.', top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.5165475010871887,\n",
       "  'token': 1387,\n",
       "  'token_str': ' Office',\n",
       "  'sequence': 'I recently bought the Microsoft Office software.'},\n",
       " {'score': 0.06393763422966003,\n",
       "  'token': 12591,\n",
       "  'token_str': ' Edge',\n",
       "  'sequence': 'I recently bought the Microsoft Edge software.'},\n",
       " {'score': 0.0584854893386364,\n",
       "  'token': 27241,\n",
       "  'token_str': ' Excel',\n",
       "  'sequence': 'I recently bought the Microsoft Excel software.'}]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\"fill-mask\", \"my_awesome_eli5_mlm_model\")\n",
    "mask_filler('I recently bought the Microsoft <mask> software.', top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "''the political shop will probably be getting more and more involved in issues,'' mr. nofziger predicted as he prepared to leave office for the less hectic and more lucrative life\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data['train'])):\n",
    "    if 'office' in data['train'][i]['text'].split()[20:30]:\n",
    "        print(' '.join(data['train'][i]['text'].split()[:30]))\n",
    "        e = ' '.join(data['train'][i]['text'].split()[:30]).replace(' office ', ' <mask> ')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.22669820487499237,\n",
       "  'token': 2302,\n",
       "  'token_str': ' politics',\n",
       "  'sequence': \"''the political shop will probably be getting more and more involved in issues,'' mr. nofziger predicted as he prepared to leave politics for the less hectic and more lucrative life\"},\n",
       " {'score': 0.16870960593223572,\n",
       "  'token': 558,\n",
       "  'token_str': ' office',\n",
       "  'sequence': \"''the political shop will probably be getting more and more involved in issues,'' mr. nofziger predicted as he prepared to leave office for the less hectic and more lucrative life\"},\n",
       " {'score': 0.07311130315065384,\n",
       "  'token': 30017,\n",
       "  'token_str': ' academia',\n",
       "  'sequence': \"''the political shop will probably be getting more and more involved in issues,'' mr. nofziger predicted as he prepared to leave academia for the less hectic and more lucrative life\"}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\"fill-mask\", \"my_awesome_eli5_mlm_model\")\n",
    "mask_filler(e, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The president held the <mask> for 10 years.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_eli5_mlm_model\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"stevhliu/my_awesome_eli5_mlm_model\")\n",
    "logits = model(**inputs).logits\n",
    "mask_token_logits = logits[0, mask_token_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The president held the  position for 10 years.\n",
      "The president held the  office for 10 years.\n",
      "The president held the  job for 10 years.\n"
     ]
    }
   ],
   "source": [
    "top_3_tokens = torch.topk(mask_token_logits, 3, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_3_tokens:\n",
    "    print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imenepy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
