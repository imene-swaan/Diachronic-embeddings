{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/articles/2015_articles.txt', 'r') as f:\n",
    "    articles = f.readlines()\n",
    "    print(f'Found {len(articles)} articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(articles[0].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (4.32.0)\n",
      "Requirement already satisfied: datasets in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (2.10.1)\n",
      "Requirement already satisfied: evaluate in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from transformers) (3.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from transformers) (2.29.0)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: aiohttp in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: multiprocess in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from datasets) (2023.3.0)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: responses<0.19 in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f34d96d80e4ae58599690bd339be42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imene-kolli\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_ENABLE_MPS_FALLBACK=1\n"
     ]
    }
   ],
   "source": [
    "%env PYTORCH_ENABLE_MPS_FALLBACK= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset eli5 (/Users/imenekolli/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa)\n"
     ]
    }
   ],
   "source": [
    "eli5 = load_dataset(\"eli5\", split=\"train_asks[:5000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5 = eli5.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': '6v2om3',\n",
       " 'title': 'Please clear things up for me about the Great Barrier Reef. How much is gone, how much is bleached, can it be saved, what are the long term effects?',\n",
       " 'selftext': '',\n",
       " 'document': '',\n",
       " 'subreddit': 'askscience',\n",
       " 'answers': {'a_id': ['dlxlj4h', 'dlyb289', 'dlyjz9a'],\n",
       "  'text': ['*Apologies in advance for a lack of scientific sourcing. If you would like anything sourced better, let me know.*\\n\\nCoral \"bleaching\" is the algae having left the \"coral\". That is, the coral itself is literally the skeleton, so being \"bleached\" is literally  to be dead.\\n\\n_URL_2_\\n\\nBut, since the \"coral\" is [mostly] just the skeleton, new algae can be reintroduced to the skeleton. However, if the cause is not mitigated, then recolonization of the coral is very unlikely. Whatever caused it to bleach, would prevent it from coming back. In this case, the main causes seem to be particulate pollution (eg sediments), increased temperature, increased acidity and increased nitrogen.\\n\\nedit/correction: As per /u/Wog, the coral is the skeleton of an animal, collectively known as \"polyps\". It is the algae that give coral their color, so bleaching is the algae leaving or dying. The polyp, which relies on the algae for food, will die soon after, though some have other means of feeding. As per the below source, corals can recover if some polyps in protected areas manage to survive. However, the coral is actually alive, and can actually die, rather than just being host to things that die. /edit\\n\\n_URL_3_\\n\\nThe latest survey shows 900 miles (of ~1,400) have bleached *since 2016*. See the below source for a nice \"bleached - not bleached\" map. I\\'m searching for an overall percentage, but most either address a certain section (eg the northern portion or shallow-water portions), are for a certain time-frame, or address \"bleaching\" rather than \"bleached\". I will edit if I can find one.\\n\\n_URL_0_\\n\\nWhether it can be saved is more of a political question - given enough resources and global commitment, certainly humanity is capable. Actual steps being taken are to decrease various pollutants in run-off, which can include the run-off itself (eg fresh water), sediment erosion and fertilizers. (Sorry, I recognize this is a partial answer. Hopefully, someone else can fill in how much will be required over current efforts to stave off further destruction.)\\n\\n_URL_1_',\n",
       "   'I would be surprised if it could be saved. The Australian government wants to allow an Adani mine to be built dangerously close to the reef. The majority of the Australian population are against it. \\n\\nSo there is hope I guess, but the government is more interested in money than the well being of the reef.',\n",
       "   'About 1/3 of the GBR has died over the past 20 years, and the other two thirds are projected to also die over the next 50 or so years.\\n\\nSome google searches and any library can provide this information.  It probably cannot be saved seeing as climate change is possibly the largest factor in its demise.'],\n",
       "  'score': [34, 20, 2]},\n",
       " 'title_urls': {'url': []},\n",
       " 'selftext_urls': {'url': []},\n",
       " 'answers_urls': {'url': ['https://www.washingtonpost.com/news/energy-environment/wp/2017/04/09/for-the-second-year-in-a-row-severe-coral-bleaching-has-struck-the-great-barrier-reef/?utm_term=.b00bdf548d40',\n",
       "   'https://www.nature.org/ourinitiatives/urgentissues/oceans/coral-reefs/ways-to-help-coral-reefs/index.htm',\n",
       "   'https://oceanservice.noaa.gov/facts/coral_bleach.html',\n",
       "   'http://www.bbc.com/earth/story/20140916-the-corals-that-come-back-from-the-dead']}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': '6v2om3',\n",
       " 'title': 'Please clear things up for me about the Great Barrier Reef. How much is gone, how much is bleached, can it be saved, what are the long term effects?',\n",
       " 'selftext': '',\n",
       " 'document': '',\n",
       " 'subreddit': 'askscience',\n",
       " 'answers.a_id': ['dlxlj4h', 'dlyb289', 'dlyjz9a'],\n",
       " 'answers.text': ['*Apologies in advance for a lack of scientific sourcing. If you would like anything sourced better, let me know.*\\n\\nCoral \"bleaching\" is the algae having left the \"coral\". That is, the coral itself is literally the skeleton, so being \"bleached\" is literally  to be dead.\\n\\n_URL_2_\\n\\nBut, since the \"coral\" is [mostly] just the skeleton, new algae can be reintroduced to the skeleton. However, if the cause is not mitigated, then recolonization of the coral is very unlikely. Whatever caused it to bleach, would prevent it from coming back. In this case, the main causes seem to be particulate pollution (eg sediments), increased temperature, increased acidity and increased nitrogen.\\n\\nedit/correction: As per /u/Wog, the coral is the skeleton of an animal, collectively known as \"polyps\". It is the algae that give coral their color, so bleaching is the algae leaving or dying. The polyp, which relies on the algae for food, will die soon after, though some have other means of feeding. As per the below source, corals can recover if some polyps in protected areas manage to survive. However, the coral is actually alive, and can actually die, rather than just being host to things that die. /edit\\n\\n_URL_3_\\n\\nThe latest survey shows 900 miles (of ~1,400) have bleached *since 2016*. See the below source for a nice \"bleached - not bleached\" map. I\\'m searching for an overall percentage, but most either address a certain section (eg the northern portion or shallow-water portions), are for a certain time-frame, or address \"bleaching\" rather than \"bleached\". I will edit if I can find one.\\n\\n_URL_0_\\n\\nWhether it can be saved is more of a political question - given enough resources and global commitment, certainly humanity is capable. Actual steps being taken are to decrease various pollutants in run-off, which can include the run-off itself (eg fresh water), sediment erosion and fertilizers. (Sorry, I recognize this is a partial answer. Hopefully, someone else can fill in how much will be required over current efforts to stave off further destruction.)\\n\\n_URL_1_',\n",
       "  'I would be surprised if it could be saved. The Australian government wants to allow an Adani mine to be built dangerously close to the reef. The majority of the Australian population are against it. \\n\\nSo there is hope I guess, but the government is more interested in money than the well being of the reef.',\n",
       "  'About 1/3 of the GBR has died over the past 20 years, and the other two thirds are projected to also die over the next 50 or so years.\\n\\nSome google searches and any library can provide this information.  It probably cannot be saved seeing as climate change is possibly the largest factor in its demise.'],\n",
       " 'answers.score': [34, 20, 2],\n",
       " 'title_urls.url': [],\n",
       " 'selftext_urls.url': [],\n",
       " 'answers_urls.url': ['https://www.washingtonpost.com/news/energy-environment/wp/2017/04/09/for-the-second-year-in-a-row-severe-coral-bleaching-has-struck-the-great-barrier-reef/?utm_term=.b00bdf548d40',\n",
       "  'https://www.nature.org/ourinitiatives/urgentissues/oceans/coral-reefs/ways-to-help-coral-reefs/index.htm',\n",
       "  'https://oceanservice.noaa.gov/facts/coral_bleach.html',\n",
       "  'http://www.bbc.com/earth/story/20140916-the-corals-that-come-back-from-the-dead']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5 = eli5.flatten()\n",
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 512  # Maximum sequence length allowed by the model\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Split the input sequences into smaller parts if they exceed the maximum length\n",
    "    texts = [\" \".join(x) for x in examples[\"answers.text\"]]\n",
    "    tokenized_examples = tokenizer(texts, truncation=True, max_length=max_sequence_length)\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b86d6b5850c4401ad0d2b08292af649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77bbfe07ccce4ebcad80bfa5bfea8ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_eli5 = eli5.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=eli5[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6e9a065394471eb1d1b3220855d26b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297141e3dab941f78b56be9c46f7fe42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1897bea2e6e24dc6828ed9217887e999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2817 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/imenepy/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:1559: UserWarning: The operator 'aten::cumsum.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_83_m9l5ufr/croot/pytorch_1686041389610/work/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.35, 'learning_rate': 1.6450124245651406e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f54214eb7564a74a52451e875b55391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/226 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30741310119628906, 'eval_runtime': 25.8692, 'eval_samples_per_second': 69.851, 'eval_steps_per_second': 8.736, 'epoch': 1.0}\n",
      "{'loss': 0.3317, 'learning_rate': 1.2900248491302805e-05, 'epoch': 1.06}\n",
      "{'loss': 0.3279, 'learning_rate': 9.350372736954207e-06, 'epoch': 1.6}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "582c8a1180c04a83ab67eac5f0f013a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/226 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30345389246940613, 'eval_runtime': 25.7735, 'eval_samples_per_second': 70.111, 'eval_steps_per_second': 8.769, 'epoch': 2.0}\n",
      "{'loss': 0.3209, 'learning_rate': 5.800496982605609e-06, 'epoch': 2.13}\n",
      "{'loss': 0.3167, 'learning_rate': 2.250621228257011e-06, 'epoch': 2.66}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd09a84231f410e82c404905d3228a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/226 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.29302874207496643, 'eval_runtime': 141.8687, 'eval_samples_per_second': 12.737, 'eval_steps_per_second': 1.593, 'epoch': 3.0}\n",
      "{'train_runtime': 3513.6486, 'train_samples_per_second': 6.414, 'train_steps_per_second': 0.802, 'train_loss': 0.32798996757098364, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2817, training_loss=0.32798996757098364, metrics={'train_runtime': 3513.6486, 'train_samples_per_second': 6.414, 'train_steps_per_second': 0.802, 'train_loss': 0.32798996757098364, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_eli5_mlm_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb288df322b4e06b7aa0ca16d8c374d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/226 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 1.35\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/imene-kolli/my_awesome_eli5_mlm_model/tree/main/'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/imene-kolli/my_awesome_eli5_mlm_model/commit/814a95c64b632d44c7d1f86f768b6760619da3b9', commit_message='Upload tokenizer', commit_description='', oid='814a95c64b632d44c7d1f86f768b6760619da3b9', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub('imene-kolli/my_awesome_eli5_mlm_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The Milky Way is a <mask> galaxy.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de97b002c784c36b6630c20f158b5a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/677 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6bfff3dda442d08378273673f2dd1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944f01cfa5734ef689b1e97204f6f943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/386 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d170456dc0bd46ecbfbcbff0ce10b4e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f8f44a3b4b42a1ab9e5583d955e881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0050f69b862346c0ae48e1eba6bb3af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1121954bc01149d0a66e2cc586f055b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.5150937438011169,\n",
       "  'token': 21300,\n",
       "  'token_str': ' spiral',\n",
       "  'sequence': 'The Milky Way is a spiral galaxy.'},\n",
       " {'score': 0.07087340205907822,\n",
       "  'token': 2232,\n",
       "  'token_str': ' massive',\n",
       "  'sequence': 'The Milky Way is a massive galaxy.'},\n",
       " {'score': 0.06434684991836548,\n",
       "  'token': 650,\n",
       "  'token_str': ' small',\n",
       "  'sequence': 'The Milky Way is a small galaxy.'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\"fill-mask\", \"stevhliu/my_awesome_eli5_mlm_model\")\n",
    "mask_filler(text, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_eli5_mlm_model\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"stevhliu/my_awesome_eli5_mlm_model\")\n",
    "logits = model(**inputs).logits\n",
    "mask_token_logits = logits[0, mask_token_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Milky Way is a  spiral galaxy.\n",
      "The Milky Way is a  massive galaxy.\n",
      "The Milky Way is a  small galaxy.\n"
     ]
    }
   ],
   "source": [
    "top_3_tokens = torch.topk(mask_token_logits, 3, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_3_tokens:\n",
    "    print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imenepy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
