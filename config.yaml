



output_dir: 'output'


data_options:
    input_type: 'txt' # 'xml'
    data_path: 'input/txt/TheNewYorkTimes{}.txt' # 'input/xml/TheNewYorkTimes{}.xml'
    # xml_tag: 'fulltext'
    sample: False
    # max_documents: 100
    shuffle: True
    # seed: 42


preprocessing_options:
    remove_stopwords: True
    remove_punctuation: True 
    remove_numbers: True
    lowercase: True
    lemmatize: True
    remove_full_stop: True
    remove_short_words: True
    

mlm_options:
    model_name: 'roberta-base'
    max_length: 128
    mlm_probability: 0.15
    batch_size: 32
    epochs: 30
    split_ratio: 0.8
    learning_rate: 5e-5
    truncation: True
    padding: 'max_length'


word2vec_options:
    initialize:
        # model_path: Optional[str] = None
        min_count: 50
        window: 5
        # negative: 5 
        # ns_exponent: 0.75
        vector_size: 300
        # workers: 1
        # sg: 1

    train:
        epochs: 30
        start_alpha: 0.025
        end_alpha: 0.00001
        # compute_loss: True

    align:
        reference: -1
        method: 'procrustes'


temporal_graph_options:
    MLM_k: 1
    Context_k: 3
    level: 2
    edge_threshold: 0.1
    accumulate: True
    use_context_only: False
    keep_k: 
        0: [6,3]
        1: [2,1]
        # 2: [1,0]

periods: [
        1980,
        1982,
        1985,
        1987,
        1989,
        1990,
        1995,
        2000,
        2001,
        2005,
        2008,
        2010,
        2012,
        2015,
        2017
    ]   

target_words: [
        "trump",
        # "office",
        # "work",
        # "job",
        # "career",
        # "profession",
        # "employment",
        # "occupation",
        # "vocation",
        # "trade",
        # "business",
        # "position",
        # "post",
        # "trump",
        # "biden",
        # "obama",
        # "bush",
        # "ford",
        # "nixon",
        # "putin",
        # "us",
        # "usa",
        # "america",
        # "russia",
        # "china",
        # "germany",
        # "uk",
        # "france",
        # "italy",
        # "japan",
        # 'abuse',
        # 'corruption',
        # 'power',
        # 'crime',
        # 'misuse',
        # 'drug',
        # 'violence',
        # 'child',
        # 'domestic',
        # 'offense',
        # 'exploitation'
    ]

obsedian_options:
    obsedian_vault: "semantics-obsedian"
    view_period: -1

models:
    roberta_path: '{}/MLM_roberta_{}'
    word2vec_path: '{}/word2vec_aligned/w2v_{}_a.model'
    tgcn_path: '{}/TGCN/model'


tgcn_options:
    epochs: 10
    split_ratio: 0.9
    learning_rate: 5e-5
    device: 'cpu'


wordgraph_options:
    title: 'Word Graph - {}'
    node_color_feature: 0
    node_size_feature: 0
    edge_label_feature: 1
    color_bar: True
    color_norm: 'auto'
    
animation_options:
    wordgraph:
        title: 'Word Traffic'
        node_color_feature: 0
        node_size_feature: 0
        edge_label_feature: 1
        # radius: 2
        # distance: 1
    
    gif:
        start: 0
        end: -1
        repeat: True
        interval: 3000



pipeline:
    load_data: False
    preprocess: False
    save_preprocessed: False
    train_mlm: False
    train_word2vec: False
    align_word2vec: False
    construct_temporal_graph: False
    save_temporal_graph: False
    load_temporal_graph: True
    visualize_obsedian: False
    train_tgcn: True
    load_tgcn: True
    Infer_tgcn: True
    Tgcn_embeddings: True
    load_embeddings: True
    semantic_score: True
    visualize_wordgraph: False
    animate_wordgraph: False
    cluster_wordgraph: False



