{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Semantic Shift Detection","text":""},{"location":"utils/","title":"utilites","text":""},{"location":"utils/#semantics.utils.utils.intersection_align_gensim","title":"<code>intersection_align_gensim(m1, m2, words=None)</code>","text":"<p>Intersect two gensim word2vec models, m1 and m2. Only the shared vocabulary between them is kept. If 'words' is set (as list or set), then the vocabulary is intersected with this list as well. Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2). These indices correspond to the new syn0 and syn0norm objects in both gensim models:     -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0     -- you can find the index of any word on the .index2word list: model.index2word.index(word) =&gt; 2 The .vocab dictionary is also updated for each model, preserving the count but updating the index.</p> Source code in <code>semantics/utils/utils.py</code> <pre><code>def intersection_align_gensim(m1, m2, words=None):\n    \"\"\"\n    Intersect two gensim word2vec models, m1 and m2.\n    Only the shared vocabulary between them is kept.\n    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n        -- you can find the index of any word on the .index2word list: model.index2word.index(word) =&gt; 2\n    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n    \"\"\"\n\n    # Get the vocab for each model\n    vocab_m1 = set(m1.wv.index_to_key)\n    vocab_m2 = set(m2.wv.index_to_key)\n\n    # Find the common vocabulary\n    common_vocab = vocab_m1 &amp; vocab_m2\n    if words: common_vocab &amp;= set(words)\n\n    # If no alignment necessary because vocab is identical...\n    if not vocab_m1 - common_vocab and not vocab_m2 - common_vocab:\n        return (m1,m2)\n\n    # Otherwise sort by frequency (summed for both)\n    common_vocab = list(common_vocab)\n    common_vocab.sort(key=lambda w: m1.wv.get_vecattr(w, \"count\") + m2.wv.get_vecattr(w, \"count\"), reverse=True)\n    # print(len(common_vocab))\n\n    # Then for each model...\n    for m in [m1, m2]:\n        # Replace old syn0norm array with new one (with common vocab)\n        indices = [m.wv.key_to_index[w] for w in common_vocab]\n        old_arr = m.wv.vectors\n        new_arr = np.array([old_arr[index] for index in indices])\n        m.wv.vectors = new_arr\n\n        # Replace old vocab dictionary with new one (with common vocab)\n        # and old index2word with new one\n        new_key_to_index = {}\n        new_index_to_key = []\n        for new_index, key in enumerate(common_vocab):\n            new_key_to_index[key] = new_index\n            new_index_to_key.append(key)\n        m.wv.key_to_index = new_key_to_index\n        m.wv.index_to_key = new_index_to_key\n\n        print(len(m.wv.key_to_index), len(m.wv.vectors))\n\n    return (m1,m2)\n</code></pre>"},{"location":"utils/#semantics.utils.utils.read_toml","title":"<code>read_toml(config_path)</code>","text":"<p>Read in a config file and return a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the config file.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The dictionary.</p> Source code in <code>semantics/utils/utils.py</code> <pre><code>def read_toml(config_path: str) -&gt; dict:\n    \"\"\"\n    Read in a config file and return a dictionary.\n\n    Args:\n        config_path (str): The path to the config file.\n\n    Returns:\n        dict: The dictionary.\n    \"\"\"\n    with open(config_path, \"rb\") as f:\n        return tomli.load(f)\n</code></pre>"},{"location":"utils/#semantics.utils.utils.read_txt","title":"<code>read_txt(file_path)</code>","text":"<p>Read in a txt file and return a list of lines.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the txt file.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of lines.</p> Source code in <code>semantics/utils/utils.py</code> <pre><code>def read_txt(file_path: str) -&gt; list:\n    \"\"\"\n    Read in a txt file and return a list of lines.\n\n    Args:\n        file_path (str): The path to the txt file.\n\n    Returns:\n        list: A list of lines.\n    \"\"\"\n\n    with open(file_path) as f:\n        return f.readlines()\n</code></pre>"},{"location":"utils/#semantics.utils.utils.read_yaml","title":"<code>read_yaml(file_path)</code>","text":"<p>Read in a yaml file and return a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the yaml file.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The dictionary.</p> Source code in <code>semantics/utils/utils.py</code> <pre><code>def read_yaml(file_path: str) -&gt; dict:\n    \"\"\"\n    Read in a yaml file and return a dictionary.\n\n    Args:\n        file_path (str): The path to the yaml file.\n\n    Returns:\n        dict: The dictionary.\n    \"\"\"\n    with open(file_path) as f:\n        return yaml.load(f, Loader=yaml.FullLoader)\n</code></pre>"},{"location":"utils/#semantics.utils.utils.sample_data","title":"<code>sample_data(data, sample_size, random_seed=None)</code>","text":"<p>Sample data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list</code> <p>The data to sample.</p> required <code>sample_size</code> <code>int</code> <p>The size of the sample.</p> required <code>random_seed</code> <code>int</code> <p>The random seed.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>sample_data</code> <code>list</code> <p>The sampled data.</p> Source code in <code>semantics/utils/utils.py</code> <pre><code>def sample_data(data: list, sample_size: int, random_seed=None):\n    \"\"\"\n    Sample data.\n\n    Args:\n        data (list): The data to sample.\n        sample_size (int): The size of the sample.\n        random_seed (int): The random seed.\n\n    Returns:\n        sample_data (list): The sampled data.\n    \"\"\"\n\n    if random_seed:\n        random.seed(random_seed)\n    data_copy = data[:]\n    random.shuffle(data_copy)\n    return data_copy[:sample_size]\n</code></pre>"},{"location":"utils/#semantics.utils.utils.smart_procrustes_align_gensim","title":"<code>smart_procrustes_align_gensim(base_embed, other_embed, words=None)</code>","text":"<p>Original script: https://gist.github.com/quadrismegistus/09a93e219a6ffc4f216fb85235535faf Procrustes align two gensim word2vec models (to allow for comparison between same word across models). Code ported from HistWords https://github.com/williamleif/histwords by William Hamilton wleif@stanford.edu.</p> <p>First, intersect the vocabularies (see <code>intersection_align_gensim</code> documentation). Then do the alignment on the other_embed model. Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version. Return other_embed.</p> <p>If <code>words</code> is set, intersect the two models' vocabulary with the vocabulary in words (see <code>intersection_align_gensim</code> documentation).</p> Source code in <code>semantics/utils/utils.py</code> <pre><code>def smart_procrustes_align_gensim(base_embed, other_embed, words=None):\n    \"\"\"\n    Original script: https://gist.github.com/quadrismegistus/09a93e219a6ffc4f216fb85235535faf\n    Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n    Code ported from HistWords &lt;https://github.com/williamleif/histwords&gt; by William Hamilton &lt;wleif@stanford.edu&gt;.\n\n    First, intersect the vocabularies (see `intersection_align_gensim` documentation).\n    Then do the alignment on the other_embed model.\n    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n    Return other_embed.\n\n    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n    \"\"\"\n\n    # patch by Richard So [https://twitter.com/richardjeanso) (thanks!) to update this code for new version of gensim\n    # base_embed.init_sims(replace=True)\n    # other_embed.init_sims(replace=True)\n\n    # make sure vocabulary and indices are aligned\n    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n\n    # re-filling the normed vectors\n    in_base_embed.wv.fill_norms(force=True)\n    in_other_embed.wv.fill_norms(force=True)\n\n    # get the (normalized) embedding matrices\n    base_vecs = in_base_embed.wv.get_normed_vectors()\n    other_vecs = in_other_embed.wv.get_normed_vectors()\n\n    # just a matrix dot product with numpy\n    m = other_vecs.T.dot(base_vecs) \n    # SVD method from numpy\n    u, _, v = np.linalg.svd(m)\n    # another matrix operation\n    ortho = u.dot(v) \n    # Replace original array with modified one, i.e. multiplying the embedding matrix by \"ortho\"\n    other_embed.wv.vectors = (other_embed.wv.vectors).dot(ortho)    \n\n    return other_embed\n</code></pre>"},{"location":"utils/#semantics.utils.utils.train_test_split","title":"<code>train_test_split(data, test_ratio=0.2, random_seed=None)</code>","text":"<p>Split the data into train and test sets.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[str]</code> <p>The data to split.</p> required <code>test_ratio</code> <code>float</code> <p>The ratio of the test set.</p> <code>0.2</code> <code>random_seed</code> <code>int</code> <p>The random seed.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple(List[str], List[str])</code> <ul> <li>train_data (List[str]): The train set.</li> </ul> <code>tuple(List[str], List[str])</code> <ul> <li>test_data (List[str]): The test set.</li> </ul> Source code in <code>semantics/utils/utils.py</code> <pre><code>def train_test_split(data: List[str], test_ratio=0.2, random_seed=None) -&gt; tuple(List[str], List[str]):\n    \"\"\"\n    Split the data into train and test sets.\n\n    Args:\n        data (List[str]): The data to split.\n        test_ratio (float): The ratio of the test set.\n        random_seed (int): The random seed.\n\n    Returns:\n        - train_data (List[str]): The train set.\n        - test_data (List[str]): The test set.\n    \"\"\"\n\n    if random_seed:\n        random.seed(random_seed)\n    data_copy = data[:]\n    random.shuffle(data_copy)\n    split_idx = int(len(data_copy) * (1 - test_ratio))\n    train_data = data_copy[:split_idx]\n    test_data = data_copy[split_idx:]\n    return train_data, test_data\n</code></pre>"},{"location":"concepts/intro/","title":"Introduction","text":"<p>Semantic shift is a phenomenon that occurs naturally in all languages as the meaning of words can change over time due to various factors such as cultural shifts, technological advancements, and language contact. Understanding the semantic evolution of words is crucial for fields such as historical linguistics, cultural studies, and language technology. Etymology, the study of the origin and history of words, plays a pivotal role in tracing the development of words from their earliest known use to their current usage, and exam- ining the ways in which their meanings and forms have changed over time. The study of semantic shift is also vital for improving language technology applications such as search engines, content recommendation systems, and machine translation. For instance, track- ing semantic change can help search engines and content recommendation systems to provide more relevant and accurate results to users, and it can also improve the accuracy of machine translation systems. Traditional methods for tracking semantic shift involve manual annotation of texts and comparison of word usages across different time periods, which can be time-consuming and labor-intensive. However, recent advancements in natural language processing have led to the development of diachronic word embeddings, which offer a more efficient and au- tomated approach to tracking semantic shift. This study aims to explore the effectiveness of diachronic embeddings for semantic shift tracking.</p>"},{"location":"doc/data/load/","title":"Load","text":""},{"location":"doc/data/load/#semantics.data.data_loader.Loader","title":"<code>Loader</code>","text":"Source code in <code>semantics/data/data_loader.py</code> <pre><code>class Loader():\n    def __init__(\n            self,\n            texts: List[str]\n            ):\n        \"\"\"\n        Class for loading data.\n\n        Args:\n            texts (List[str]): List of texts.\n\n        Methods:\n            from_txt: Reads texts from a text file.\n            from_xml: Reads texts from an XML file.\n            forward: Filters the texts based on the target words and the maximum number of documents.\n            split_xml: Splits an XML file into multiple XML files with a maximum number of children.\n        \"\"\"\n\n        self.texts = texts\n\n\n    @classmethod\n    def from_txt(cls,\n                 path: Union[str, Path]\n                 ):\n        \"\"\"\n        Reads texts from a text file.\n\n        Args:\n            path (Union[str, Path]): Path to the text file.\n        \"\"\"\n        return cls(read_txt(path))\n\n    @classmethod\n    def from_xml(cls,\n                 path: Union[str, Path],\n                 tag: str\n                 ):\n        \"\"\"\n        Reads texts from an XML file.\n\n        Args:\n            path (Union[str, Path]): Path to the XML file.\n            tag (str): Tag of the XML file.\n        \"\"\"\n        size = os.path.getsize(path)\n        if size &gt; 1e8:\n            raise ValueError(\"File size is too large. Please split the file into smaller files.\")\n        tree = ET.parse(path)\n        root = tree.getroot()\n        texts = []\n        for elem in root.findall('.//' + tag):\n            if isinstance(elem.text, str):\n                texts.append(elem.text)\n        return cls(texts)\n\n\n\n    def forward(\n            self, \n            target_words: Optional[Union[List[str], str]] = None, \n            max_documents: Optional[int] = None, \n            shuffle: bool = True, \n            random_seed: Optional[int] = None\n            ) -&gt; List[str]:\n        \"\"\"\n        Filters the texts based on the target words and the maximum number of documents.\n\n        Args:\n            target_words (List[str], str, None): List of target words. Defaults to None.\n            max_documents (int, None): Maximum number of documents. Defaults to None.\n            shuffle (bool): Whether to shuffle the data. Defaults to True.\n            random_seed (int, None): Random seed. Defaults to None.\n\n        Returns:\n            List[str]: List of texts.\n        \"\"\"\n\n        if target_words:\n            relevant_texts = []\n            for text in self.texts:\n                if any([' ' + word + ' ' in text for word in target_words]):\n                    relevant_texts.append(text)\n\n            self.texts = relevant_texts\n\n        if max_documents is not None:\n            if shuffle:\n                self.texts = sample_data(self.texts, max_documents, random_seed)\n            else:\n                self.texts = self.texts[:max_documents]\n\n        return self.texts\n</code></pre>"},{"location":"doc/data/load/#semantics.data.data_loader.Loader.__init__","title":"<code>__init__(texts)</code>","text":"<p>Class for loading data.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[str]</code> <p>List of texts.</p> required <p>Functions:</p> Name Description <code>from_txt</code> <p>Reads texts from a text file.</p> <code>from_xml</code> <p>Reads texts from an XML file.</p> <code>forward</code> <p>Filters the texts based on the target words and the maximum number of documents.</p> <code>split_xml</code> <p>Splits an XML file into multiple XML files with a maximum number of children.</p> Source code in <code>semantics/data/data_loader.py</code> <pre><code>def __init__(\n        self,\n        texts: List[str]\n        ):\n    \"\"\"\n    Class for loading data.\n\n    Args:\n        texts (List[str]): List of texts.\n\n    Methods:\n        from_txt: Reads texts from a text file.\n        from_xml: Reads texts from an XML file.\n        forward: Filters the texts based on the target words and the maximum number of documents.\n        split_xml: Splits an XML file into multiple XML files with a maximum number of children.\n    \"\"\"\n\n    self.texts = texts\n</code></pre>"},{"location":"doc/data/load/#semantics.data.data_loader.Loader.forward","title":"<code>forward(target_words=None, max_documents=None, shuffle=True, random_seed=None)</code>","text":"<p>Filters the texts based on the target words and the maximum number of documents.</p> <p>Parameters:</p> Name Type Description Default <code>target_words</code> <code>(List[str], str, None)</code> <p>List of target words. Defaults to None.</p> <code>None</code> <code>max_documents</code> <code>(int, None)</code> <p>Maximum number of documents. Defaults to None.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data. Defaults to True.</p> <code>True</code> <code>random_seed</code> <code>(int, None)</code> <p>Random seed. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of texts.</p> Source code in <code>semantics/data/data_loader.py</code> <pre><code>def forward(\n        self, \n        target_words: Optional[Union[List[str], str]] = None, \n        max_documents: Optional[int] = None, \n        shuffle: bool = True, \n        random_seed: Optional[int] = None\n        ) -&gt; List[str]:\n    \"\"\"\n    Filters the texts based on the target words and the maximum number of documents.\n\n    Args:\n        target_words (List[str], str, None): List of target words. Defaults to None.\n        max_documents (int, None): Maximum number of documents. Defaults to None.\n        shuffle (bool): Whether to shuffle the data. Defaults to True.\n        random_seed (int, None): Random seed. Defaults to None.\n\n    Returns:\n        List[str]: List of texts.\n    \"\"\"\n\n    if target_words:\n        relevant_texts = []\n        for text in self.texts:\n            if any([' ' + word + ' ' in text for word in target_words]):\n                relevant_texts.append(text)\n\n        self.texts = relevant_texts\n\n    if max_documents is not None:\n        if shuffle:\n            self.texts = sample_data(self.texts, max_documents, random_seed)\n        else:\n            self.texts = self.texts[:max_documents]\n\n    return self.texts\n</code></pre>"},{"location":"doc/data/load/#semantics.data.data_loader.Loader.from_txt","title":"<code>from_txt(path)</code>  <code>classmethod</code>","text":"<p>Reads texts from a text file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to the text file.</p> required Source code in <code>semantics/data/data_loader.py</code> <pre><code>@classmethod\ndef from_txt(cls,\n             path: Union[str, Path]\n             ):\n    \"\"\"\n    Reads texts from a text file.\n\n    Args:\n        path (Union[str, Path]): Path to the text file.\n    \"\"\"\n    return cls(read_txt(path))\n</code></pre>"},{"location":"doc/data/load/#semantics.data.data_loader.Loader.from_xml","title":"<code>from_xml(path, tag)</code>  <code>classmethod</code>","text":"<p>Reads texts from an XML file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to the XML file.</p> required <code>tag</code> <code>str</code> <p>Tag of the XML file.</p> required Source code in <code>semantics/data/data_loader.py</code> <pre><code>@classmethod\ndef from_xml(cls,\n             path: Union[str, Path],\n             tag: str\n             ):\n    \"\"\"\n    Reads texts from an XML file.\n\n    Args:\n        path (Union[str, Path]): Path to the XML file.\n        tag (str): Tag of the XML file.\n    \"\"\"\n    size = os.path.getsize(path)\n    if size &gt; 1e8:\n        raise ValueError(\"File size is too large. Please split the file into smaller files.\")\n    tree = ET.parse(path)\n    root = tree.getroot()\n    texts = []\n    for elem in root.findall('.//' + tag):\n        if isinstance(elem.text, str):\n            texts.append(elem.text)\n    return cls(texts)\n</code></pre>"},{"location":"doc/data/load/#semantics.data.data_loader.split_xml","title":"<code>split_xml(path, output_dir, max_children=1000)</code>","text":"<p>Splits an XML file into multiple XML files with a maximum number of children.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the XML file.</p> required <code>output_dir</code> <code>str</code> <p>Path to the output directory.</p> required <code>max_children</code> <code>int</code> <p>Maximum number of children. Defaults to 1000.</p> <code>1000</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of paths to the new XML files.</p> Source code in <code>semantics/data/data_loader.py</code> <pre><code>def split_xml(path:str, output_dir:str, max_children:int = 1000) -&gt; List[str]:\n    \"\"\"\n    Splits an XML file into multiple XML files with a maximum number of children.\n\n    Args:\n        path (str): Path to the XML file.\n        output_dir (str): Path to the output directory.\n        max_children (int, optional): Maximum number of children. Defaults to 1000.\n\n    Returns:\n        List[str]: List of paths to the new XML files.\n    \"\"\"\n\n    # Parse the XML\n    tree = ET.parse(path)\n    root = tree.getroot()\n    file_name = Path(path).stem\n\n    paths = []\n    # Create new XML trees based on the split\n    for idx, i in enumerate(range(0, len(root), max_children)):\n        new_root = ET.Element(root.tag, root.attrib)\n        new_root.extend(root[i:i + max_children])\n        new_tree = ET.ElementTree(new_root)\n        new_tree.write(f\"{output_dir}/{file_name}_{idx}.xml\")\n        paths.append(f\"{output_dir}/{file_name}_{idx}.xml\")\n\n    return paths\n</code></pre>"},{"location":"doc/data/preprocess/","title":"Preprocess","text":""},{"location":"doc/data/preprocess/#semantics.data.data_preprocessing.PREPROCESS","title":"<code>PREPROCESS</code>","text":"Source code in <code>semantics/data/data_preprocessing.py</code> <pre><code>class PREPROCESS():\n    def __init__(self):\n        \"\"\"\n        This class is used to preprocess text data.\n        \"\"\"\n        pass\n\n    def forward(self,\n                text,\n                remove_punctuation: bool = True,\n                remove_numbers: bool = True,\n                lowercase: bool = True,\n                lemmatize: bool = True,\n                remove_stopwords: bool = True,            \n        ):\n\n        \"\"\"\n        This function preprocesses text data.\n\n        Args:\n            text (str): Text to be preprocessed.\n            remove_punctuation (bool): Whether to remove punctuation. Defaults to True.\n            remove_numbers (bool): Whether to remove numbers. Defaults to True.\n            lowercase (bool): Whether to lowercase. Defaults to True.\n            lemmatize (bool): Whether to lemmatize. Defaults to True.\n            remove_stopwords (bool): Whether to remove stopwords. Defaults to True.\n\n        Returns:\n            newtext (str): Preprocessed text.\n        \"\"\"\n\n        newtext = re.sub('\\n', ' ', text) # Remove ordinary linebreaks (there shouldn't be, so this might be redundant)\n\n        if remove_punctuation:\n            newtext = re.sub(r'[^a-zA-Z0-9\\s\\.]', '', str(newtext)) # Remove anything that is not a space, a letter, a dot, or a number\n\n        if remove_numbers:\n            newtext = re.sub(r'[0-9]', '', str(newtext)) # Remove numbers\n\n        if lowercase:\n            newtext = str(newtext).lower() # Lowercase\n\n        if lemmatize:\n            from nltk.stem import WordNetLemmatizer\n            from nltk.corpus import wordnet \n\n            lemmatizer = WordNetLemmatizer()\n            newtext = ' '.join(list(map(lambda x: lemmatizer.lemmatize(x, \"n\"), newtext.split())))\n            # newtext = ' '.join(list(map(lambda x: lemmatizer.lemmatize(x, \"v\"), newtext.split())))\n\n        if remove_stopwords:\n            from nltk.corpus import stopwords\n            stop_words = set(stopwords.words('english'))\n            newtext = ' '.join(list(filter(lambda x: x not in stop_words, newtext.split())))\n\n        return newtext\n</code></pre>"},{"location":"doc/data/preprocess/#semantics.data.data_preprocessing.PREPROCESS.__init__","title":"<code>__init__()</code>","text":"<p>This class is used to preprocess text data.</p> Source code in <code>semantics/data/data_preprocessing.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    This class is used to preprocess text data.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"doc/data/preprocess/#semantics.data.data_preprocessing.PREPROCESS.forward","title":"<code>forward(text, remove_punctuation=True, remove_numbers=True, lowercase=True, lemmatize=True, remove_stopwords=True)</code>","text":"<p>This function preprocesses text data.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to be preprocessed.</p> required <code>remove_punctuation</code> <code>bool</code> <p>Whether to remove punctuation. Defaults to True.</p> <code>True</code> <code>remove_numbers</code> <code>bool</code> <p>Whether to remove numbers. Defaults to True.</p> <code>True</code> <code>lowercase</code> <code>bool</code> <p>Whether to lowercase. Defaults to True.</p> <code>True</code> <code>lemmatize</code> <code>bool</code> <p>Whether to lemmatize. Defaults to True.</p> <code>True</code> <code>remove_stopwords</code> <code>bool</code> <p>Whether to remove stopwords. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>newtext</code> <code>str</code> <p>Preprocessed text.</p> Source code in <code>semantics/data/data_preprocessing.py</code> <pre><code>def forward(self,\n            text,\n            remove_punctuation: bool = True,\n            remove_numbers: bool = True,\n            lowercase: bool = True,\n            lemmatize: bool = True,\n            remove_stopwords: bool = True,            \n    ):\n\n    \"\"\"\n    This function preprocesses text data.\n\n    Args:\n        text (str): Text to be preprocessed.\n        remove_punctuation (bool): Whether to remove punctuation. Defaults to True.\n        remove_numbers (bool): Whether to remove numbers. Defaults to True.\n        lowercase (bool): Whether to lowercase. Defaults to True.\n        lemmatize (bool): Whether to lemmatize. Defaults to True.\n        remove_stopwords (bool): Whether to remove stopwords. Defaults to True.\n\n    Returns:\n        newtext (str): Preprocessed text.\n    \"\"\"\n\n    newtext = re.sub('\\n', ' ', text) # Remove ordinary linebreaks (there shouldn't be, so this might be redundant)\n\n    if remove_punctuation:\n        newtext = re.sub(r'[^a-zA-Z0-9\\s\\.]', '', str(newtext)) # Remove anything that is not a space, a letter, a dot, or a number\n\n    if remove_numbers:\n        newtext = re.sub(r'[0-9]', '', str(newtext)) # Remove numbers\n\n    if lowercase:\n        newtext = str(newtext).lower() # Lowercase\n\n    if lemmatize:\n        from nltk.stem import WordNetLemmatizer\n        from nltk.corpus import wordnet \n\n        lemmatizer = WordNetLemmatizer()\n        newtext = ' '.join(list(map(lambda x: lemmatizer.lemmatize(x, \"n\"), newtext.split())))\n        # newtext = ' '.join(list(map(lambda x: lemmatizer.lemmatize(x, \"v\"), newtext.split())))\n\n    if remove_stopwords:\n        from nltk.corpus import stopwords\n        stop_words = set(stopwords.words('english'))\n        newtext = ' '.join(list(filter(lambda x: x not in stop_words, newtext.split())))\n\n    return newtext\n</code></pre>"},{"location":"doc/feature_extraction/bert/","title":"BERT","text":""},{"location":"doc/feature_extraction/bert/#semantics.feature_extraction.bert.BertEmbeddings","title":"<code>BertEmbeddings</code>","text":"<p>This class is used to infer the vector embeddings of a word from a sentence.</p>"},{"location":"doc/feature_extraction/bert/#semantics.feature_extraction.bert.BertEmbeddings--methods","title":"Methods","text":"<pre><code>infer_vector(doc:str, main_word:str)\n    This method is used to infer the vector embeddings of a word from a sentence.\n_bert_case_preparation()\n    This method is used to prepare the BERT model for the inference.\n</code></pre> Source code in <code>semantics/feature_extraction/bert.py</code> <pre><code>class BertEmbeddings:\n    \"\"\"\n    This class is used to infer the vector embeddings of a word from a sentence.\n\n    Methods\n    -------\n        infer_vector(doc:str, main_word:str)\n            This method is used to infer the vector embeddings of a word from a sentence.\n        _bert_case_preparation()\n            This method is used to prepare the BERT model for the inference.\n    \"\"\"\n    def __init__(\n        self,\n        pretrained_model_path:Union[str, Path] = None,\n    ):\n        self.model_path = pretrained_model_path\n        if pretrained_model_path is not None:\n            if not os.path.exists(pretrained_model_path):\n                raise ValueError(\n                    f'The path {pretrained_model_path} does not exist'\n                )\n            self.model_path = Path(pretrained_model_path)\n\n        self._tokens = []\n        self.model = None\n        self.vocab = False\n        self.lematizer = None\n\n        lg.set_verbosity_error()\n        self._bert_case_preparation()\n\n    @property\n    def tokens(self):\n        return self._tokens\n\n    def _bert_case_preparation(self) -&gt; None:\n        \"\"\"\n        This method is used to prepare the BERT model for the inference.\n        \"\"\"\n        model_path = self.model_path if self.model_path is not None else 'bert-base-uncased'\n        self.bert_tokenizer = BertTokenizer.from_pretrained(model_path)\n        self.model = BertModel.from_pretrained(\n            model_path,\n            output_hidden_states = True,\n        )\n        self.model.eval()\n        self.vocab = True\n\n    def infer_vector(self, doc:str, main_word:str):\n        \"\"\"\n        This method is used to infer the vector embeddings of a word from a sentence.\n        Args:\n            doc: Document to process\n            main_word: Main work to extract the vector embeddings for.\n\n        Returns: torch.Tensor\n\n        \"\"\"\n        if not self.vocab:\n            raise ValueError(\n                f'The Embedding model {self.model.__class__.__name__} has not been initialized'\n            )\n        marked_text = \"[CLS] \" + doc + \" [SEP]\"\n        tokens = self.bert_tokenizer.tokenize(marked_text)\n        try:\n            main_token_id = tokens.index(main_word.lower())\n            idx = self.bert_tokenizer.convert_tokens_to_ids(tokens)\n            segment_id = [1] * len(tokens)\n\n            self.tokens_tensor = torch.tensor([idx])\n            self.segments_tensors = torch.tensor([segment_id])\n\n            with torch.no_grad():\n                outputs = self.model(self.tokens_tensor, self.segments_tensors)\n                hidden_states = outputs[2]\n\n            return hidden_states[-2][0][main_token_id]\n\n        except ValueError:\n            raise ValueError(\n                f'The word: \"{main_word}\" does not exist in the list of tokens: {tokens} from {doc}'\n            )\n</code></pre>"},{"location":"doc/feature_extraction/bert/#semantics.feature_extraction.bert.BertEmbeddings.infer_vector","title":"<code>infer_vector(doc, main_word)</code>","text":"<p>This method is used to infer the vector embeddings of a word from a sentence. Args:     doc: Document to process     main_word: Main work to extract the vector embeddings for.</p> <p>Returns: torch.Tensor</p> Source code in <code>semantics/feature_extraction/bert.py</code> <pre><code>def infer_vector(self, doc:str, main_word:str):\n    \"\"\"\n    This method is used to infer the vector embeddings of a word from a sentence.\n    Args:\n        doc: Document to process\n        main_word: Main work to extract the vector embeddings for.\n\n    Returns: torch.Tensor\n\n    \"\"\"\n    if not self.vocab:\n        raise ValueError(\n            f'The Embedding model {self.model.__class__.__name__} has not been initialized'\n        )\n    marked_text = \"[CLS] \" + doc + \" [SEP]\"\n    tokens = self.bert_tokenizer.tokenize(marked_text)\n    try:\n        main_token_id = tokens.index(main_word.lower())\n        idx = self.bert_tokenizer.convert_tokens_to_ids(tokens)\n        segment_id = [1] * len(tokens)\n\n        self.tokens_tensor = torch.tensor([idx])\n        self.segments_tensors = torch.tensor([segment_id])\n\n        with torch.no_grad():\n            outputs = self.model(self.tokens_tensor, self.segments_tensors)\n            hidden_states = outputs[2]\n\n        return hidden_states[-2][0][main_token_id]\n\n    except ValueError:\n        raise ValueError(\n            f'The word: \"{main_word}\" does not exist in the list of tokens: {tokens} from {doc}'\n        )\n</code></pre>"},{"location":"doc/feature_extraction/roberta/","title":"RoBERTa","text":""},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.CustomDataset","title":"<code>CustomDataset</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>This class is used to create a custom dataset for the Roberta model.</p>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.CustomDataset--methods","title":"Methods","text":"<pre><code>__init__(data: List[str], tokenizer, max_length=128, truncation=True, padding=True)\n    The constructor for the CustomDataset class.\n__len__()\n    This method is used to get the length of the dataset.\n__getitem__(idx)\n    This method is used to get the item at a specific index.\n</code></pre> Source code in <code>semantics/feature_extraction/roberta.py</code> <pre><code>class CustomDataset(Dataset):\n    \"\"\"\n    This class is used to create a custom dataset for the Roberta model.\n\n    Methods\n    -------\n        __init__(data: List[str], tokenizer, max_length=128, truncation=True, padding=True)\n            The constructor for the CustomDataset class.\n        __len__()\n            This method is used to get the length of the dataset.\n        __getitem__(idx)\n            This method is used to get the item at a specific index.  \n    \"\"\"\n    def __init__(\n            self, \n            data: List[str], \n            tokenizer, \n            max_length=128,\n            truncation=True,\n            padding= \"max_length\",\n            ):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.tokenized_data = tokenizer(data, truncation=truncation, padding=padding, max_length=max_length)\n\n    def __len__(self):\n        return len(self.tokenized_data.input_ids)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Retrieves the item at the specified index.\n\n        Parameters:\n            idx (int): Index of the item to retrieve.\n\n        Returns:\n            tokenized_data (dict): Dictionary containing the input_ids, attention_mask, and labels.\n        \"\"\"\n        # Get the tokenized inputs at the specified index\n        input_ids = self.tokenized_data.input_ids[idx]\n        attention_mask = self.tokenized_data.attention_mask[idx]\n\n        # Return a dictionary containing input_ids, attention_mask, and labels (if applicable)\n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask\n            # Add 'labels': labels if you have labels for your data\n        }\n</code></pre>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.CustomDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Retrieves the item at the specified index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the item to retrieve.</p> required <p>Returns:</p> Name Type Description <code>tokenized_data</code> <code>dict</code> <p>Dictionary containing the input_ids, attention_mask, and labels.</p> Source code in <code>semantics/feature_extraction/roberta.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"\n    Retrieves the item at the specified index.\n\n    Parameters:\n        idx (int): Index of the item to retrieve.\n\n    Returns:\n        tokenized_data (dict): Dictionary containing the input_ids, attention_mask, and labels.\n    \"\"\"\n    # Get the tokenized inputs at the specified index\n    input_ids = self.tokenized_data.input_ids[idx]\n    attention_mask = self.tokenized_data.attention_mask[idx]\n\n    # Return a dictionary containing input_ids, attention_mask, and labels (if applicable)\n    return {\n        'input_ids': input_ids,\n        'attention_mask': attention_mask\n        # Add 'labels': labels if you have labels for your data\n    }\n</code></pre>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaEmbedding","title":"<code>RobertaEmbedding</code>","text":"<p>This class is used to infer vector embeddings from a sentence.</p>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaEmbedding--methods","title":"Methods","text":"<pre><code>__init__(pretrained_model_path:Union[str, Path] = None)\n    The constructor for the VectorEmbeddings class.\n_roberta_case_preparation()\n    This method is used to prepare the Roberta model for the inference.\ninfer_vector(doc:str, main_word:str)\n    This method is used to infer the vector embeddings of a word from a sentence.\ninfer_mask_logits(doc:str)\n    This method is used to infer the logits of a word from a sentence.\n</code></pre> Source code in <code>semantics/feature_extraction/roberta.py</code> <pre><code>class RobertaEmbedding:\n    \"\"\"\n    This class is used to infer vector embeddings from a sentence.\n\n    Methods\n    -------\n        __init__(pretrained_model_path:Union[str, Path] = None)\n            The constructor for the VectorEmbeddings class.\n        _roberta_case_preparation()\n            This method is used to prepare the Roberta model for the inference.\n        infer_vector(doc:str, main_word:str)\n            This method is used to infer the vector embeddings of a word from a sentence.\n        infer_mask_logits(doc:str)\n            This method is used to infer the logits of a word from a sentence.\n    \"\"\"\n    def __init__(\n        self,\n        pretrained_model_path:Union[str, Path] = None,\n    ):\n        self.model_path = pretrained_model_path\n        if pretrained_model_path is not None:\n            if not os.path.exists(pretrained_model_path):\n                raise ValueError(\n                    f'The path {pretrained_model_path} does not exist'\n                )\n            self.model_path = Path(pretrained_model_path)\n\n        self._tokens = []\n        self.model = None\n        self.vocab = False\n\n        lg.set_verbosity_error()\n        self._roberta_case_preparation()\n\n    @property\n    def tokens(self):\n        return self._tokens\n\n    def _roberta_case_preparation(self) -&gt; None:\n        \"\"\"\n        This method is used to prepare the BERT model for the inference.\n        \"\"\"\n        model_path = self.model_path if self.model_path is not None else 'roberta-base'\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        self.model = RobertaModel.from_pretrained(\n            model_path, \n            output_hidden_states=True\n            )\n        self.MLM = RobertaForMaskedLM.from_pretrained(\n            model_path,\n            output_hidden_states = True,\n        )\n        self.max_length = self.model.config.max_position_embeddings\n        self.model.eval()\n        self.MLM.eval()\n        self.vocab = True\n\n    def infer_vector(self, doc:str, main_word:str) -&gt; torch.Tensor:\n        \"\"\"\n        This method is used to infer the vector embeddings of a word from a sentence.\n        Args:\n            doc: Document to process\n            main_word: Main work to extract the vector embeddings for.\n\n        Returns: \n            embeddings: Tensor of stacked embeddings (torch.Tensor) of shape (num_embeddings, embedding_size) where num_embeddings is the number of times the main_word appears in the doc.\n\n        Examples:\n            &gt;&gt;&gt; model = RobertaEmbedding()\n            &gt;&gt;&gt; model.infer_vector(doc=\"The brown fox jumps over the lazy dog\", main_word=\"fox\")\n            tensor([[-0.2182, -0.1597, -0.1723,  ..., -0.1706, -0.1709, -0.1709],\n                    [-0.2182, -0.1597, -0.1723,  ..., -0.1706, -0.1709, -0.1709],\n                    [-0.2182, -0.1597, -0.1723,  ..., -0.1706, -0.1709, -0.1709],\n                    [-0.2182, -0.1597, -0.1723,  ..., -0.1706, -0.1709, -0.1709],\n                    [-0.2182, -0.1597, -0.1723,  ..., -0.1706, -0.1709, -0.1709]])\n        \"\"\"\n        if not self.vocab:\n            raise ValueError(\n                f'The Embedding model {self.model.__class__.__name__} has not been initialized'\n            )\n\n\n        input_ids = self.tokenizer(doc, return_tensors=\"pt\", max_length=self.max_length).input_ids\n        token = self.tokenizer.encode(main_word, add_special_tokens=False)[0]\n\n        word_token_index = torch.where(input_ids == token)[1]\n        emb = []\n\n        try:\n            with torch.no_grad():\n                embeddings = self.model(input_ids).last_hidden_state\n\n            emb = [embeddings[0, idx] for idx in word_token_index]\n            return torch.stack(emb)\n\n        except:\n            print(f'The word: \"{main_word}\" does not exist in the list of tokens')\n            return torch.tensor(np.array(emb))\n\n\n\n\n    def infer_mask_logits(self, doc:str) -&gt; torch.Tensor:\n        \"\"\"\n        This method is used to infer the logits of the mask token in a sentence.\n        Args:\n            doc (str): Document to process where the mask token is present.\n\n        Returns: \n            logits: Tensor of stacked logits (torch.Tensor) of shape (num_embeddings, logits_size) where num_embeddings is the number of times the mask token (&lt;mask&gt;) appears in the doc.\n\n        Examples:\n            &gt;&gt;&gt; model = RobertaEmbedding()\n            &gt;&gt;&gt; model.infer_mask_logits(doc=\"The brown fox &lt;mask&gt; over the lazy dog\")\n            tensor([[-2.1816e-01, -1.5967e-01, -1.7225e-01,  ..., -1.7064e-01,\n                    -1.7090e-01, -1.7093e-01],\n                    [-2.1816e-01, -1.5967e-01, -1.7225e-01,  ..., -1.7064e-01,\n                    -1.7090e-01, -1.7093e-01],\n                    [-2.1816e-01, -1.5967e-01, -1.7225e-01,  ..., -1.7064e-01,\n                    -1.7090e-01, -1.7093e-01],\n                    [-2.1816e-01, -1.5967e-01, -1.7225e-01,  ..., -1.7064e-01,\n                    -1.7090e-01, -1.7093e-01],\n                    [-2.1816e-01, -1.5967e-01, -1.7225e-01,  ..., -1.7064e-01,\n                    -1.7090e-01, -1.7093e-01]])\n        \"\"\"\n\n        if not self.vocab:\n            raise ValueError(\n                f'The Embedding model {self.MLM.__class__.__name__} has not been initialized'\n            )\n\n        input_ids = self.tokenizer(doc, return_tensors=\"pt\", max_length=self.max_length).input_ids\n        mask_token_index = torch.where(input_ids == self.tokenizer.mask_token_id)[1]\n        l = []\n        try:\n            with torch.no_grad():\n                logits = self.MLM(input_ids).logits\n\n            l = [logits[0, idx] for idx in mask_token_index]\n            return torch.stack(l) if len(l) &gt; 0 else torch.empty(0)\n\n        except IndexError:\n            raise ValueError(f'The mask falls outside of the max length of {self.max_length}, please use a smaller sentence')\n</code></pre>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaEmbedding.infer_mask_logits","title":"<code>infer_mask_logits(doc)</code>","text":"<p>This method is used to infer the logits of the mask token in a sentence. Args:     doc (str): Document to process where the mask token is present.</p> <p>Returns:</p> Name Type Description <code>logits</code> <code>Tensor</code> <p>Tensor of stacked logits (torch.Tensor) of shape (num_embeddings, logits_size) where num_embeddings is the number of times the mask token () appears in the doc. <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = RobertaEmbedding()\n&gt;&gt;&gt; model.infer_mask_logits(doc=\"The brown fox &lt;mask&gt; over the lazy dog\")\ntensor([[-2.1816e-01, -1.5967e-01, -1.7225e-01,  ..., -1.7064e-01,\n        -1.7090e-01, -1.7093e-01],\n        [-2.1816e-01, -1.5967e-01, -1.7225e-01,  ..., -1.7064e-01,\n        -1.7090e-01, -1.7093e-01],\n        [-2.1816e-01, -1.5967e-01, -1.7225e-01,  ..., -1.7064e-01,\n        -1.7090e-01, -1.7093e-01],\n        [-2.1816e-01, -1.5967e-01, -1.7225e-01,  ..., -1.7064e-01,\n        -1.7090e-01, -1.7093e-01],\n        [-2.1816e-01, -1.5967e-01, -1.7225e-01,  ..., -1.7064e-01,\n        -1.7090e-01, -1.7093e-01]])\n</code></pre> Source code in <code>semantics/feature_extraction/roberta.py</code> <pre><code>def infer_mask_logits(self, doc:str) -&gt; torch.Tensor:\n    \"\"\"\n    This method is used to infer the logits of the mask token in a sentence.\n    Args:\n        doc (str): Document to process where the mask token is present.\n\n    Returns: \n        logits: Tensor of stacked logits (torch.Tensor) of shape (num_embeddings, logits_size) where num_embeddings is the number of times the mask token (&lt;mask&gt;) appears in the doc.\n\n    Examples:\n        &gt;&gt;&gt; model = RobertaEmbedding()\n        &gt;&gt;&gt; model.infer_mask_logits(doc=\"The brown fox &lt;mask&gt; over the lazy dog\")\n        tensor([[-2.1816e-01, -1.5967e-01, -1.7225e-01,  ..., -1.7064e-01,\n                -1.7090e-01, -1.7093e-01],\n                [-2.1816e-01, -1.5967e-01, -1.7225e-01,  ..., -1.7064e-01,\n                -1.7090e-01, -1.7093e-01],\n                [-2.1816e-01, -1.5967e-01, -1.7225e-01,  ..., -1.7064e-01,\n                -1.7090e-01, -1.7093e-01],\n                [-2.1816e-01, -1.5967e-01, -1.7225e-01,  ..., -1.7064e-01,\n                -1.7090e-01, -1.7093e-01],\n                [-2.1816e-01, -1.5967e-01, -1.7225e-01,  ..., -1.7064e-01,\n                -1.7090e-01, -1.7093e-01]])\n    \"\"\"\n\n    if not self.vocab:\n        raise ValueError(\n            f'The Embedding model {self.MLM.__class__.__name__} has not been initialized'\n        )\n\n    input_ids = self.tokenizer(doc, return_tensors=\"pt\", max_length=self.max_length).input_ids\n    mask_token_index = torch.where(input_ids == self.tokenizer.mask_token_id)[1]\n    l = []\n    try:\n        with torch.no_grad():\n            logits = self.MLM(input_ids).logits\n\n        l = [logits[0, idx] for idx in mask_token_index]\n        return torch.stack(l) if len(l) &gt; 0 else torch.empty(0)\n\n    except IndexError:\n        raise ValueError(f'The mask falls outside of the max length of {self.max_length}, please use a smaller sentence')\n</code></pre>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaEmbedding.infer_vector","title":"<code>infer_vector(doc, main_word)</code>","text":"<p>This method is used to infer the vector embeddings of a word from a sentence. Args:     doc: Document to process     main_word: Main work to extract the vector embeddings for.</p> <p>Returns:</p> Name Type Description <code>embeddings</code> <code>Tensor</code> <p>Tensor of stacked embeddings (torch.Tensor) of shape (num_embeddings, embedding_size) where num_embeddings is the number of times the main_word appears in the doc.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = RobertaEmbedding()\n&gt;&gt;&gt; model.infer_vector(doc=\"The brown fox jumps over the lazy dog\", main_word=\"fox\")\ntensor([[-0.2182, -0.1597, -0.1723,  ..., -0.1706, -0.1709, -0.1709],\n        [-0.2182, -0.1597, -0.1723,  ..., -0.1706, -0.1709, -0.1709],\n        [-0.2182, -0.1597, -0.1723,  ..., -0.1706, -0.1709, -0.1709],\n        [-0.2182, -0.1597, -0.1723,  ..., -0.1706, -0.1709, -0.1709],\n        [-0.2182, -0.1597, -0.1723,  ..., -0.1706, -0.1709, -0.1709]])\n</code></pre> Source code in <code>semantics/feature_extraction/roberta.py</code> <pre><code>def infer_vector(self, doc:str, main_word:str) -&gt; torch.Tensor:\n    \"\"\"\n    This method is used to infer the vector embeddings of a word from a sentence.\n    Args:\n        doc: Document to process\n        main_word: Main work to extract the vector embeddings for.\n\n    Returns: \n        embeddings: Tensor of stacked embeddings (torch.Tensor) of shape (num_embeddings, embedding_size) where num_embeddings is the number of times the main_word appears in the doc.\n\n    Examples:\n        &gt;&gt;&gt; model = RobertaEmbedding()\n        &gt;&gt;&gt; model.infer_vector(doc=\"The brown fox jumps over the lazy dog\", main_word=\"fox\")\n        tensor([[-0.2182, -0.1597, -0.1723,  ..., -0.1706, -0.1709, -0.1709],\n                [-0.2182, -0.1597, -0.1723,  ..., -0.1706, -0.1709, -0.1709],\n                [-0.2182, -0.1597, -0.1723,  ..., -0.1706, -0.1709, -0.1709],\n                [-0.2182, -0.1597, -0.1723,  ..., -0.1706, -0.1709, -0.1709],\n                [-0.2182, -0.1597, -0.1723,  ..., -0.1706, -0.1709, -0.1709]])\n    \"\"\"\n    if not self.vocab:\n        raise ValueError(\n            f'The Embedding model {self.model.__class__.__name__} has not been initialized'\n        )\n\n\n    input_ids = self.tokenizer(doc, return_tensors=\"pt\", max_length=self.max_length).input_ids\n    token = self.tokenizer.encode(main_word, add_special_tokens=False)[0]\n\n    word_token_index = torch.where(input_ids == token)[1]\n    emb = []\n\n    try:\n        with torch.no_grad():\n            embeddings = self.model(input_ids).last_hidden_state\n\n        emb = [embeddings[0, idx] for idx in word_token_index]\n        return torch.stack(emb)\n\n    except:\n        print(f'The word: \"{main_word}\" does not exist in the list of tokens')\n        return torch.tensor(np.array(emb))\n</code></pre>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaInference","title":"<code>RobertaInference</code>","text":"<p>This class is used to infer vector embeddings from a sentence.</p>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaInference--methods","title":"Methods","text":"<pre><code>__init__(pretrained_model_path:Union[str, Path] = None)\n    The constructor for the VectorEmbeddings class.\n_roberta_case_preparation()\n    This method is used to prepare the Roberta model for the inference.\nget_embedding(word:str, sentence:str)\n    This method is used to infer the vector embeddings of a word from a sentence.\nget_top_k_words(word:str, sentence:str, k:int=3)\n    This method is used to infer the vector embeddings of a word from a sentence.\n</code></pre> Source code in <code>semantics/feature_extraction/roberta.py</code> <pre><code>class RobertaInference:\n    \"\"\"\n    This class is used to infer vector embeddings from a sentence.\n\n    Methods\n    -------\n        __init__(pretrained_model_path:Union[str, Path] = None)\n            The constructor for the VectorEmbeddings class.\n        _roberta_case_preparation()\n            This method is used to prepare the Roberta model for the inference.\n        get_embedding(word:str, sentence:str)\n            This method is used to infer the vector embeddings of a word from a sentence.\n        get_top_k_words(word:str, sentence:str, k:int=3)\n            This method is used to infer the vector embeddings of a word from a sentence.\n    \"\"\"\n\n    def __init__(\n            self,\n            pretrained_model_path:Union[str, Path] = None,\n    ):\n        self.model_path = pretrained_model_path\n        if pretrained_model_path is not None:\n            if not os.path.exists(pretrained_model_path):\n                raise ValueError(\n                    f'The path {pretrained_model_path} does not exist'\n                )\n            self.model_path = Path(pretrained_model_path)\n\n        self.word_vectorizor = None\n        self.vocab = False\n\n\n        lg.set_verbosity_error()\n        self._roberta_case_preparation()\n\n\n    def _roberta_case_preparation(self) -&gt; None:\n        \"\"\"\n        This method is used to prepare the Roberta model for the inference.\n        \"\"\"\n        model_path = self.model_path if self.model_path is not None else 'roberta-base'\n        self.tokenizer = RobertaTokenizer.from_pretrained(model_path)\n        self.word_vectorizor = RobertaEmbedding(pretrained_model_path=model_path)\n        self.vocab = True\n\n\n    def get_embedding(\n            self,\n            word : str, \n            sentence: Union[str, List[str]] = None,\n            mask : bool = False\n            ) -&gt; torch.Tensor:\n\n        \"\"\"\n        This method is used to infer the vector embeddings of a word from a sentence.\n        Args:\n            word: Word to get the vector embeddings for\n            sentence: Sentence to get the vector embeddings from. If None, the word is assumed to be in the sentence. Defaults to None.\n            mask: Whether to mask the word in the sentence or not. Defaults to False.\n\n        Returns: \n            embeddings: Tensor of stacked embeddings (torch.Tensor) of shape (num_embeddings, embedding_size) where num_embeddings is the number of times the main_word appears in the doc, depending on the mask parameter.\n\n        Examples:\n            &gt;&gt;&gt; model = RobertaInference()\n            &gt;&gt;&gt; model.get_embedding(word=\"office\", sentence=\"The brown office is very big\")\n            Sentence:  The brown office is very big\n\n            &gt;&gt;&gt; model.get_embedding(word=\"office\", sentence=\"The brown office is very big\", mask=True)\n            Sentence:  The brown &lt;mask&gt; is very big\n        \"\"\"\n\n        if not self.vocab:\n            raise ValueError(\n                f'The Embedding model {self.model.__class__.__name__} has not been initialized'\n            )\n\n        if sentence is None:\n            sentence = ' ' + word.strip() + ' '\n\n        if mask:\n            sentence = sentence.replace(word, self.tokenizer.mask_token)\n            word = self.tokenizer.mask_token\n\n        else:\n            word = ' ' + word.strip()\n\n        embeddings = self.word_vectorizor.infer_vector(doc=sentence, main_word=word)\n        return embeddings\n\n    def get_top_k_words(\n            self,\n            word : str,\n            sentence: str,\n            k: int = 3\n            ) -&gt; List[str]:\n        \"\"\"\n        This method is used to infer the vector embeddings of a word from a sentence.\n        Args:\n            word: Word to mask\n            sentence: Sentence to mask the word in\n            k: Number of top words to return\n\n        Returns:\n            top_k_words (List[str]): List of top k words\n\n        Examples:\n            &gt;&gt;&gt; model = RobertaInference()\n            &gt;&gt;&gt; model.get_top_k_words(word=\"office\", sentence=\"The brown office is very big\")\n            ['room', 'eye', 'bear']\n        \"\"\"\n        if not self.vocab:\n            raise ValueError(\n                f'The Embedding model {self.model.__class__.__name__} has not been initialized'\n            )\n\n        masked_sentence = sentence.replace(word, '&lt;mask&gt;')\n        try:\n            logits = self.word_vectorizor.infer_mask_logits(doc=masked_sentence)\n\n            top_k = []\n\n            for logit_set in logits:\n                top_k_tokens = torch.topk(logit_set, k).indices\n                top_k_words = [self.tokenizer.decode(token.item()).strip() for token in top_k_tokens]\n\n                top_k.extend(top_k_words)\n\n            return top_k\n\n        except ValueError:\n            return []\n</code></pre>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaInference.get_embedding","title":"<code>get_embedding(word, sentence=None, mask=False)</code>","text":"<p>This method is used to infer the vector embeddings of a word from a sentence. Args:     word: Word to get the vector embeddings for     sentence: Sentence to get the vector embeddings from. If None, the word is assumed to be in the sentence. Defaults to None.     mask: Whether to mask the word in the sentence or not. Defaults to False.</p> <p>Returns:</p> Name Type Description <code>embeddings</code> <code>Tensor</code> <p>Tensor of stacked embeddings (torch.Tensor) of shape (num_embeddings, embedding_size) where num_embeddings is the number of times the main_word appears in the doc, depending on the mask parameter.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = RobertaInference()\n&gt;&gt;&gt; model.get_embedding(word=\"office\", sentence=\"The brown office is very big\")\nSentence:  The brown office is very big\n</code></pre> <pre><code>&gt;&gt;&gt; model.get_embedding(word=\"office\", sentence=\"The brown office is very big\", mask=True)\nSentence:  The brown &lt;mask&gt; is very big\n</code></pre> Source code in <code>semantics/feature_extraction/roberta.py</code> <pre><code>def get_embedding(\n        self,\n        word : str, \n        sentence: Union[str, List[str]] = None,\n        mask : bool = False\n        ) -&gt; torch.Tensor:\n\n    \"\"\"\n    This method is used to infer the vector embeddings of a word from a sentence.\n    Args:\n        word: Word to get the vector embeddings for\n        sentence: Sentence to get the vector embeddings from. If None, the word is assumed to be in the sentence. Defaults to None.\n        mask: Whether to mask the word in the sentence or not. Defaults to False.\n\n    Returns: \n        embeddings: Tensor of stacked embeddings (torch.Tensor) of shape (num_embeddings, embedding_size) where num_embeddings is the number of times the main_word appears in the doc, depending on the mask parameter.\n\n    Examples:\n        &gt;&gt;&gt; model = RobertaInference()\n        &gt;&gt;&gt; model.get_embedding(word=\"office\", sentence=\"The brown office is very big\")\n        Sentence:  The brown office is very big\n\n        &gt;&gt;&gt; model.get_embedding(word=\"office\", sentence=\"The brown office is very big\", mask=True)\n        Sentence:  The brown &lt;mask&gt; is very big\n    \"\"\"\n\n    if not self.vocab:\n        raise ValueError(\n            f'The Embedding model {self.model.__class__.__name__} has not been initialized'\n        )\n\n    if sentence is None:\n        sentence = ' ' + word.strip() + ' '\n\n    if mask:\n        sentence = sentence.replace(word, self.tokenizer.mask_token)\n        word = self.tokenizer.mask_token\n\n    else:\n        word = ' ' + word.strip()\n\n    embeddings = self.word_vectorizor.infer_vector(doc=sentence, main_word=word)\n    return embeddings\n</code></pre>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaInference.get_top_k_words","title":"<code>get_top_k_words(word, sentence, k=3)</code>","text":"<p>This method is used to infer the vector embeddings of a word from a sentence. Args:     word: Word to mask     sentence: Sentence to mask the word in     k: Number of top words to return</p> <p>Returns:</p> Name Type Description <code>top_k_words</code> <code>List[str]</code> <p>List of top k words</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = RobertaInference()\n&gt;&gt;&gt; model.get_top_k_words(word=\"office\", sentence=\"The brown office is very big\")\n['room', 'eye', 'bear']\n</code></pre> Source code in <code>semantics/feature_extraction/roberta.py</code> <pre><code>def get_top_k_words(\n        self,\n        word : str,\n        sentence: str,\n        k: int = 3\n        ) -&gt; List[str]:\n    \"\"\"\n    This method is used to infer the vector embeddings of a word from a sentence.\n    Args:\n        word: Word to mask\n        sentence: Sentence to mask the word in\n        k: Number of top words to return\n\n    Returns:\n        top_k_words (List[str]): List of top k words\n\n    Examples:\n        &gt;&gt;&gt; model = RobertaInference()\n        &gt;&gt;&gt; model.get_top_k_words(word=\"office\", sentence=\"The brown office is very big\")\n        ['room', 'eye', 'bear']\n    \"\"\"\n    if not self.vocab:\n        raise ValueError(\n            f'The Embedding model {self.model.__class__.__name__} has not been initialized'\n        )\n\n    masked_sentence = sentence.replace(word, '&lt;mask&gt;')\n    try:\n        logits = self.word_vectorizor.infer_mask_logits(doc=masked_sentence)\n\n        top_k = []\n\n        for logit_set in logits:\n            top_k_tokens = torch.topk(logit_set, k).indices\n            top_k_words = [self.tokenizer.decode(token.item()).strip() for token in top_k_tokens]\n\n            top_k.extend(top_k_words)\n\n        return top_k\n\n    except ValueError:\n        return []\n</code></pre>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaTrainer","title":"<code>RobertaTrainer</code>","text":"<p>This class is used to train a Roberta model.</p>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaTrainer--methods","title":"Methods","text":"<pre><code>__init__(model_name=\"roberta-base\", max_length=128, mlm_probability=0.15, batch_size=4, learning_rate=1e-5, epochs=3, warmup_steps=500, split_ratio=0.8)\n    The constructor for the RobertaTrainer class.\nprepare_dataset(data)\n    This method is used to prepare the dataset for training.\ntrain(data, output_dir: Union[str, Path] = None)\n    This method is used to train the model.\n</code></pre> Source code in <code>semantics/feature_extraction/roberta.py</code> <pre><code>class RobertaTrainer:\n    \"\"\"\n    This class is used to train a Roberta model.\n\n    Methods\n    -------\n        __init__(model_name=\"roberta-base\", max_length=128, mlm_probability=0.15, batch_size=4, learning_rate=1e-5, epochs=3, warmup_steps=500, split_ratio=0.8)\n            The constructor for the RobertaTrainer class.\n        prepare_dataset(data)\n            This method is used to prepare the dataset for training.\n        train(data, output_dir: Union[str, Path] = None)\n            This method is used to train the model.\n    \"\"\"\n    def __init__(\n            self, \n            model_name: str = \"roberta-base\", \n            max_length: int = 128, \n            mlm_probability: float = 0.15, \n            batch_size: int = 4, \n            learning_rate: float = 1e-5, \n            epochs: int = 3, \n            warmup_steps: int = 500, \n            split_ratio: float = 0.8, \n            truncation: bool = True, \n            padding: str = \"max_length\"\n            ):\n\n        \"\"\"\n        Args:\n            model_name (str): Name of the model to train. Defaults to \"roberta-base\".\n            max_length (int): Maximum length of the input sequence. Defaults to 128.\n            mlm_probability (float): Probability of masking tokens in the input sequence. Defaults to 0.15.\n            batch_size (int): Size of the batch. Defaults to 4.\n            learning_rate (float): Learning rate of the optimizer. Defaults to 1e-5.\n            epochs (int): Number of epochs to train the model for. Defaults to 3.\n            warmup_steps (int): Number of warmup steps for the learning rate scheduler. Defaults to 500.\n            split_ratio (float): Ratio to split the data into train and test. Defaults to 0.8.\n            truncation (bool): Whether to truncate the input sequence to max_length or not. Defaults to True.\n            padding (str): Whether to pad the input sequence to max_length or not. Defaults to \"max_length\".\n        \"\"\"\n\n\n        self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n        self.model = RobertaForMaskedLM.from_pretrained(model_name)\n\n        self.data_collator = DataCollatorForLanguageModeling(\n            tokenizer=self.tokenizer, \n            mlm=True, \n            mlm_probability=mlm_probability\n            )\n\n        self.split_ratio = split_ratio\n        self.truncation = truncation\n        self.padding = padding\n        self.max_length = max_length\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.warmup_steps = warmup_steps\n        self.accelerator = Accelerator()\n\n    def prepare_dataset(self, data: List[str]):\n        \"\"\"\n        This method is used to prepare the dataset for training.\n        Args:\n            data: List of strings to train the model on.\n\n        Returns:\n            train_loader (DataLoader): DataLoader object containing the training data.\n            dataset (CustomDataset): CustomDataset object containing the training data.\n        \"\"\"\n        dataset = CustomDataset(\n            data, \n            self.tokenizer, \n            max_length=self.max_length,\n            truncation=self.truncation,\n            padding=self.padding\n            )\n\n        train_loader = DataLoader(\n            dataset, \n            batch_size=self.batch_size, \n            shuffle=True, \n            collate_fn=self.data_collator\n            )\n\n        return train_loader, dataset\n\n    def train(\n            self, \n            data: List[str],\n            output_dir: Optional[Union[str, Path]] = None\n            ) -&gt; None:\n        \"\"\"\n        This method is used to train the model.\n        Args:\n            data (List[str]): List of strings to train the model on.\n            output_dir (str, Path, None): Path to save the model to. Defaults to None.\n        \"\"\"\n\n        train_data, test_data = train_test_split(\n            data, \n            test_ratio=1 - self.split_ratio, \n            random_seed=42\n            )\n\n        train_loader, _ = self.prepare_dataset(train_data)\n        test_loader, _ = self.prepare_dataset(test_data)\n\n        optimizer = optim.AdamW(\n            self.model.parameters(), \n            lr=self.learning_rate\n            )\n\n        model, optimizer, train_loader, test_loader = self.accelerator.prepare(\n            self.model, \n            optimizer, \n            train_loader, \n            test_loader\n            )\n\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, \n            num_warmup_steps=self.warmup_steps, \n            num_training_steps=len(train_loader) * self.epochs\n            )\n\n        progress_bar = tqdm.tqdm(\n            range(len(train_loader) * self.epochs), \n            desc=\"Training\", \n            dynamic_ncols=True\n            )\n\n        for epoch in range(self.epochs):\n            self.model.train()\n\n            for batch in train_loader:\n                outputs = self.model(**batch)\n                loss = outputs.loss\n                self.accelerator.backward(loss)\n                optimizer.step()\n                scheduler.step()  # Update learning rate scheduler\n                optimizer.zero_grad()\n                progress_bar.update(1)\n\n            self.model.eval()\n            losses = []\n            for step, batch in enumerate(test_loader):\n                with torch.no_grad():\n                    outputs = self.model(**batch)\n\n                loss = outputs.loss\n                losses.append(self.accelerator.gather(loss.repeat(self.batch_size)))\n\n            losses = torch.cat(losses)\n            losses = losses[:len(test_data)]\n\n            try:\n                perplexity = math.exp(torch.mean(losses))\n            except OverflowError:\n                perplexity = float(\"inf\")\n            print(f\"Epoch: {epoch} | Loss: {torch.mean(losses)} | Perplexity: {perplexity}\")\n\n            # Save model\n            if output_dir is not None:\n                self.accelerator.wait_for_everyone()\n                unwrapped_model = self.accelerator.unwrap_model(model)\n                unwrapped_model.save_pretrained(output_dir, save_function=self.accelerator.save)\n                if self.accelerator.is_main_process:\n                    self.tokenizer.save_pretrained(output_dir)\n</code></pre>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaTrainer.__init__","title":"<code>__init__(model_name='roberta-base', max_length=128, mlm_probability=0.15, batch_size=4, learning_rate=1e-05, epochs=3, warmup_steps=500, split_ratio=0.8, truncation=True, padding='max_length')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to train. Defaults to \"roberta-base\".</p> <code>'roberta-base'</code> <code>max_length</code> <code>int</code> <p>Maximum length of the input sequence. Defaults to 128.</p> <code>128</code> <code>mlm_probability</code> <code>float</code> <p>Probability of masking tokens in the input sequence. Defaults to 0.15.</p> <code>0.15</code> <code>batch_size</code> <code>int</code> <p>Size of the batch. Defaults to 4.</p> <code>4</code> <code>learning_rate</code> <code>float</code> <p>Learning rate of the optimizer. Defaults to 1e-5.</p> <code>1e-05</code> <code>epochs</code> <code>int</code> <p>Number of epochs to train the model for. Defaults to 3.</p> <code>3</code> <code>warmup_steps</code> <code>int</code> <p>Number of warmup steps for the learning rate scheduler. Defaults to 500.</p> <code>500</code> <code>split_ratio</code> <code>float</code> <p>Ratio to split the data into train and test. Defaults to 0.8.</p> <code>0.8</code> <code>truncation</code> <code>bool</code> <p>Whether to truncate the input sequence to max_length or not. Defaults to True.</p> <code>True</code> <code>padding</code> <code>str</code> <p>Whether to pad the input sequence to max_length or not. Defaults to \"max_length\".</p> <code>'max_length'</code> Source code in <code>semantics/feature_extraction/roberta.py</code> <pre><code>def __init__(\n        self, \n        model_name: str = \"roberta-base\", \n        max_length: int = 128, \n        mlm_probability: float = 0.15, \n        batch_size: int = 4, \n        learning_rate: float = 1e-5, \n        epochs: int = 3, \n        warmup_steps: int = 500, \n        split_ratio: float = 0.8, \n        truncation: bool = True, \n        padding: str = \"max_length\"\n        ):\n\n    \"\"\"\n    Args:\n        model_name (str): Name of the model to train. Defaults to \"roberta-base\".\n        max_length (int): Maximum length of the input sequence. Defaults to 128.\n        mlm_probability (float): Probability of masking tokens in the input sequence. Defaults to 0.15.\n        batch_size (int): Size of the batch. Defaults to 4.\n        learning_rate (float): Learning rate of the optimizer. Defaults to 1e-5.\n        epochs (int): Number of epochs to train the model for. Defaults to 3.\n        warmup_steps (int): Number of warmup steps for the learning rate scheduler. Defaults to 500.\n        split_ratio (float): Ratio to split the data into train and test. Defaults to 0.8.\n        truncation (bool): Whether to truncate the input sequence to max_length or not. Defaults to True.\n        padding (str): Whether to pad the input sequence to max_length or not. Defaults to \"max_length\".\n    \"\"\"\n\n\n    self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n    self.model = RobertaForMaskedLM.from_pretrained(model_name)\n\n    self.data_collator = DataCollatorForLanguageModeling(\n        tokenizer=self.tokenizer, \n        mlm=True, \n        mlm_probability=mlm_probability\n        )\n\n    self.split_ratio = split_ratio\n    self.truncation = truncation\n    self.padding = padding\n    self.max_length = max_length\n    self.batch_size = batch_size\n    self.learning_rate = learning_rate\n    self.epochs = epochs\n    self.warmup_steps = warmup_steps\n    self.accelerator = Accelerator()\n</code></pre>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaTrainer.prepare_dataset","title":"<code>prepare_dataset(data)</code>","text":"<p>This method is used to prepare the dataset for training. Args:     data: List of strings to train the model on.</p> <p>Returns:</p> Name Type Description <code>train_loader</code> <code>DataLoader</code> <p>DataLoader object containing the training data.</p> <code>dataset</code> <code>CustomDataset</code> <p>CustomDataset object containing the training data.</p> Source code in <code>semantics/feature_extraction/roberta.py</code> <pre><code>def prepare_dataset(self, data: List[str]):\n    \"\"\"\n    This method is used to prepare the dataset for training.\n    Args:\n        data: List of strings to train the model on.\n\n    Returns:\n        train_loader (DataLoader): DataLoader object containing the training data.\n        dataset (CustomDataset): CustomDataset object containing the training data.\n    \"\"\"\n    dataset = CustomDataset(\n        data, \n        self.tokenizer, \n        max_length=self.max_length,\n        truncation=self.truncation,\n        padding=self.padding\n        )\n\n    train_loader = DataLoader(\n        dataset, \n        batch_size=self.batch_size, \n        shuffle=True, \n        collate_fn=self.data_collator\n        )\n\n    return train_loader, dataset\n</code></pre>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaTrainer.train","title":"<code>train(data, output_dir=None)</code>","text":"<p>This method is used to train the model. Args:     data (List[str]): List of strings to train the model on.     output_dir (str, Path, None): Path to save the model to. Defaults to None.</p> Source code in <code>semantics/feature_extraction/roberta.py</code> <pre><code>def train(\n        self, \n        data: List[str],\n        output_dir: Optional[Union[str, Path]] = None\n        ) -&gt; None:\n    \"\"\"\n    This method is used to train the model.\n    Args:\n        data (List[str]): List of strings to train the model on.\n        output_dir (str, Path, None): Path to save the model to. Defaults to None.\n    \"\"\"\n\n    train_data, test_data = train_test_split(\n        data, \n        test_ratio=1 - self.split_ratio, \n        random_seed=42\n        )\n\n    train_loader, _ = self.prepare_dataset(train_data)\n    test_loader, _ = self.prepare_dataset(test_data)\n\n    optimizer = optim.AdamW(\n        self.model.parameters(), \n        lr=self.learning_rate\n        )\n\n    model, optimizer, train_loader, test_loader = self.accelerator.prepare(\n        self.model, \n        optimizer, \n        train_loader, \n        test_loader\n        )\n\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=self.warmup_steps, \n        num_training_steps=len(train_loader) * self.epochs\n        )\n\n    progress_bar = tqdm.tqdm(\n        range(len(train_loader) * self.epochs), \n        desc=\"Training\", \n        dynamic_ncols=True\n        )\n\n    for epoch in range(self.epochs):\n        self.model.train()\n\n        for batch in train_loader:\n            outputs = self.model(**batch)\n            loss = outputs.loss\n            self.accelerator.backward(loss)\n            optimizer.step()\n            scheduler.step()  # Update learning rate scheduler\n            optimizer.zero_grad()\n            progress_bar.update(1)\n\n        self.model.eval()\n        losses = []\n        for step, batch in enumerate(test_loader):\n            with torch.no_grad():\n                outputs = self.model(**batch)\n\n            loss = outputs.loss\n            losses.append(self.accelerator.gather(loss.repeat(self.batch_size)))\n\n        losses = torch.cat(losses)\n        losses = losses[:len(test_data)]\n\n        try:\n            perplexity = math.exp(torch.mean(losses))\n        except OverflowError:\n            perplexity = float(\"inf\")\n        print(f\"Epoch: {epoch} | Loss: {torch.mean(losses)} | Perplexity: {perplexity}\")\n\n        # Save model\n        if output_dir is not None:\n            self.accelerator.wait_for_everyone()\n            unwrapped_model = self.accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(output_dir, save_function=self.accelerator.save)\n            if self.accelerator.is_main_process:\n                self.tokenizer.save_pretrained(output_dir)\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/","title":"Word2Vec","text":""},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecAlign","title":"<code>Word2VecAlign</code>","text":"Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>class Word2VecAlign:\n    def __init__(\n            self, \n            model_paths: List[str],\n\n            ):\n        \"\"\"\n        Aligns multiple Word2Vec models.\n\n        Parameters\n        ----------\n        model_paths : List[str]\n            List of paths to the models\n\n        Attributes\n        ----------\n        model_paths : List[str]\n            List of paths to the models\n        reference_model : gensim.models.Word2Vec\n            The reference model\n        models : List[gensim.models.Word2Vec]\n            List of models\n        model_names : List[str]\n            List of model names\n        aligned_models : List[gensim.models.Word2Vec]\n            List of aligned models\n\n        Methods\n        -------\n        load_models()\n            Load the models\n        align_models(reference_index, output_dir, method)\n            Align the models        \n        \"\"\"\n        self.model_paths = model_paths\n        self.reference_model = None\n        self.models = []\n        self.model_names = [Path(model_path).stem for model_path in model_paths]\n        self.aligned_models = []\n\n        self.load_models()\n\n    def load_models(self) -&gt; None:\n        \"\"\"\n        Load the models\n        \"\"\"\n        for model_path in self.model_paths:\n            self.models.append(Word2Vec.load(model_path))\n\n    def align_models(\n            self,\n            reference_index: int = -1,\n            output_dir: Optional[str] = None,\n            method: str = \"procrustes\",\n            ) -&gt; List[Word2Vec]:\n        \"\"\"\n        Align the models\n\n        Parameters\n        ----------\n        reference_index : int, optional\n            Index of the reference model, by default -1\n        output_dir : str, optional\n            Path to save the aligned models, by default None\n        method : str, optional\n            Alignment method, by default \"procrustes\"\n\n        Returns\n        -------\n        List[Word2Vec]\n            List of aligned models\n        \"\"\"\n\n        if method != \"procrustes\":\n            raise NotImplementedError(\"Only procrustes alignment is implemented. Please use method='procrustes'\")\n\n\n        self.reference_model = self.models[reference_index]\n        self.reference_model.save(f\"{output_dir}/{self.model_names[reference_index]}_aligned.model\")\n        self.aligned_models.append(self.reference_model)\n        self.models.pop(reference_index)\n\n        for i, model in enumerate(self.models):\n            aligned_model = smart_procrustes_align_gensim(self.reference_model,model)\n            aligned_model.save(f\"{output_dir}/{self.model_names[i]}_aligned.model\")\n            self.aligned_models.append(aligned_model)\n\n        return self.aligned_models\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecAlign.__init__","title":"<code>__init__(model_paths)</code>","text":"<p>Aligns multiple Word2Vec models.</p>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecAlign.__init__--parameters","title":"Parameters","text":"<p>model_paths : List[str]     List of paths to the models</p>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecAlign.__init__--attributes","title":"Attributes","text":"<p>model_paths : List[str]     List of paths to the models reference_model : gensim.models.Word2Vec     The reference model models : List[gensim.models.Word2Vec]     List of models model_names : List[str]     List of model names aligned_models : List[gensim.models.Word2Vec]     List of aligned models</p>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecAlign.__init__--methods","title":"Methods","text":"<p>load_models()     Load the models align_models(reference_index, output_dir, method)     Align the models</p> Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>def __init__(\n        self, \n        model_paths: List[str],\n\n        ):\n    \"\"\"\n    Aligns multiple Word2Vec models.\n\n    Parameters\n    ----------\n    model_paths : List[str]\n        List of paths to the models\n\n    Attributes\n    ----------\n    model_paths : List[str]\n        List of paths to the models\n    reference_model : gensim.models.Word2Vec\n        The reference model\n    models : List[gensim.models.Word2Vec]\n        List of models\n    model_names : List[str]\n        List of model names\n    aligned_models : List[gensim.models.Word2Vec]\n        List of aligned models\n\n    Methods\n    -------\n    load_models()\n        Load the models\n    align_models(reference_index, output_dir, method)\n        Align the models        \n    \"\"\"\n    self.model_paths = model_paths\n    self.reference_model = None\n    self.models = []\n    self.model_names = [Path(model_path).stem for model_path in model_paths]\n    self.aligned_models = []\n\n    self.load_models()\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecAlign.align_models","title":"<code>align_models(reference_index=-1, output_dir=None, method='procrustes')</code>","text":"<p>Align the models</p>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecAlign.align_models--parameters","title":"Parameters","text":"<p>reference_index : int, optional     Index of the reference model, by default -1 output_dir : str, optional     Path to save the aligned models, by default None method : str, optional     Alignment method, by default \"procrustes\"</p>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecAlign.align_models--returns","title":"Returns","text":"<p>List[Word2Vec]     List of aligned models</p> Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>def align_models(\n        self,\n        reference_index: int = -1,\n        output_dir: Optional[str] = None,\n        method: str = \"procrustes\",\n        ) -&gt; List[Word2Vec]:\n    \"\"\"\n    Align the models\n\n    Parameters\n    ----------\n    reference_index : int, optional\n        Index of the reference model, by default -1\n    output_dir : str, optional\n        Path to save the aligned models, by default None\n    method : str, optional\n        Alignment method, by default \"procrustes\"\n\n    Returns\n    -------\n    List[Word2Vec]\n        List of aligned models\n    \"\"\"\n\n    if method != \"procrustes\":\n        raise NotImplementedError(\"Only procrustes alignment is implemented. Please use method='procrustes'\")\n\n\n    self.reference_model = self.models[reference_index]\n    self.reference_model.save(f\"{output_dir}/{self.model_names[reference_index]}_aligned.model\")\n    self.aligned_models.append(self.reference_model)\n    self.models.pop(reference_index)\n\n    for i, model in enumerate(self.models):\n        aligned_model = smart_procrustes_align_gensim(self.reference_model,model)\n        aligned_model.save(f\"{output_dir}/{self.model_names[i]}_aligned.model\")\n        self.aligned_models.append(aligned_model)\n\n    return self.aligned_models\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecAlign.load_models","title":"<code>load_models()</code>","text":"<p>Load the models</p> Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>def load_models(self) -&gt; None:\n    \"\"\"\n    Load the models\n    \"\"\"\n    for model_path in self.model_paths:\n        self.models.append(Word2Vec.load(model_path))\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecEmbeddings","title":"<code>Word2VecEmbeddings</code>","text":"Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>class Word2VecEmbeddings:\n    def __init__(\n            self,\n            pretrained_model_path: Optional[str] = None,\n            ):\n        \"\"\"\n        Wrapper class for gensim.models.Word2Vec\n\n        Parameters\n        ----------\n        pretrained_model_path : str\n            Path to a pretrained model, by default None\n\n        Attributes\n        ----------\n        model_path : str\n            Path to the pretrained model\n        model : gensim.models.Word2Vec\n            The Word2Vec model\n        vocab : bool\n            Whether the model has been initialized\n\n        Methods\n        -------\n        _word2vec_case_preparation()\n            Prepare the Word2Vec model\n        infer_vector(word, norm)\n            Infer the vector of a word\n        \"\"\"\n        self.model_path = pretrained_model_path\n        if pretrained_model_path is not None:\n            if not os.path.exists(pretrained_model_path):\n                raise ValueError(\n                    f\"Model path {pretrained_model_path} does not exist.\"\n                )\n            self.model_path = pretrained_model_path\n\n        self.model = None\n        self.vocab = False\n\n        self._word2vec_case_preparation()\n\n    def _word2vec_case_preparation(self) -&gt; None:\n        \"\"\"\n        Prepare the Word2Vec model\n        \"\"\"\n        if self.model_path is None:\n            self.model = Word2Vec()\n        else:\n            self.model = Word2Vec.load(self.model_path)\n        self.vocab = True\n\n    def infer_vector(self, word:str, norm = False) -&gt; List[float]:\n        \"\"\"\n        Infer the vector of a word\n\n        Parameters\n        ----------\n        word : str\n            The word\n        norm : bool, optional\n            Whether to normalize the vector, by default False\n\n        Returns\n        -------\n        List[float]\n            The vector of the word\n        \"\"\"\n        if not self.vocab:\n            raise ValueError(\n                f'The Embedding model {self.model.__class__.__name__} has not been initialized'\n            )\n        return self.model.wv.get_vector(word, norm = norm)\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecEmbeddings.__init__","title":"<code>__init__(pretrained_model_path=None)</code>","text":"<p>Wrapper class for gensim.models.Word2Vec</p>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecEmbeddings.__init__--parameters","title":"Parameters","text":"<p>pretrained_model_path : str     Path to a pretrained model, by default None</p>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecEmbeddings.__init__--attributes","title":"Attributes","text":"<p>model_path : str     Path to the pretrained model model : gensim.models.Word2Vec     The Word2Vec model vocab : bool     Whether the model has been initialized</p>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecEmbeddings.__init__--methods","title":"Methods","text":"<p>_word2vec_case_preparation()     Prepare the Word2Vec model infer_vector(word, norm)     Infer the vector of a word</p> Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>def __init__(\n        self,\n        pretrained_model_path: Optional[str] = None,\n        ):\n    \"\"\"\n    Wrapper class for gensim.models.Word2Vec\n\n    Parameters\n    ----------\n    pretrained_model_path : str\n        Path to a pretrained model, by default None\n\n    Attributes\n    ----------\n    model_path : str\n        Path to the pretrained model\n    model : gensim.models.Word2Vec\n        The Word2Vec model\n    vocab : bool\n        Whether the model has been initialized\n\n    Methods\n    -------\n    _word2vec_case_preparation()\n        Prepare the Word2Vec model\n    infer_vector(word, norm)\n        Infer the vector of a word\n    \"\"\"\n    self.model_path = pretrained_model_path\n    if pretrained_model_path is not None:\n        if not os.path.exists(pretrained_model_path):\n            raise ValueError(\n                f\"Model path {pretrained_model_path} does not exist.\"\n            )\n        self.model_path = pretrained_model_path\n\n    self.model = None\n    self.vocab = False\n\n    self._word2vec_case_preparation()\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecEmbeddings.infer_vector","title":"<code>infer_vector(word, norm=False)</code>","text":"<p>Infer the vector of a word</p>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecEmbeddings.infer_vector--parameters","title":"Parameters","text":"<p>word : str     The word norm : bool, optional     Whether to normalize the vector, by default False</p>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecEmbeddings.infer_vector--returns","title":"Returns","text":"<p>List[float]     The vector of the word</p> Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>def infer_vector(self, word:str, norm = False) -&gt; List[float]:\n    \"\"\"\n    Infer the vector of a word\n\n    Parameters\n    ----------\n    word : str\n        The word\n    norm : bool, optional\n        Whether to normalize the vector, by default False\n\n    Returns\n    -------\n    List[float]\n        The vector of the word\n    \"\"\"\n    if not self.vocab:\n        raise ValueError(\n            f'The Embedding model {self.model.__class__.__name__} has not been initialized'\n        )\n    return self.model.wv.get_vector(word, norm = norm)\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecInference","title":"<code>Word2VecInference</code>","text":"Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>class Word2VecInference:\n    def __init__(\n            self,\n            pretrained_model_path: Optional[str] = None,\n            ):\n        \"\"\"\n        Wrapper class for gensim.models.Word2Vec\n\n        Parameters\n        ----------\n        pretrained_model_path : str\n            Path to a pretrained model, by default None\n\n        Attributes\n        ----------\n        word_vectorizor : WordEmbeddings\n            The Word2Vec model\n\n        Methods\n        -------\n        get_embedding(word, norm)\n            Infer the vector of a word\n        get_similarity(word1, word2)\n            Get the cosine similarity between two words\n        get_top_k_words(word, k)\n            Get the top k most similar words to a word in the vocabulary of the model. Default k = 10  \n        \"\"\"\n        self.word_vectorizor = Word2VecEmbeddings(pretrained_model_path)\n\n    def get_embedding(self, word:str, norm: bool = False) -&gt; List[float]:\n        \"\"\"\n        Infer the vector of a word\n\n        Parameters\n        ----------\n        word : str\n            The word\n        norm : bool, optional\n            Whether to normalize the vector, by default False\n\n        Returns\n        -------\n        List[float]\n            The vector of the word\n        \"\"\"\n        return self.word_vectorizor.infer_vector(word= word, norm = norm)\n\n    def get_similarity(self, word1: str, word2: str) -&gt; float:\n        \"\"\"\n        Get the cosine similarity between two words\n\n        Parameters\n        ----------\n        word1 : str\n            The first word\n        word2 : str\n            The second word\n\n        Returns\n        -------\n        float\n            The cosine similarity between the two words\n        \"\"\"\n        return self.word_vectorizor.model.wv.similarity(word1, word2)\n\n    def get_top_k_words(\n            self,\n            word: str,\n            k: int = 10,\n            ):\n        \"\"\"\n        Get the top k most similar words to a word in the vocabulary of the model. Default k = 10\n\n        Parameters\n        ----------\n        word : str\n            The word\n        k : int, optional\n            The number of similar words to return, by default 10\n\n        Returns\n        -------\n        List[str]\n            List of similar words\n        \"\"\"\n\n        try:\n            sims = self.word_vectorizor.model.wv.most_similar(\n                word,\n                topn=k\n                )\n            return tuple(map(list, zip(*sims)))\n\n        except KeyError:\n            print(\"The word in the input is not in the model vocabulary.\")\n            return [], []\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecInference.__init__","title":"<code>__init__(pretrained_model_path=None)</code>","text":"<p>Wrapper class for gensim.models.Word2Vec</p>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecInference.__init__--parameters","title":"Parameters","text":"<p>pretrained_model_path : str     Path to a pretrained model, by default None</p>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecInference.__init__--attributes","title":"Attributes","text":"<p>word_vectorizor : WordEmbeddings     The Word2Vec model</p>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecInference.__init__--methods","title":"Methods","text":"<p>get_embedding(word, norm)     Infer the vector of a word get_similarity(word1, word2)     Get the cosine similarity between two words get_top_k_words(word, k)     Get the top k most similar words to a word in the vocabulary of the model. Default k = 10</p> Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>def __init__(\n        self,\n        pretrained_model_path: Optional[str] = None,\n        ):\n    \"\"\"\n    Wrapper class for gensim.models.Word2Vec\n\n    Parameters\n    ----------\n    pretrained_model_path : str\n        Path to a pretrained model, by default None\n\n    Attributes\n    ----------\n    word_vectorizor : WordEmbeddings\n        The Word2Vec model\n\n    Methods\n    -------\n    get_embedding(word, norm)\n        Infer the vector of a word\n    get_similarity(word1, word2)\n        Get the cosine similarity between two words\n    get_top_k_words(word, k)\n        Get the top k most similar words to a word in the vocabulary of the model. Default k = 10  \n    \"\"\"\n    self.word_vectorizor = Word2VecEmbeddings(pretrained_model_path)\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecInference.get_embedding","title":"<code>get_embedding(word, norm=False)</code>","text":"<p>Infer the vector of a word</p>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecInference.get_embedding--parameters","title":"Parameters","text":"<p>word : str     The word norm : bool, optional     Whether to normalize the vector, by default False</p>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecInference.get_embedding--returns","title":"Returns","text":"<p>List[float]     The vector of the word</p> Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>def get_embedding(self, word:str, norm: bool = False) -&gt; List[float]:\n    \"\"\"\n    Infer the vector of a word\n\n    Parameters\n    ----------\n    word : str\n        The word\n    norm : bool, optional\n        Whether to normalize the vector, by default False\n\n    Returns\n    -------\n    List[float]\n        The vector of the word\n    \"\"\"\n    return self.word_vectorizor.infer_vector(word= word, norm = norm)\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecInference.get_similarity","title":"<code>get_similarity(word1, word2)</code>","text":"<p>Get the cosine similarity between two words</p>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecInference.get_similarity--parameters","title":"Parameters","text":"<p>word1 : str     The first word word2 : str     The second word</p>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecInference.get_similarity--returns","title":"Returns","text":"<p>float     The cosine similarity between the two words</p> Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>def get_similarity(self, word1: str, word2: str) -&gt; float:\n    \"\"\"\n    Get the cosine similarity between two words\n\n    Parameters\n    ----------\n    word1 : str\n        The first word\n    word2 : str\n        The second word\n\n    Returns\n    -------\n    float\n        The cosine similarity between the two words\n    \"\"\"\n    return self.word_vectorizor.model.wv.similarity(word1, word2)\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecInference.get_top_k_words","title":"<code>get_top_k_words(word, k=10)</code>","text":"<p>Get the top k most similar words to a word in the vocabulary of the model. Default k = 10</p>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecInference.get_top_k_words--parameters","title":"Parameters","text":"<p>word : str     The word k : int, optional     The number of similar words to return, by default 10</p>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecInference.get_top_k_words--returns","title":"Returns","text":"<p>List[str]     List of similar words</p> Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>def get_top_k_words(\n        self,\n        word: str,\n        k: int = 10,\n        ):\n    \"\"\"\n    Get the top k most similar words to a word in the vocabulary of the model. Default k = 10\n\n    Parameters\n    ----------\n    word : str\n        The word\n    k : int, optional\n        The number of similar words to return, by default 10\n\n    Returns\n    -------\n    List[str]\n        List of similar words\n    \"\"\"\n\n    try:\n        sims = self.word_vectorizor.model.wv.most_similar(\n            word,\n            topn=k\n            )\n        return tuple(map(list, zip(*sims)))\n\n    except KeyError:\n        print(\"The word in the input is not in the model vocabulary.\")\n        return [], []\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecTrainer","title":"<code>Word2VecTrainer</code>","text":"Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>class Word2VecTrainer:    \n    def __init__(\n            self,\n            model_path: Optional[str] = None,\n            min_count=0,\n            window=15,\n            negative=5,\n            ns_exponent=0.75,\n            vector_size=100,\n            workers=1,\n            sg=1,\n            **kwargs\n            ):\n        \"\"\"\n        Wrapper class for gensim.models.Word2Vec\n\n        Parameters\n        ----------\n        model_path : str, optional\n            Path to a pretrained model, by default None\n            min_count : int, optional\n            window : int, optional\n            negative : int, optional\n            ns_exponent : float, optional\n            vector_size : int, optional\n            workers : int, optional\n            sg : int, optional\n            **kwargs : optional\n                Additional parameters for gensim.models.Word2Vec\n\n        Attributes\n        ----------\n        model : gensim.models.Word2Vec\n            The Word2Vec model\n\n        Methods\n        -------\n        train(data, output_path, epochs, alpha, min_alpha, compute_loss, **kwargs)\n            Train the Word2Vec model on the given data\n\n        \"\"\"\n\n        if model_path:\n            self.model = Word2Vec.load(model_path)\n        else:\n            self.model = Word2Vec(\n                    min_count=min_count,\n                    window=window,\n                    negative=negative,\n                    ns_exponent=ns_exponent,\n                    vector_size=vector_size,\n                    workers=workers,\n                    sg=sg,\n                    **kwargs\n                    )\n\n    def train(\n            self, \n            data: List[str],\n            output_dir: Optional[Union[str, Path]] = None,\n            epochs=5,\n            start_alpha=0.025,\n            end_alpha=0.0001,\n            compute_loss=True,\n            **kwargs\n            ):\n        \"\"\"\n        Train the Word2Vec model on the given data\n\n        Parameters\n        ----------\n        data : List[str]\n            List of documents\n        output_path : Union[str, Path], optional\n            Path to save the trained model, by default None\n        epochs : int, optional\n            Number of epochs, by default 5\n        start_alpha : float, optional\n            Learning rate, by default 0.025\n        end_alpha : float, optional\n            Minimum learning rate, by default 0.0001\n        compute_loss : bool, optional\n            Whether to compute the loss, by default True\n        **kwargs : optional\n            Additional parameters for gensim.models.Word2Vec.train\n        \"\"\"\n        self.model.build_vocab(data)\n        total_examples = self.model.corpus_count\n        self.model.train(\n                data,\n                total_examples=total_examples,\n                epochs=epochs,\n                start_alpha=start_alpha,\n                end_alpha=end_alpha,\n                compute_loss=compute_loss,\n                **kwargs\n                )\n        if output_dir:\n            self.model.save(output_dir)\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecTrainer.__init__","title":"<code>__init__(model_path=None, min_count=0, window=15, negative=5, ns_exponent=0.75, vector_size=100, workers=1, sg=1, **kwargs)</code>","text":"<p>Wrapper class for gensim.models.Word2Vec</p>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecTrainer.__init__--parameters","title":"Parameters","text":"<p>model_path : str, optional     Path to a pretrained model, by default None     min_count : int, optional     window : int, optional     negative : int, optional     ns_exponent : float, optional     vector_size : int, optional     workers : int, optional     sg : int, optional     **kwargs : optional         Additional parameters for gensim.models.Word2Vec</p>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecTrainer.__init__--attributes","title":"Attributes","text":"<p>model : gensim.models.Word2Vec     The Word2Vec model</p>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecTrainer.__init__--methods","title":"Methods","text":"<p>train(data, output_path, epochs, alpha, min_alpha, compute_loss, **kwargs)     Train the Word2Vec model on the given data</p> Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>def __init__(\n        self,\n        model_path: Optional[str] = None,\n        min_count=0,\n        window=15,\n        negative=5,\n        ns_exponent=0.75,\n        vector_size=100,\n        workers=1,\n        sg=1,\n        **kwargs\n        ):\n    \"\"\"\n    Wrapper class for gensim.models.Word2Vec\n\n    Parameters\n    ----------\n    model_path : str, optional\n        Path to a pretrained model, by default None\n        min_count : int, optional\n        window : int, optional\n        negative : int, optional\n        ns_exponent : float, optional\n        vector_size : int, optional\n        workers : int, optional\n        sg : int, optional\n        **kwargs : optional\n            Additional parameters for gensim.models.Word2Vec\n\n    Attributes\n    ----------\n    model : gensim.models.Word2Vec\n        The Word2Vec model\n\n    Methods\n    -------\n    train(data, output_path, epochs, alpha, min_alpha, compute_loss, **kwargs)\n        Train the Word2Vec model on the given data\n\n    \"\"\"\n\n    if model_path:\n        self.model = Word2Vec.load(model_path)\n    else:\n        self.model = Word2Vec(\n                min_count=min_count,\n                window=window,\n                negative=negative,\n                ns_exponent=ns_exponent,\n                vector_size=vector_size,\n                workers=workers,\n                sg=sg,\n                **kwargs\n                )\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecTrainer.train","title":"<code>train(data, output_dir=None, epochs=5, start_alpha=0.025, end_alpha=0.0001, compute_loss=True, **kwargs)</code>","text":"<p>Train the Word2Vec model on the given data</p>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecTrainer.train--parameters","title":"Parameters","text":"<p>data : List[str]     List of documents output_path : Union[str, Path], optional     Path to save the trained model, by default None epochs : int, optional     Number of epochs, by default 5 start_alpha : float, optional     Learning rate, by default 0.025 end_alpha : float, optional     Minimum learning rate, by default 0.0001 compute_loss : bool, optional     Whether to compute the loss, by default True **kwargs : optional     Additional parameters for gensim.models.Word2Vec.train</p> Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>def train(\n        self, \n        data: List[str],\n        output_dir: Optional[Union[str, Path]] = None,\n        epochs=5,\n        start_alpha=0.025,\n        end_alpha=0.0001,\n        compute_loss=True,\n        **kwargs\n        ):\n    \"\"\"\n    Train the Word2Vec model on the given data\n\n    Parameters\n    ----------\n    data : List[str]\n        List of documents\n    output_path : Union[str, Path], optional\n        Path to save the trained model, by default None\n    epochs : int, optional\n        Number of epochs, by default 5\n    start_alpha : float, optional\n        Learning rate, by default 0.025\n    end_alpha : float, optional\n        Minimum learning rate, by default 0.0001\n    compute_loss : bool, optional\n        Whether to compute the loss, by default True\n    **kwargs : optional\n        Additional parameters for gensim.models.Word2Vec.train\n    \"\"\"\n    self.model.build_vocab(data)\n    total_examples = self.model.corpus_count\n    self.model.train(\n            data,\n            total_examples=total_examples,\n            epochs=epochs,\n            start_alpha=start_alpha,\n            end_alpha=end_alpha,\n            compute_loss=compute_loss,\n            **kwargs\n            )\n    if output_dir:\n        self.model.save(output_dir)\n</code></pre>"},{"location":"doc/graph/edges/","title":"Edges","text":"Source code in <code>semantics/graphs/temporal_graph.py</code> <pre><code>class Edges:\n    def __init__(\n            self,\n            node_features: np.ndarray,\n            node_embeddings: np.ndarray,\n        ):\n\n        self.node_features = node_features\n        self.node_embeddings = node_embeddings\n\n\n    def get_similarity(self, emb1: np.ndarray , emb2: np.ndarray) -&gt; float:\n        \"\"\"\n        This method is used to get the similarity between two nodes.\n\n        Args:\n            emb1 (np.ndarray): the embedding of the first word node\n            emb2 (np.ndarray): the embedding of the second word node\n\n        Returns:\n            similarity (float): the similarity between the two embeddings\n        \"\"\"\n        # np.dot(node1, node2) / (np.linalg.norm(node1) * np.linalg.norm(node2))\n        return torch.cosine_similarity(torch.tensor(emb1).reshape(1,-1), torch.tensor(emb2).reshape(1,-1)).item()\n\n\n    def get_pmi(self, node1: str, node2: str) -&gt; float:\n        \"\"\"\n        This method is used to get the PMI between two nodes.\n\n        Args:\n            node1 (str): the first node\n            node2 (str): the second node\n\n        Returns:\n            pmi (float): the PMI between the two nodes\n        \"\"\"\n\n        return 0.0\n</code></pre>"},{"location":"doc/graph/edges/#semantics.graphs.temporal_graph.Edges.get_pmi","title":"<code>get_pmi(node1, node2)</code>","text":"<p>This method is used to get the PMI between two nodes.</p> <p>Parameters:</p> Name Type Description Default <code>node1</code> <code>str</code> <p>the first node</p> required <code>node2</code> <code>str</code> <p>the second node</p> required <p>Returns:</p> Name Type Description <code>pmi</code> <code>float</code> <p>the PMI between the two nodes</p> Source code in <code>semantics/graphs/temporal_graph.py</code> <pre><code>def get_pmi(self, node1: str, node2: str) -&gt; float:\n    \"\"\"\n    This method is used to get the PMI between two nodes.\n\n    Args:\n        node1 (str): the first node\n        node2 (str): the second node\n\n    Returns:\n        pmi (float): the PMI between the two nodes\n    \"\"\"\n\n    return 0.0\n</code></pre>"},{"location":"doc/graph/edges/#semantics.graphs.temporal_graph.Edges.get_similarity","title":"<code>get_similarity(emb1, emb2)</code>","text":"<p>This method is used to get the similarity between two nodes.</p> <p>Parameters:</p> Name Type Description Default <code>emb1</code> <code>ndarray</code> <p>the embedding of the first word node</p> required <code>emb2</code> <code>ndarray</code> <p>the embedding of the second word node</p> required <p>Returns:</p> Name Type Description <code>similarity</code> <code>float</code> <p>the similarity between the two embeddings</p> Source code in <code>semantics/graphs/temporal_graph.py</code> <pre><code>def get_similarity(self, emb1: np.ndarray , emb2: np.ndarray) -&gt; float:\n    \"\"\"\n    This method is used to get the similarity between two nodes.\n\n    Args:\n        emb1 (np.ndarray): the embedding of the first word node\n        emb2 (np.ndarray): the embedding of the second word node\n\n    Returns:\n        similarity (float): the similarity between the two embeddings\n    \"\"\"\n    # np.dot(node1, node2) / (np.linalg.norm(node1) * np.linalg.norm(node2))\n    return torch.cosine_similarity(torch.tensor(emb1).reshape(1,-1), torch.tensor(emb2).reshape(1,-1)).item()\n</code></pre>"},{"location":"doc/graph/nodes/","title":"Nodes","text":"<p>This class is used to get the nodes of the word graph.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>str, dataset: List[str], level: int, k: int, c: int, word2vec_model_path: str, mlm_model_path: str, mlm_model_type: str = 'roberta') The constructor of the Nodes class.</p> <code>_get_similar_nodes</code> <p>str) -&gt; List[str] This method is used to get the similar nodes of a word.</p> <code>_get_context_nodes</code> <p>str) -&gt; List[str] This method is used to get the context nodes of a word.</p> Source code in <code>semantics/graphs/temporal_graph.py</code> <pre><code>class Nodes:\n    \"\"\"\n    This class is used to get the nodes of the word graph.\n\n    methods:\n        __init__(self, target_word: str, dataset: List[str], level: int, k: int, c: int, word2vec_model_path: str, mlm_model_path: str, mlm_model_type: str = 'roberta')\n            The constructor of the Nodes class.\n        _get_similar_nodes(self, word: str) -&gt; List[str]\n            This method is used to get the similar nodes of a word.\n\n        _get_context_nodes(self, word: str) -&gt; List[str]\n            This method is used to get the context nodes of a word.\n\n        get_nodes(self) -&gt; Dict[str, List[str]]\n            This method is used to get the nodes of the word graph.\n    \"\"\"\n    def __init__(\n            self,\n            target_word: str,\n            dataset: List[str],\n            level: int,\n            k: int,\n            c: int,\n            word2vec_model: Word2VecInference,\n            mlm_model: Union[RobertaInference, BertInference]\n            ):\n\n        \"\"\"\n        Args:\n            target_word (str): the word to get the nodes for\n            dataset (List[str]): the sentences to get the nodes from\n            level (int): the level of the graph to get\n            k (int): the number of similar nodes to get for each occurrence of the target word\n            c (int): the number of context nodes to get for the target word\n            word2vec_model (Word2VecInference): the word2vec model's Inference class\n            mlm_model (RobertaInference, BertInference): the MLM model's Inference class\n        \"\"\"\n\n        self.target_word = target_word\n        self.dataset = dataset\n        self.k = k\n        self.c = c\n        self.level = level\n        self.word2vec = word2vec_model\n        self.mlm = mlm_model\n\n\n    def _get_similar_nodes(self, word: str) -&gt; List[str]:\n        \"\"\"\n        This method is used to get the similar nodes of a word using the MLM model.\n\n        Args:\n            word (str): the word to get the similar nodes for\n\n        Returns:\n            similar nodes (List[str]): the list of similar nodes of the word\n        \"\"\"\n        similar_nodes = []\n        for sentence in self.dataset:\n            similar_nodes += self.mlm.get_top_k_words(word, sentence, self.k)\n        return list(set(similar_nodes))\n\n    def _get_context_nodes(self, word: str) -&gt; List[str]:\n        \"\"\"\n        This method is used to get the context nodes of a word using the word2vec model.\n\n        Args:\n            word (str): the word to get the context nodes for\n\n        Returns:\n            context nodes (List[str]): the list of context nodes of the word\n        \"\"\"\n        context_nodes, _ = self.word2vec.get_top_k_words(word, self.c)\n        return list(set(context_nodes))\n\n    def get_nodes(self) -&gt; Dict[str, List[str]]:\n        \"\"\"\n        This method is used to get the nodes of the word graph (similar nodes, context nodes, and target node).\n\n        Returns:\n            nodes (Dict[str, List[str]]): the nodes of the word graph\n        \"\"\"\n        nodes = {'target_node': [], 'similar_nodes': [], 'context_nodes': []}\n        for level in range(self.level):\n            if level == 0:\n                similar_nodes = self._get_similar_nodes(self.target_word)\n                context_nodes = self._get_context_nodes(self.target_word)\n\n                nodes['similar_nodes'].append(similar_nodes)\n                nodes['context_nodes'].append(context_nodes)\n                nodes['target_node'].append([self.target_word])\n\n            else:\n                similar_nodes = []\n                context_nodes = []\n                for word in nodes['similar_nodes'][level-1]:\n                    similar_nodes += self._get_similar_nodes(word)\n                    context_nodes += self._get_context_nodes(word)\n\n\n                for word in nodes['context_nodes'][level-1]:\n                    similar_nodes += self._get_similar_nodes(word)\n                    context_nodes += self._get_context_nodes(word)\n\n                nodes['similar_nodes'].append(similar_nodes)\n                nodes['context_nodes'].append(context_nodes)          \n        return nodes\n\n    def get_node_features(self, nodes: Dict[str, List[str]]):\n        \"\"\"\n        This method is used to get the features of the nodes of the word graph.\n\n        Args:\n            nodes (Dict[str, List[str]]): the nodes of the word graph\n\n        Returns:\n            - words (List[str]): the words of the nodes\n            - node_ids (List[int]): the ids of the nodes. The target node has id 0.\n            - node_features (np.ndarray): the features of the nodes of the word graph of shape (num_nodes, 3) where num_nodes is the number of nodes in the graph. The features are:\n                - node_type: the type of the node (target: 0, similar: 1, context: 2).\n                - node_level: the level of the node in the graph. The target node is level 0.\n                - frequency: the frequency of the word node in the dataset.\n            - embeddings (np.ndarray): the embeddings of the nodes of the word graph from the MLM model, of shape (num_nodes, 768).\n\n        Examples:\n            &gt;&gt;&gt; word2vec = Word2VecInference('word2vec.model')\n            &gt;&gt;&gt; mlm = RobertaInference('MLM_roberta')\n            &gt;&gt;&gt; nodes = Nodes(target_word='sentence', dataset=['this is a sentence', 'this is another sentence'], level=3, k=2, c=2, word2vec_model = word2vec, mlm_model = mlm).get_nodes()\n            &gt;&gt;&gt; words, node_ids, node_features, embeddings = n.get_node_features(nodes)\n            &gt;&gt;&gt; print(words)\n            ['sentence', 'this', 'is', 'a', 'another']\n            &gt;&gt;&gt; print(node_ids)\n            [0, 1, 2, 3, 4]\n            &gt;&gt;&gt; print(node_features)\n            [[0, 0, 2], [1, 1, 2], [1, 1, 2], [1, 1, 2], [2, 1, 2]]\n            &gt;&gt;&gt; print(embeddings.shape)\n            (5, 768)\n        \"\"\"\n        words = []\n        node_ids = []\n        node_types = []\n        node_levels = []\n        frequencies = []\n        embeddings = []\n        count = 0\n        for node_type in ['target_node', 'similar_nodes', 'context_nodes']:\n            for level in range(len(nodes[node_type])):\n                for node in nodes[node_type][level]:\n                    words.append(node)\n                    node_ids.append(count)\n                    count += 1 \n                    if node_type == 'target_node':\n                        node_types.append(0)\n                    elif node_type == 'similar_nodes':\n                        node_types.append(1)\n                    else:\n                        node_types.append(2)\n                    node_levels.append(level)\n                    frequencies.append(sum(node in s for s in self.dataset))\n                    embeddings.append(self.mlm.get_embedding(word=node).mean(axis=0))\n\n        embeddings = np.array(embeddings)\n        node_features = np.stack([node_types, node_levels, frequencies]).T\n        # node_features = np.concatenate((node_features, embeddings), axis=1)\n        return words, node_ids, node_features, embeddings\n</code></pre>"},{"location":"doc/graph/nodes/#semantics.graphs.temporal_graph.Nodes.__init__","title":"<code>__init__(target_word, dataset, level, k, c, word2vec_model, mlm_model)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>target_word</code> <code>str</code> <p>the word to get the nodes for</p> required <code>dataset</code> <code>List[str]</code> <p>the sentences to get the nodes from</p> required <code>level</code> <code>int</code> <p>the level of the graph to get</p> required <code>k</code> <code>int</code> <p>the number of similar nodes to get for each occurrence of the target word</p> required <code>c</code> <code>int</code> <p>the number of context nodes to get for the target word</p> required <code>word2vec_model</code> <code>Word2VecInference</code> <p>the word2vec model's Inference class</p> required <code>mlm_model</code> <code>(RobertaInference, BertInference)</code> <p>the MLM model's Inference class</p> required Source code in <code>semantics/graphs/temporal_graph.py</code> <pre><code>def __init__(\n        self,\n        target_word: str,\n        dataset: List[str],\n        level: int,\n        k: int,\n        c: int,\n        word2vec_model: Word2VecInference,\n        mlm_model: Union[RobertaInference, BertInference]\n        ):\n\n    \"\"\"\n    Args:\n        target_word (str): the word to get the nodes for\n        dataset (List[str]): the sentences to get the nodes from\n        level (int): the level of the graph to get\n        k (int): the number of similar nodes to get for each occurrence of the target word\n        c (int): the number of context nodes to get for the target word\n        word2vec_model (Word2VecInference): the word2vec model's Inference class\n        mlm_model (RobertaInference, BertInference): the MLM model's Inference class\n    \"\"\"\n\n    self.target_word = target_word\n    self.dataset = dataset\n    self.k = k\n    self.c = c\n    self.level = level\n    self.word2vec = word2vec_model\n    self.mlm = mlm_model\n</code></pre>"},{"location":"doc/graph/nodes/#semantics.graphs.temporal_graph.Nodes.get_node_features","title":"<code>get_node_features(nodes)</code>","text":"<p>This method is used to get the features of the nodes of the word graph.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>Dict[str, List[str]]</code> <p>the nodes of the word graph</p> required <p>Returns:</p> Type Description <ul> <li>words (List[str]): the words of the nodes</li> </ul> <ul> <li>node_ids (List[int]): the ids of the nodes. The target node has id 0.</li> </ul> <ul> <li>node_features (np.ndarray): the features of the nodes of the word graph of shape (num_nodes, 3) where num_nodes is the number of nodes in the graph. The features are:</li> <li>node_type: the type of the node (target: 0, similar: 1, context: 2).</li> <li>node_level: the level of the node in the graph. The target node is level 0.</li> <li>frequency: the frequency of the word node in the dataset.</li> </ul> <ul> <li>embeddings (np.ndarray): the embeddings of the nodes of the word graph from the MLM model, of shape (num_nodes, 768).</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; word2vec = Word2VecInference('word2vec.model')\n&gt;&gt;&gt; mlm = RobertaInference('MLM_roberta')\n&gt;&gt;&gt; nodes = Nodes(target_word='sentence', dataset=['this is a sentence', 'this is another sentence'], level=3, k=2, c=2, word2vec_model = word2vec, mlm_model = mlm).get_nodes()\n&gt;&gt;&gt; words, node_ids, node_features, embeddings = n.get_node_features(nodes)\n&gt;&gt;&gt; print(words)\n['sentence', 'this', 'is', 'a', 'another']\n&gt;&gt;&gt; print(node_ids)\n[0, 1, 2, 3, 4]\n&gt;&gt;&gt; print(node_features)\n[[0, 0, 2], [1, 1, 2], [1, 1, 2], [1, 1, 2], [2, 1, 2]]\n&gt;&gt;&gt; print(embeddings.shape)\n(5, 768)\n</code></pre> Source code in <code>semantics/graphs/temporal_graph.py</code> <pre><code>def get_node_features(self, nodes: Dict[str, List[str]]):\n    \"\"\"\n    This method is used to get the features of the nodes of the word graph.\n\n    Args:\n        nodes (Dict[str, List[str]]): the nodes of the word graph\n\n    Returns:\n        - words (List[str]): the words of the nodes\n        - node_ids (List[int]): the ids of the nodes. The target node has id 0.\n        - node_features (np.ndarray): the features of the nodes of the word graph of shape (num_nodes, 3) where num_nodes is the number of nodes in the graph. The features are:\n            - node_type: the type of the node (target: 0, similar: 1, context: 2).\n            - node_level: the level of the node in the graph. The target node is level 0.\n            - frequency: the frequency of the word node in the dataset.\n        - embeddings (np.ndarray): the embeddings of the nodes of the word graph from the MLM model, of shape (num_nodes, 768).\n\n    Examples:\n        &gt;&gt;&gt; word2vec = Word2VecInference('word2vec.model')\n        &gt;&gt;&gt; mlm = RobertaInference('MLM_roberta')\n        &gt;&gt;&gt; nodes = Nodes(target_word='sentence', dataset=['this is a sentence', 'this is another sentence'], level=3, k=2, c=2, word2vec_model = word2vec, mlm_model = mlm).get_nodes()\n        &gt;&gt;&gt; words, node_ids, node_features, embeddings = n.get_node_features(nodes)\n        &gt;&gt;&gt; print(words)\n        ['sentence', 'this', 'is', 'a', 'another']\n        &gt;&gt;&gt; print(node_ids)\n        [0, 1, 2, 3, 4]\n        &gt;&gt;&gt; print(node_features)\n        [[0, 0, 2], [1, 1, 2], [1, 1, 2], [1, 1, 2], [2, 1, 2]]\n        &gt;&gt;&gt; print(embeddings.shape)\n        (5, 768)\n    \"\"\"\n    words = []\n    node_ids = []\n    node_types = []\n    node_levels = []\n    frequencies = []\n    embeddings = []\n    count = 0\n    for node_type in ['target_node', 'similar_nodes', 'context_nodes']:\n        for level in range(len(nodes[node_type])):\n            for node in nodes[node_type][level]:\n                words.append(node)\n                node_ids.append(count)\n                count += 1 \n                if node_type == 'target_node':\n                    node_types.append(0)\n                elif node_type == 'similar_nodes':\n                    node_types.append(1)\n                else:\n                    node_types.append(2)\n                node_levels.append(level)\n                frequencies.append(sum(node in s for s in self.dataset))\n                embeddings.append(self.mlm.get_embedding(word=node).mean(axis=0))\n\n    embeddings = np.array(embeddings)\n    node_features = np.stack([node_types, node_levels, frequencies]).T\n    # node_features = np.concatenate((node_features, embeddings), axis=1)\n    return words, node_ids, node_features, embeddings\n</code></pre>"},{"location":"doc/graph/nodes/#semantics.graphs.temporal_graph.Nodes.get_nodes","title":"<code>get_nodes()</code>","text":"<p>This method is used to get the nodes of the word graph (similar nodes, context nodes, and target node).</p> <p>Returns:</p> Name Type Description <code>nodes</code> <code>Dict[str, List[str]]</code> <p>the nodes of the word graph</p> Source code in <code>semantics/graphs/temporal_graph.py</code> <pre><code>def get_nodes(self) -&gt; Dict[str, List[str]]:\n    \"\"\"\n    This method is used to get the nodes of the word graph (similar nodes, context nodes, and target node).\n\n    Returns:\n        nodes (Dict[str, List[str]]): the nodes of the word graph\n    \"\"\"\n    nodes = {'target_node': [], 'similar_nodes': [], 'context_nodes': []}\n    for level in range(self.level):\n        if level == 0:\n            similar_nodes = self._get_similar_nodes(self.target_word)\n            context_nodes = self._get_context_nodes(self.target_word)\n\n            nodes['similar_nodes'].append(similar_nodes)\n            nodes['context_nodes'].append(context_nodes)\n            nodes['target_node'].append([self.target_word])\n\n        else:\n            similar_nodes = []\n            context_nodes = []\n            for word in nodes['similar_nodes'][level-1]:\n                similar_nodes += self._get_similar_nodes(word)\n                context_nodes += self._get_context_nodes(word)\n\n\n            for word in nodes['context_nodes'][level-1]:\n                similar_nodes += self._get_similar_nodes(word)\n                context_nodes += self._get_context_nodes(word)\n\n            nodes['similar_nodes'].append(similar_nodes)\n            nodes['context_nodes'].append(context_nodes)          \n    return nodes\n</code></pre>"}]}