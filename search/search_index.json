{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Semantic Shift Detection","text":""},{"location":"utils/","title":"utilites","text":""},{"location":"utils/#semantics.utils.utils.count_occurence","title":"<code>count_occurence(data, word=None)</code>","text":"<p>Count the occurence of one, multiple, or all words in a list of strings (data).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[str]</code> <p>The list of strings.</p> required <code>word</code> <code>Union[List[str], str]</code> <p>The word or list of words to count. if a list of words is provided, the function will count the co-occurence of the words in each data string. if word is None, the function return the count of all words in the data.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>count</code> <code>int</code> <p>The count of occurences.</p> Source code in <code>semantics/utils/utils.py</code> <pre><code>def count_occurence(data: List[str], word: Optional[Union[List[str], str]] = None) -&gt; int:\n    \"\"\"\n    Count the occurence of one, multiple, or all words in a list of strings (data).\n\n    Args:\n        data (List[str]): The list of strings.\n        word (Union[List[str], str]): The word or list of words to count. if a list of words is provided, the function will count the co-occurence of the words in each data string. if word is None, the function return the count of all words in the data.\n\n    Returns:\n        count (int): The count of occurences.\n    \"\"\"\n\n    if word is None:\n        return sum([len(sentence.split()) for sentence in data])\n\n    if isinstance(word, str):\n        word = [word]\n\n    if len(word) == 1:\n        return sum([1 for w in data if word[0] in w])\n\n    else:\n        counts = 0\n        for sentence in data:\n            if all([w in sentence for w in word]):\n                counts += 1\n        return counts\n</code></pre>"},{"location":"utils/#semantics.utils.utils.intersection_align_gensim","title":"<code>intersection_align_gensim(m1, m2, words=None)</code>","text":"<p>Intersect two gensim word2vec models, m1 and m2. Only the shared vocabulary between them is kept. If 'words' is set (as list or set), then the vocabulary is intersected with this list as well. Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2). These indices correspond to the new syn0 and syn0norm objects in both gensim models:     -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0     -- you can find the index of any word on the .index2word list: model.index2word.index(word) =&gt; 2 The .vocab dictionary is also updated for each model, preserving the count but updating the index.</p> Source code in <code>semantics/utils/utils.py</code> <pre><code>def intersection_align_gensim(m1, m2, words=None):\n    \"\"\"\n    Intersect two gensim word2vec models, m1 and m2.\n    Only the shared vocabulary between them is kept.\n    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n        -- you can find the index of any word on the .index2word list: model.index2word.index(word) =&gt; 2\n    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n    \"\"\"\n\n    # Get the vocab for each model\n    vocab_m1 = set(m1.wv.index_to_key)\n    vocab_m2 = set(m2.wv.index_to_key)\n\n    # Find the common vocabulary\n    common_vocab = vocab_m1 &amp; vocab_m2\n    if words: common_vocab &amp;= set(words)\n\n    # If no alignment necessary because vocab is identical...\n    if not vocab_m1 - common_vocab and not vocab_m2 - common_vocab:\n        return (m1,m2)\n\n    # Otherwise sort by frequency (summed for both)\n    common_vocab = list(common_vocab)\n    common_vocab.sort(key=lambda w: m1.wv.get_vecattr(w, \"count\") + m2.wv.get_vecattr(w, \"count\"), reverse=True)\n    # print(len(common_vocab))\n\n    # Then for each model...\n    for m in [m1, m2]:\n        # Replace old syn0norm array with new one (with common vocab)\n        indices = [m.wv.key_to_index[w] for w in common_vocab]\n        old_arr = m.wv.vectors\n        new_arr = np.array([old_arr[index] for index in indices])\n        m.wv.vectors = new_arr\n\n        # Replace old vocab dictionary with new one (with common vocab)\n        # and old index2word with new one\n        new_key_to_index = {}\n        new_index_to_key = []\n        for new_index, key in enumerate(common_vocab):\n            new_key_to_index[key] = new_index\n            new_index_to_key.append(key)\n        m.wv.key_to_index = new_key_to_index\n        m.wv.index_to_key = new_index_to_key\n\n        print(len(m.wv.key_to_index), len(m.wv.vectors))\n\n    return (m1,m2)\n</code></pre>"},{"location":"utils/#semantics.utils.utils.most_frequent","title":"<code>most_frequent(my_list, n=1)</code>","text":"<p>Return the n most frequent words in a list.</p> <p>Parameters:</p> Name Type Description Default <code>my_list</code> <code>List[str]</code> <p>The list of words.</p> required <code>n</code> <code>int</code> <p>The number of most frequent words to return.</p> <code>1</code> <p>Returns:</p> Type Description <p>List[str]: The n most frequent words.</p> Source code in <code>semantics/utils/utils.py</code> <pre><code>def most_frequent(my_list: List[str], n: int = 1):\n    \"\"\"\n    Return the n most frequent words in a list.\n\n    Args:\n        my_list (List[str]): The list of words.\n        n (int): The number of most frequent words to return.\n\n    Returns:\n        List[str]: The n most frequent words.\n    \"\"\"\n    occurence_count = Counter(my_list)\n    return [w for w, _ in occurence_count.most_common(n)]\n</code></pre>"},{"location":"utils/#semantics.utils.utils.read_toml","title":"<code>read_toml(config_path)</code>","text":"<p>Read in a config file and return a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the config file.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The dictionary.</p> Source code in <code>semantics/utils/utils.py</code> <pre><code>def read_toml(config_path: str) -&gt; dict:\n    \"\"\"\n    Read in a config file and return a dictionary.\n\n    Args:\n        config_path (str): The path to the config file.\n\n    Returns:\n        dict: The dictionary.\n    \"\"\"\n    with open(config_path, \"rb\") as f:\n        return tomli.load(f)\n</code></pre>"},{"location":"utils/#semantics.utils.utils.read_txt","title":"<code>read_txt(file_path)</code>","text":"<p>Read in a txt file and return a list of lines.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the txt file.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of lines.</p> Source code in <code>semantics/utils/utils.py</code> <pre><code>def read_txt(file_path: str) -&gt; list:\n    \"\"\"\n    Read in a txt file and return a list of lines.\n\n    Args:\n        file_path (str): The path to the txt file.\n\n    Returns:\n        list: A list of lines.\n    \"\"\"\n\n    with open(file_path) as f:\n        return f.readlines()\n</code></pre>"},{"location":"utils/#semantics.utils.utils.read_yaml","title":"<code>read_yaml(file_path)</code>","text":"<p>Read in a yaml file and return a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the yaml file.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The dictionary.</p> Source code in <code>semantics/utils/utils.py</code> <pre><code>def read_yaml(file_path: str) -&gt; dict:\n    \"\"\"\n    Read in a yaml file and return a dictionary.\n\n    Args:\n        file_path (str): The path to the yaml file.\n\n    Returns:\n        dict: The dictionary.\n    \"\"\"\n    with open(file_path) as f:\n        return yaml.load(f, Loader=yaml.FullLoader)\n</code></pre>"},{"location":"utils/#semantics.utils.utils.sample_data","title":"<code>sample_data(data, sample_size, random_seed=None)</code>","text":"<p>Sample data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list</code> <p>The data to sample.</p> required <code>sample_size</code> <code>int</code> <p>The size of the sample.</p> required <code>random_seed</code> <code>int</code> <p>The random seed.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>sample_data</code> <code>list</code> <p>The sampled data.</p> Source code in <code>semantics/utils/utils.py</code> <pre><code>def sample_data(data: list, sample_size: int, random_seed=None):\n    \"\"\"\n    Sample data.\n\n    Args:\n        data (list): The data to sample.\n        sample_size (int): The size of the sample.\n        random_seed (int): The random seed.\n\n    Returns:\n        sample_data (list): The sampled data.\n    \"\"\"\n\n    if random_seed:\n        random.seed(random_seed)\n    data_copy = data[:]\n    random.shuffle(data_copy)\n    return data_copy[:sample_size]\n</code></pre>"},{"location":"utils/#semantics.utils.utils.smart_procrustes_align_gensim","title":"<code>smart_procrustes_align_gensim(base_embed, other_embed, words=None)</code>","text":"<p>Original script: https://gist.github.com/quadrismegistus/09a93e219a6ffc4f216fb85235535faf Procrustes align two gensim word2vec models (to allow for comparison between same word across models). Code ported from HistWords https://github.com/williamleif/histwords by William Hamilton wleif@stanford.edu.</p> <p>First, intersect the vocabularies (see <code>intersection_align_gensim</code> documentation). Then do the alignment on the other_embed model. Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version. Return other_embed.</p> <p>If <code>words</code> is set, intersect the two models' vocabulary with the vocabulary in words (see <code>intersection_align_gensim</code> documentation).</p> Source code in <code>semantics/utils/utils.py</code> <pre><code>def smart_procrustes_align_gensim(base_embed, other_embed, words=None):\n    \"\"\"\n    Original script: https://gist.github.com/quadrismegistus/09a93e219a6ffc4f216fb85235535faf\n    Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n    Code ported from HistWords &lt;https://github.com/williamleif/histwords&gt; by William Hamilton &lt;wleif@stanford.edu&gt;.\n\n    First, intersect the vocabularies (see `intersection_align_gensim` documentation).\n    Then do the alignment on the other_embed model.\n    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n    Return other_embed.\n\n    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n    \"\"\"\n\n    # patch by Richard So [https://twitter.com/richardjeanso) (thanks!) to update this code for new version of gensim\n    # base_embed.init_sims(replace=True)\n    # other_embed.init_sims(replace=True)\n\n    # make sure vocabulary and indices are aligned\n    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n\n    # re-filling the normed vectors\n    in_base_embed.wv.fill_norms(force=True)\n    in_other_embed.wv.fill_norms(force=True)\n\n    # get the (normalized) embedding matrices\n    base_vecs = in_base_embed.wv.get_normed_vectors()\n    other_vecs = in_other_embed.wv.get_normed_vectors()\n\n    # just a matrix dot product with numpy\n    m = other_vecs.T.dot(base_vecs) \n    # SVD method from numpy\n    u, _, v = np.linalg.svd(m)\n    # another matrix operation\n    ortho = u.dot(v) \n    # Replace original array with modified one, i.e. multiplying the embedding matrix by \"ortho\"\n    other_embed.wv.vectors = (other_embed.wv.vectors).dot(ortho)    \n\n    return other_embed\n</code></pre>"},{"location":"utils/#semantics.utils.utils.train_test_split","title":"<code>train_test_split(data, test_ratio=0.2, random_seed=None)</code>","text":"<p>Split the data into train and test sets.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[str]</code> <p>The data to split.</p> required <code>test_ratio</code> <code>float</code> <p>The ratio of the test set.</p> <code>0.2</code> <code>random_seed</code> <code>int</code> <p>The random seed.</p> <code>None</code> <p>Returns:</p> Type Description <ul> <li>train_data (List[str]): The train set.</li> </ul> <ul> <li>test_data (List[str]): The test set.</li> </ul> Source code in <code>semantics/utils/utils.py</code> <pre><code>def train_test_split(data: List[str], test_ratio=0.2, random_seed=None):\n    \"\"\"\n    Split the data into train and test sets.\n\n    Args:\n        data (List[str]): The data to split.\n        test_ratio (float): The ratio of the test set.\n        random_seed (int): The random seed.\n\n    Returns:\n        - train_data (List[str]): The train set.\n        - test_data (List[str]): The test set.\n    \"\"\"\n\n    if random_seed:\n        random.seed(random_seed)\n    data_copy = data[:]\n    random.shuffle(data_copy)\n    split_idx = int(len(data_copy) * (1 - test_ratio))\n    train_data = data_copy[:split_idx]\n    test_data = data_copy[split_idx:]\n    return train_data, test_data\n</code></pre>"},{"location":"concepts/intro/","title":"Introduction","text":"<p>Semantic shift is a phenomenon that occurs naturally in all languages as the meaning of words can change over time due to various factors such as cultural shifts, technological advancements, and language contact. Understanding the semantic evolution of words is crucial for fields such as historical linguistics, cultural studies, and language technology. Etymology, the study of the origin and history of words, plays a pivotal role in tracing the development of words from their earliest known use to their current usage, and exam- ining the ways in which their meanings and forms have changed over time. The study of semantic shift is also vital for improving language technology applications such as search engines, content recommendation systems, and machine translation. For instance, track- ing semantic change can help search engines and content recommendation systems to provide more relevant and accurate results to users, and it can also improve the accuracy of machine translation systems. Traditional methods for tracking semantic shift involve manual annotation of texts and comparison of word usages across different time periods, which can be time-consuming and labor-intensive. However, recent advancements in natural language processing have led to the development of diachronic word embeddings, which offer a more efficient and au- tomated approach to tracking semantic shift. This study aims to explore the effectiveness of diachronic embeddings for semantic shift tracking.</p>"},{"location":"concepts/literature_review/","title":"Literature Review","text":"<p>The literature review serves as the foundation for the proposed research, which aims to explore the effectiveness of diachronic embeddings for semantic shift tracking in English language data.</p> <p>In recent years, there has been a growing interest in the automatic detection of semantic change. Word embeddings have been particularly popular in this field <sup>1</sup>, and were used in various studies to track semantic shift in different languages and domains. Hamilton<sup>2</sup> and Dubossarsky<sup>3</sup> trained diachronic word embeddings on corpora spanning long periods of time to track changes in word meanings and analyze new semantic laws. There has also been a surge in research on short-term semantic changes, including analysis of Amazon reviews, news articles, and Twitter data <sup>4</sup>. While Kutuzov<sup>5</sup> used diachronic embeddings to track occurences of events such as armed conflict.</p> <p>To track semantic shift, a prototypical representation of a word's meanings is needed <sup>6</sup>. Giulianelli<sup>7</sup> offered a distinction between form-based approaches, where one embedding per word is used to represent all its possible meanings in a certain time period, and sense-based approaches, where contextual embeddings are trained to represent the meaning of a word at each occurence. Hamilton<sup>2</sup> and Dubossarsky<sup>3</sup> used form-based approaches to train independent Word2Vec embeddings on corpora organized by time periods. They recorded the semantic changepoint and shift score of the target word at different time periods, after aligning their vector spaces. While Kim Yoon <sup>8</sup> used vector initialisation to initialize the embeddings for each word using the embeddings of the previous time period. However, form-based approached have their limitations, especially when dealing with polysemy or words with multiple meanings, as they capture and represent words only through their dominant sense. This paved the way for more advanced models, which embedded words in their contextual habitats, heralding a new era of semantic modeling.</p> <p>As for sense-based approaches, their ability in delineating relationships between words within a sentence ---particularly leveraging the attention mechanism inherent in the BERT transformer architecture--- has rendered them apt for tracking semantic shifts. Given that such methodologies yield distinct embeddings for each occurence of a word, an aggregation phase is implemented. This phase collates these varied embeddings, particularly when the word is situated in semantically analogous contexts, culminating in a prototypical representation that captures the diverse senses of the word. <sup>4</sup> used contextual BERT transformer embeddings trained on an annotated dictionary corpus to model the distribution of the finite set of senses of target words across time, after taking the average of each word-sense embeddings as a prototypical representation of the word-sense. While <sup>9</sup> used clustering and alignment to aggregate contextual embeddings into seperate identifiable word meanings across time. Recently, <sup>10</sup> employed incremental clustering to gradually cluster the available embeddings from different time steps and avoid alignment.</p> <p>To measure the shift in meanings of a word, <sup>11</sup> proposed a graded semantic change score based on the Jensen-Shannon distance between the sense clusters distributions. <sup>7</sup> used the average pairwise euclidean distance between the sense clusters instead. <sup>10</sup> employed the cosine similarity between the target word's sense embedding at each time period and the barycenter of the sense cluster at the most recent time period. Others (Eg. <sup>12</sup>), used the number of embeddings in each cluster through time as a novelty score, where the maximum ratio is interpreted as the semantic shift score. For form-based approaches, the cosine similarity, between the target's embeddings from different time periods and the most recent embedding, is the most frequently used measure of semantic shift.</p>"},{"location":"concepts/literature_review/#limitations-of-form-based-approaches","title":"Limitations of form-based approaches","text":"<p>Static word embeddings, by their very design, offer a singular representation for each lexeme. This design inherently biases towards a word's dominant or most prevalent sense, effectively sidelining its subordinate or less common senses. The inability to represent multiple senses is a significant drawback, especially in diachronic linguistic studies where a lexeme's subordinate sense in one era might evolve to become its dominant sense in another. When static embeddings are derived using smaller window sizes, the resulting vectors predominantly capture immediate lexical surroundings. The cosine similarity between such embeddings often reveals interchangeability, aligning them closely with synonyms or collocations. However, a purely synonym-based understanding of semantics is reductive. Linguistic studies, grounded in comprehensive semantic frameworks, advocate a more holistic representation of word meanings. This includes not just synonyms but also antonyms, hyponyms, and for verbs, associated agents and roles. Relying solely on synonyms, therefore, results in a myopic semantic perspective, failing to capture the multifaceted nature of lexemes. Conversely, when static embeddings employ larger window sizes, the vectors tend to encapsulate broader contextual relationships rather than the inherent meanings of the words. As a result, words that frequently appear in similar contexts, but aren't necessarily synonymous or interchangeable, exhibit high cosine similarity. Such embeddings, while capturing contextual relationships, can often obfuscate the true semantic nuances of words. They may not necessarily provide insights into the core meanings of lexemes but rather highlight the contexts they're frequently associated with. Static word embeddings, while pioneering and effective for certain applications, exhibit inherent limitations when deployed for detecting semantic shifts. Their singular representations, coupled with the constraints posed by varying window sizes, often result in either an overly narrow or overly broad semantic understanding. For a comprehensive exploration of diachronic semantics, more dynamic and multifaceted approaches are imperative.</p>"},{"location":"concepts/literature_review/#limitations-of-sense-based-approaches","title":"Limitations of sense-based approaches","text":"<p>Contextual embeddings, such as those derived from BERT, inherently produce distinct representations for each occurrence of a word based on its specific context. This granularity, while beneficial for tasks requiring context sensitivity, poses challenges for semantic shift detection. The need to aggregate these disparate embeddings becomes paramount to discern and represent the diverse senses a word may adopt. Research such as that by <sup>4</sup> has ventured into average aggregation of these embeddings. While this approach offers a consolidated perspective, it isn't devoid of limitations:</p> <p>The approach mandates a supervised training of sense embeddings. As a consequence, the possible senses are confined to a pre-established, dictionary-based set. This presents two primary challenges:</p> <p>- The method is not conducive to detecting the emergence of novel senses over time, instead merely quantifying shifts within the predefined sense distribution.</p> <p>- The reliance on a predetermined set of senses demands manually labeled data, inherently restricting the approach's scalability and adaptability.</p> <p>Alternative methods, like the one proposed by <sup>9</sup>, employ clustering for aggregation. This approach, however, presents its own set of challenges:</p> <p>- Cluster alignment is essential to discern consistent word meanings across successive temporal instances. Alternatively, an incremental clustering process is necessitated.</p> <p>- Algorithms demanding a predefined cluster count ('K') fail to accommodate the detection of novel senses emerging over time.</p> <p>- Clustering, by its nature, can be influenced by biases inherent in word forms. Addressing this, researchers have proposed clustering refinement techniques. Some methods involve removing or merging clusters with minimal members, while others, as suggested by <sup>10</sup>, discard clusters deemed insufficiently informative based on their size relative to the entire embeddings set. However, such refinement techniques, especially in corpora with imbalanced word meanings, must be applied judiciously. Even smaller clusters could be pivotal, offering insights into minority or emerging senses, potentially overlooked when adopting a one-size-fits-all refinement strategy.</p> <p>Contextual embeddings, while offering depth and granularity, present specific challenges when employed for semantic shift detection. The complexities of aggregation, whether average-based or clustering-based, alongside inherent limitations of supervised and predefined approaches, necessitate an adaptable methodology for accurate and comprehensive semantic analyses.</p> <ol> <li> <p>Andrey Kutuzov, Lilja \u00d8vrelid, Terrence Szymanski, and Erik Velldal. Diachronic word embeddings and semantic shifts: a survey. In Proceedings of the 27th International Conference on Computational Linguistics, 1384\u20131397. Santa Fe, New Mexico, USA, August 2018. Association for Computational Linguistics. URL: https://aclanthology.org/C18-1117.\u00a0\u21a9</p> </li> <li> <p>William L. Hamilton, Jure Leskovec, and Dan Jurafsky. Diachronic word embeddings reveal statistical laws of semantic change. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1489\u20131501. Berlin, Germany, August 2016. Association for Computational Linguistics. URL: https://aclanthology.org/P16-1141, doi:10.18653/v1/P16-1141.\u00a0\u21a9\u21a9</p> </li> <li> <p>Haim Dubossarsky, Daphna Weinshall, and Eitan Grossman. Outta control: laws of semantic change and inherent biases in word representation models. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, 1136\u20131145. Copenhagen, Denmark, September 2017. Association for Computational Linguistics. URL: https://aclanthology.org/D17-1118, doi:10.18653/v1/D17-1118.\u00a0\u21a9\u21a9</p> </li> <li> <p>Renfen Hu, Shen Li, and Shichen Liang. Diachronic sense modeling with deep contextualized word embeddings: an ecological view. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 3899\u20133908. Florence, Italy, July 2019. Association for Computational Linguistics. URL: https://aclanthology.org/P19-1379, doi:10.18653/v1/P19-1379.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Andrey Kutuzov, Erik Velldal, and Lilja \u00d8vrelid. Tracing armed conflicts with diachronic word embedding models. In Proceedings of the Events and Stories in the News Workshop, 31\u201336. Vancouver, Canada, August 2017. Association for Computational Linguistics. URL: https://aclanthology.org/W17-2705, doi:10.18653/v1/W17-2705.\u00a0\u21a9</p> </li> <li> <p>Stefano Montanelli and Francesco Periti. A survey on contextualised semantic shift detection. 2023. arXiv:2304.01666.\u00a0\u21a9</p> </li> <li> <p>Mario Giulianelli, Marco Del Tredici, and Raquel Fern\u00e1ndez. Analysing lexical semantic change with contextualised word representations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 3960\u20133973. Online, July 2020. Association for Computational Linguistics. URL: https://aclanthology.org/2020.acl-main.365, doi:10.18653/v1/2020.acl-main.365.\u00a0\u21a9\u21a9</p> </li> <li> <p>Yoon Kim, Yi-I Chiu, Kentaro Hanaki, Darshan Hegde, and Slav Petrov. Temporal analysis of language through neural language models. In Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, 61\u201365. Baltimore, MD, USA, June 2014. Association for Computational Linguistics. URL: https://aclanthology.org/W14-2517, doi:10.3115/v1/W14-2517.\u00a0\u21a9</p> </li> <li> <p>Vani Kanjirangat, Sandra Mitrovic, Alessandro Antonucci, and Fabio Rinaldi. SST-BERT at SemEval-2020 task 1: semantic shift tracing by clustering in BERT-based embedding spaces. In Proceedings of the Fourteenth Workshop on Semantic Evaluation, 214\u2013221. Barcelona (online), December 2020. International Committee for Computational Linguistics. URL: https://aclanthology.org/2020.semeval-1.26, doi:10.18653/v1/2020.semeval-1.26.\u00a0\u21a9\u21a9</p> </li> <li> <p>Francesco Periti, Alfio Ferrara, Stefano Montanelli, and Martin Ruskov. What is done is done: an incremental approach to semantic shift detection. In Proceedings of the 3rd Workshop on Computational Approaches to Historical Language Change, 33\u201343. Dublin, Ireland, May 2022. Association for Computational Linguistics. URL: https://aclanthology.org/2022.lchange-1.4, doi:10.18653/v1/2022.lchange-1.4.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Dominik Schlechtweg, Barbara McGillivray, Simon Hengchen, Haim Dubossarsky, and Nina Tahmasebi. SemEval-2020 task 1: unsupervised lexical semantic change detection. In Proceedings of the Fourteenth Workshop on Semantic Evaluation, 1\u201323. Barcelona (online), December 2020. International Committee for Computational Linguistics. URL: https://aclanthology.org/2020.semeval-1.1, doi:10.18653/v1/2020.semeval-1.1.\u00a0\u21a9</p> </li> <li> <p>Paul Cook, Jey Han Lau, Diana McCarthy, and Timothy Baldwin. Novel word-sense identification. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, 1624\u20131635. Dublin, Ireland, August 2014. Dublin City University and Association for Computational Linguistics. URL: https://aclanthology.org/C14-1154.\u00a0\u21a9</p> </li> </ol>"},{"location":"concepts/problem_statement/","title":"Problem Statement","text":"<p>The evolution of computational linguistics has seen significant strides in embedding techniques, promising nuanced representations of words in vector spaces. However, the aggregation of these embeddings, particularly in the realm of contextual embeddings, has been a point of contention.</p> <p>Firstly, the very act of aggregation, whether it be through averaging mechanisms or clustering methodologies, carries an inherent risk of information loss. Each instance of a word in textual data is rooted in a specific context, influenced by myriad factors ranging from syntactic structures to broader discursive themes. Aggregating embeddings attempts to distill this richness into a singular or limited set of representations, often sidelining the less frequent or less predictable nuances.</p> <p>Furthermore, aggregation is underpinned by a slew of assumptions. It presumes a certain homogeneity within the aggregated data points, often prioritizing dominant senses at the expense of overshadowing emergent or peripheral meanings. The granularity and intricacy of language, where words can adopt varied shades of meanings based on contexts, temporalities, and audiences, challenges the very foundational premise of aggregation.</p> <p>The assertion in <sup>1</sup> is particularly illuminating in this debate. Kilgariff delineates the difference between 'lumping' and 'splitting' when defining word senses. The act of lumping seeks to group together instances based on perceived similarities, often influenced by factors like frequency and predictability. In contrast, splitting allows for the recognition of distinctions, emphasizing the heterogeneity of word usages. Aggregation, by its nature, leans towards the lumping paradigm. While it offers the advantage of simplification, it does so at the cost of potentially overlooking the intricate mosaic of semantic landscapes. A telling example from Kilgarriff's research elucidates this concern. In <sup>1</sup>, the sense of the word 'handbag' is explored, highlighting an unconventional interpretation: handbag-as-weapon. This specific sense, though valid and meaningful in certain contexts, remains absent from many conventional dictionaries. Such an omission can largely be attributed to the infrequent deployment of 'handbag' in this particular sense. This example underscores the pitfalls of aggregation and the perils of biasing towards frequency---it demonstrates how meaningful, albeit less common, interpretations can be easily overlooked. The term run exemplifies the complexities inherent in semantic predictability. Predominantly associated with locomotion, run encompasses an array of interpretations, from operating machinery to temporal lateness. However, in the realm of journalism, run acquires a specialized nuance, denoting the publication of a story. This specific interpretation, although contextually significant, might not be immediately discernible based on the term's more ubiquitous senses. Models predisposed to dominant patterns could thus misapprehend or overlook this journalistic context, underscoring the challenges of predictability. Essentially, a sense's predictability, or lack thereof, based on prevalent uses does not diminish its validity, highlighting the intricacies of semantic modeling.</p> <p>These challenges can be addressed in practical endeavors by establishing task-dependent foundations for the aggregation process. Depending on the specific task, the aggregation can be tailored to prioritize certain senses, underpinning the rationale for such grouping or clustering. However, this approach also presents drawbacks. Its inherent subjectivity means that outcomes are not solely dependent on the raw data (corpus) but are also influenced by editorial philosophies and the target audience. These external determinants can introduce biases, potentially distorting the perceived semantic landscape.</p>"},{"location":"concepts/problem_statement/#motivation","title":"Motivation","text":"<p>In response to the challenges faced by traditional word embedding aggregation techniques, a graph-based approach offers a promising alternative. Graphs, by their nature, are inherently flexible and adaptable. They can be molded to suit specific needs, accommodating a range of data types and structures. Graphs also offer a more nuanced representation of data, allowing for the inclusion of multiple relationships and connections. This is particularly relevant in the realm of semantic modeling, where words can be associated with a range of meanings and contexts. Graphs, therefore, facilitate a deeper, more comprehensive exploration of semantic terrains without resorting to the lumping that aggregation methods often impose.</p> <p>In this methodology, individual words are represented as nodes in a graph, while their corresponding embeddings serve as node features. The relationships or edges between these nodes are established based on features of semantic closeness. Graphs also allow for the use of multiple types of edges, representing varied semantic associations. Utilizing a temporal graph forecasting model, the methodology aspires to predict the dynamism of a target word's semantic relationships across time, with other words in the corpus. This allows for an effective capture and forecasting of word meaning transitions and relational evolutions. To elucidate, let's consider the lexeme web. Historically situated within a biological or fabric-oriented semantic realm, its associations were predominantly with entities such as spider or weave. In the graph representation, the node web during this epoch would exhibit strong edge weights with these terms. Yet, with the digital revolution and the advent of the internet, web began its semantic transition. If we were to examine its node in a more recent temporal slice of the graph, we'd anticipate strengthened connections with nodes representing browser, online, and internet. The temporal graph forecasting model, by leveraging historical data and current embeddings, could predict this evolving topography of edges for the web node, flagging its semantic shift. The graph-centric methodology for detecting semantic shifts, underpinned by embeddings as node features, embodies a promising departure from traditional techniques. However, its effective deployment demands a conscientious navigation of its inherent challenges, ensuring its robustness in varied linguistic landscapes.</p>"},{"location":"concepts/problem_statement/#project-objectives","title":"Project Objectives","text":"<p>The core ambition of this project is to construct a graph-based model that intricately captures the relationships between words, offering a solution that surpasses the constraints of traditional word embedding aggregation techniques. By representing words and their associated meanings in this interconnected format, the model aims to provide a deeper perspective on semantics.</p> <p>Introducing a temporal dimension becomes essential to capture the fluidity of language, reflecting its evolving nature. This dynamic representation will enable insights into how word meanings and associations shift across time, painting a comprehensive picture of linguistic transitions. Alongside this, effective node selection is paramount, as the vocabulary size of a corpus can be vast. By including words that mirror the topological characteristics or are proximal to the target word in the graph, the model aims to ensure that the graph is both comprehensive and precise in its semantic representation. Integrating techniques such as similarity metrics, distributional semantics, and unsupervised graph clustering, complemented by insights from domain experts can also enhance this representation.</p> <p>Forecasting is another crucial aspect. Utilizing temporal graph forecasting models, the objective is to predict potential shifts in word relationships and meanings across different chronological spans. This predictive approach is further enriched by tapping into external knowledge bases, such as WordNet and ConceptNet, which promise a deeper, layered understanding of semantic interconnections.</p> <p>Continuous evaluation forms the backbone of this endeavor. The outcomes derived from the initial model iterations will be subjected to rigorous assessment, leading to iterative refinements to enhance accuracy and maintain semantic fidelity. The graph-centric approach will be juxtaposed with traditional aggregation methods, allowing for a critical examination of the strengths, weaknesses, and unique insights each methodology brings to the fore. Finally, in recognizing the expansive nature of language, considerations of scalability and efficiency are paramount. This project is dedicated to ensuring that the developed model is adaptable to large vocabularies and complex relationships while optimizing computational demands.</p> <p>In conclusion, this project embarks on a journey to delve deep into the intricate tapestry of language, aiming to unravel broader semantic themes, linguistic trends, and the cultural implications of observed semantic shifts.</p> <ol> <li> <p>Adam Kilgarriff. \"i don't believe in word senses\". Computers and the Humanities, 31:, 04 1999.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"concepts/research_methodology/","title":"Research Methodology","text":"<p>The methodology devised for this research can be categorized into several phases, each building upon the previous to facilitate a comprehensive exploration of semantic shifts.</p> <p>Initially, for every period-specific corpus, we generate word embeddings. Employing neural network models facilitates the translation of textual data into high-dimensional vector spaces, ensuring that words are adequately represented in relation to their semantic and contextual attributes. This process yields a set of embeddings for each period, effectively capturing the diachronic nature of language.</p> <p>Post embedding generation, the subsequent task is to determine which words merit representation as nodes in the graph for every period. This decision is informed by a mix of criteria: the embeddings' inherent semantics, the frequency and distribution of words in the corpora, and potential significance in tracking semantic shifts. The aim is to ensure the graph's nodes are both representative of the period and pertinent to the overarching research objectives. One foundational strategy for node inclusion in graph-based semantic modeling is determining similarity scores, like cosine similarity, between the target word and the entire vocabulary. Words that exceed a set similarity threshold can be integrated into the graph. Additionally, the principle of distributional semantics suggests that words sharing contextual proximity often possess semantic congruence. Thus, including words that frequently co-occur within a specific window size of the target word enriches the semantic landscape. Furthermore, unsupervised graph clustering algorithms applied on word embeddings can delineate clusters of semantically related words, guiding node selection. Knowledge bases, such as WordNet or ConceptNet, offer repositories of predefined semantic relationships, allowing for the inclusion of semantically linked words to the target. Meanwhile, metrics like Point Mutual Information (PMI) can gauge the strength of word associations, emphasizing words that share strong contextual intersections with the target. Techniques such as topological data analysis emphasize structural semantics, while diversity sampling ensures a panoramic view of the semantic space. An iterative refinement based on initial outcomes, coupled with domain expert insights, further optimizes the graph's semantic representation.</p> <p>Edges in our graph are designed to represent the relationships between words (nodes). For each period, the edge features are constructed based on the cosine similarity or point-mutual-information between word embeddings, providing a metric of semantic closeness. Additionally, we categorize these relationships into various edge types, representing relationships such as synonymy, collocation, context words, or other semantic associations. Edge weights are then assigned, quantifying the strength of these relationships.</p> <p>With the graph structured, we proceed to model the temporal graph data. For this, we adopt a Dynamic Graph Neural Network infused with temporal signals, utilizing the PyTorch Geometric library. This state-of-the-art tool allows for the intricate handling of time-evolving graph data, accommodating the diachronic nuances of our dataset.</p> <p>Our inferential approach for semantic shifts spans several methodologies. Primarily, we predict future edge feature values between nodes, drawing parallels from methods employed in traffic forecasting tasks. This enables a projection into potential future linguistic trends, capturing shifts in word sense usage. Concurrently, we deploy time series anomaly detection, ensuring that any sudden or unprecedented semantic shifts are promptly flagged. Link prediction in graphs supplements this by highlighting how semantic relationships between words may evolve or transform over time. Lastly, for those periods where a threshold criteria is applied to streamline the graph, analyzing nodes that either emerge or fade becomes crucial. This offers insights into the entrance or exit of particular word meanings in the lexicon.</p> <p>In addition to the above methods, it might be beneficial to integrate community detection within the graph. This could identify clusters of words that share semantic trajectories, potentially unearthing broader linguistic themes or trends.</p> <p>In summary, this rigorous methodology, characterized by its depth and breadth, aspires to meticulously chart the intricate dynamics of semantic shifts across time, offering invaluable insights into the evolution of language and meaning.</p>"},{"location":"doc/data/load/","title":"Load","text":""},{"location":"doc/data/load/#semantics.data.data_loader.Loader","title":"<code>Loader</code>","text":"<p>Class for loading data.</p>"},{"location":"doc/data/load/#semantics.data.data_loader.Loader--methods","title":"Methods","text":"<pre><code>from_txt: Reads texts from a text file.\nfrom_xml: Reads texts from an XML file.\nforward: Filters the texts based on the target words and the maximum number of documents.\n</code></pre> Source code in <code>semantics/data/data_loader.py</code> <pre><code>class Loader():\n    \"\"\"\n    Class for loading data.\n\n    Methods\n    -------\n        from_txt: Reads texts from a text file.\n        from_xml: Reads texts from an XML file.\n        forward: Filters the texts based on the target words and the maximum number of documents.\n    \"\"\"\n    def __init__(\n            self,\n            texts: List[str]\n            ):\n        \"\"\"\n        Args:\n            texts (List[str]): List of texts.\n\n        Attributes:\n            texts (List[str]): List of texts.\n        \"\"\"\n\n        self.texts = texts\n\n\n    @classmethod\n    def from_txt(cls,\n                 path: Union[str, Path]\n                 ):\n        \"\"\"\n        Class method to read texts from a text file.\n\n        Args:\n            path (Union[str, Path]): Path to the text file.\n        \"\"\"\n        return cls(read_txt(path))\n\n    @classmethod\n    def from_xml(cls,\n                 path: Union[str, Path],\n                 tag: str\n                 ):\n        \"\"\"\n        Class method to read texts from an XML file.\n\n        Args:\n            path (Union[str, Path]): Path to the XML file.\n            tag (str): Tag of the XML file to extract the texts from.\n        \"\"\"\n        size = os.path.getsize(path)\n        if size &gt; 1e8:\n            raise ValueError(\"File size is too large. Please split the file into smaller files.\")\n        tree = ET.parse(path)\n        root = tree.getroot()\n        texts = []\n        for elem in root.findall('.//' + tag):\n            if isinstance(elem.text, str):\n                texts.append(elem.text)\n        return cls(texts)\n\n\n\n    def forward(\n            self, \n            target_words: Optional[Union[List[str], str]] = None, \n            max_documents: Optional[int] = None, \n            shuffle: bool = True, \n            random_seed: Optional[int] = None\n            ) -&gt; List[str]:\n        \"\"\"\n        Filters the texts based on the target words and the maximum number of documents.\n\n        Args:\n            target_words (List[str], str, None): List of target words. Defaults to None.\n            max_documents (int, None): Maximum number of documents. Defaults to None.\n            shuffle (bool): Whether to shuffle the data. Defaults to True.\n            random_seed (int, None): Random seed. Defaults to None.\n\n        Returns:\n            texts (List[str]): List of texts.\n\n\n        Examples:\n            &gt;&gt;&gt; from semantics.data.data_loader import Loader\n            &gt;&gt;&gt; texts = ['This is a test.', 'This is another test.', 'This is a third test.']\n            &gt;&gt;&gt; print('Original texts: ', texts)\n            &gt;&gt;&gt; print('Filtered texts: ', Loader(texts).forward(target_words=['third'], max_documents=1, shuffle=False))\n            Original texts:  ['This is a test.', 'This is another test.', 'This is a third test.\n            Filtered texts:  ['This is a third test.']\n        \"\"\"\n\n        if target_words:\n            relevant_texts = []\n            for text in self.texts:\n                if any([' ' + word + ' ' in text for word in target_words]):\n                    relevant_texts.append(text)\n\n            self.texts = relevant_texts\n\n        if max_documents is not None:\n            if shuffle:\n                self.texts = sample_data(self.texts, max_documents, random_seed)\n            else:\n                self.texts = self.texts[:max_documents]\n\n        return self.texts\n</code></pre>"},{"location":"doc/data/load/#semantics.data.data_loader.Loader.__init__","title":"<code>__init__(texts)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[str]</code> <p>List of texts.</p> required <p>Attributes:</p> Name Type Description <code>texts</code> <code>List[str]</code> <p>List of texts.</p> Source code in <code>semantics/data/data_loader.py</code> <pre><code>def __init__(\n        self,\n        texts: List[str]\n        ):\n    \"\"\"\n    Args:\n        texts (List[str]): List of texts.\n\n    Attributes:\n        texts (List[str]): List of texts.\n    \"\"\"\n\n    self.texts = texts\n</code></pre>"},{"location":"doc/data/load/#semantics.data.data_loader.Loader.forward","title":"<code>forward(target_words=None, max_documents=None, shuffle=True, random_seed=None)</code>","text":"<p>Filters the texts based on the target words and the maximum number of documents.</p> <p>Parameters:</p> Name Type Description Default <code>target_words</code> <code>(List[str], str, None)</code> <p>List of target words. Defaults to None.</p> <code>None</code> <code>max_documents</code> <code>(int, None)</code> <p>Maximum number of documents. Defaults to None.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data. Defaults to True.</p> <code>True</code> <code>random_seed</code> <code>(int, None)</code> <p>Random seed. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>texts</code> <code>List[str]</code> <p>List of texts.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from semantics.data.data_loader import Loader\n&gt;&gt;&gt; texts = ['This is a test.', 'This is another test.', 'This is a third test.']\n&gt;&gt;&gt; print('Original texts: ', texts)\n&gt;&gt;&gt; print('Filtered texts: ', Loader(texts).forward(target_words=['third'], max_documents=1, shuffle=False))\nOriginal texts:  ['This is a test.', 'This is another test.', 'This is a third test.\nFiltered texts:  ['This is a third test.']\n</code></pre> Source code in <code>semantics/data/data_loader.py</code> <pre><code>def forward(\n        self, \n        target_words: Optional[Union[List[str], str]] = None, \n        max_documents: Optional[int] = None, \n        shuffle: bool = True, \n        random_seed: Optional[int] = None\n        ) -&gt; List[str]:\n    \"\"\"\n    Filters the texts based on the target words and the maximum number of documents.\n\n    Args:\n        target_words (List[str], str, None): List of target words. Defaults to None.\n        max_documents (int, None): Maximum number of documents. Defaults to None.\n        shuffle (bool): Whether to shuffle the data. Defaults to True.\n        random_seed (int, None): Random seed. Defaults to None.\n\n    Returns:\n        texts (List[str]): List of texts.\n\n\n    Examples:\n        &gt;&gt;&gt; from semantics.data.data_loader import Loader\n        &gt;&gt;&gt; texts = ['This is a test.', 'This is another test.', 'This is a third test.']\n        &gt;&gt;&gt; print('Original texts: ', texts)\n        &gt;&gt;&gt; print('Filtered texts: ', Loader(texts).forward(target_words=['third'], max_documents=1, shuffle=False))\n        Original texts:  ['This is a test.', 'This is another test.', 'This is a third test.\n        Filtered texts:  ['This is a third test.']\n    \"\"\"\n\n    if target_words:\n        relevant_texts = []\n        for text in self.texts:\n            if any([' ' + word + ' ' in text for word in target_words]):\n                relevant_texts.append(text)\n\n        self.texts = relevant_texts\n\n    if max_documents is not None:\n        if shuffle:\n            self.texts = sample_data(self.texts, max_documents, random_seed)\n        else:\n            self.texts = self.texts[:max_documents]\n\n    return self.texts\n</code></pre>"},{"location":"doc/data/load/#semantics.data.data_loader.Loader.from_txt","title":"<code>from_txt(path)</code>  <code>classmethod</code>","text":"<p>Class method to read texts from a text file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to the text file.</p> required Source code in <code>semantics/data/data_loader.py</code> <pre><code>@classmethod\ndef from_txt(cls,\n             path: Union[str, Path]\n             ):\n    \"\"\"\n    Class method to read texts from a text file.\n\n    Args:\n        path (Union[str, Path]): Path to the text file.\n    \"\"\"\n    return cls(read_txt(path))\n</code></pre>"},{"location":"doc/data/load/#semantics.data.data_loader.Loader.from_xml","title":"<code>from_xml(path, tag)</code>  <code>classmethod</code>","text":"<p>Class method to read texts from an XML file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to the XML file.</p> required <code>tag</code> <code>str</code> <p>Tag of the XML file to extract the texts from.</p> required Source code in <code>semantics/data/data_loader.py</code> <pre><code>@classmethod\ndef from_xml(cls,\n             path: Union[str, Path],\n             tag: str\n             ):\n    \"\"\"\n    Class method to read texts from an XML file.\n\n    Args:\n        path (Union[str, Path]): Path to the XML file.\n        tag (str): Tag of the XML file to extract the texts from.\n    \"\"\"\n    size = os.path.getsize(path)\n    if size &gt; 1e8:\n        raise ValueError(\"File size is too large. Please split the file into smaller files.\")\n    tree = ET.parse(path)\n    root = tree.getroot()\n    texts = []\n    for elem in root.findall('.//' + tag):\n        if isinstance(elem.text, str):\n            texts.append(elem.text)\n    return cls(texts)\n</code></pre>"},{"location":"doc/data/load/#semantics.data.data_loader.split_xml","title":"<code>split_xml(path, output_dir, max_children=1000)</code>","text":"<p>Splits an XML file into multiple XML files with a maximum number of children.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the XML file.</p> required <code>output_dir</code> <code>str</code> <p>Path to the output directory.</p> required <code>max_children</code> <code>int</code> <p>Maximum number of children. Defaults to 1000.</p> <code>1000</code> <p>Returns:</p> Name Type Description <code>paths</code> <code>List[str]</code> <p>List of paths to the new XML files.</p> Source code in <code>semantics/data/data_loader.py</code> <pre><code>def split_xml(path:str, output_dir:str, max_children:int = 1000) -&gt; List[str]:\n    \"\"\"\n    Splits an XML file into multiple XML files with a maximum number of children.\n\n    Args:\n        path (str): Path to the XML file.\n        output_dir (str): Path to the output directory.\n        max_children (int, optional): Maximum number of children. Defaults to 1000.\n\n    Returns:\n        paths (List[str]): List of paths to the new XML files.\n    \"\"\"\n\n    # Parse the XML\n    tree = ET.parse(path)\n    root = tree.getroot()\n    file_name = Path(path).stem\n\n    paths = []\n    # Create new XML trees based on the split\n    for idx, i in enumerate(range(0, len(root), max_children)):\n        new_root = ET.Element(root.tag, root.attrib)\n        new_root.extend(root[i:i + max_children])\n        new_tree = ET.ElementTree(new_root)\n        new_tree.write(f\"{output_dir}/{file_name}_{idx}.xml\")\n        paths.append(f\"{output_dir}/{file_name}_{idx}.xml\")\n\n    return paths\n</code></pre>"},{"location":"doc/data/preprocess/","title":"Preprocess","text":""},{"location":"doc/data/preprocess/#semantics.data.data_preprocessing.PREPROCESS","title":"<code>PREPROCESS</code>","text":"<p>This class is used to preprocess text data.</p>"},{"location":"doc/data/preprocess/#semantics.data.data_preprocessing.PREPROCESS--methods","title":"Methods","text":"<pre><code>forward: Preprocesses text data.\n</code></pre> Source code in <code>semantics/data/data_preprocessing.py</code> <pre><code>class PREPROCESS():\n    \"\"\"\n    This class is used to preprocess text data.\n\n    Methods\n    -------\n        forward: Preprocesses text data.\n    \"\"\"\n    def __init__(self):\n        pass\n\n    def forward(self,\n                text,\n                remove_punctuation: bool = True,\n                remove_numbers: bool = True,\n                lowercase: bool = True,\n                lemmatize: bool = True,\n                remove_stopwords: bool = True,            \n        ):\n\n        \"\"\"\n        This function preprocesses text data.\n\n        Args:\n            text (str): Text to be preprocessed.\n            remove_punctuation (bool): Whether to remove punctuation. Defaults to True.\n            remove_numbers (bool): Whether to remove numbers. Defaults to True.\n            lowercase (bool): Whether to lowercase. Defaults to True.\n            lemmatize (bool): Whether to lemmatize. Defaults to True.\n            remove_stopwords (bool): Whether to remove stopwords. Defaults to True.\n\n        Returns:\n            newtext (str): Preprocessed text.\n\n        Examples:\n            &gt;&gt;&gt; from semantics.data.data_preprocessing import PREPROCESS\n            &gt;&gt;&gt; text = 'This is a test. 1234'\n            &gt;&gt;&gt; print('Original text: ', text)\n            &gt;&gt;&gt; print('Preprocessed text: ', PREPROCESS().forward(text, remove_punctuation=True, remove_numbers=True, lowercase=True, lemmatize=True, remove_stopwords=True))\n            Original text:  This is a test. 1234\n            Preprocessed text:  test\n        \"\"\"\n\n        newtext = re.sub('\\n', ' ', text) # Remove ordinary linebreaks (there shouldn't be, so this might be redundant)\n\n        if remove_punctuation:\n            newtext = re.sub(r'[^a-zA-Z0-9\\s\\.]', '', str(newtext)) # Remove anything that is not a space, a letter, a dot, or a number\n\n        if remove_numbers:\n            newtext = re.sub(r'[0-9]', '', str(newtext)) # Remove numbers\n\n        if lowercase:\n            newtext = str(newtext).lower() # Lowercase\n\n        if lemmatize:\n            from nltk.stem import WordNetLemmatizer\n            from nltk.corpus import wordnet \n\n            lemmatizer = WordNetLemmatizer()\n            newtext = ' '.join(list(map(lambda x: lemmatizer.lemmatize(x, \"n\"), newtext.split())))\n            # newtext = ' '.join(list(map(lambda x: lemmatizer.lemmatize(x, \"v\"), newtext.split())))\n\n        if remove_stopwords:\n            from nltk.corpus import stopwords\n            stop_words = set(stopwords.words('english'))\n            newtext = ' '.join(list(filter(lambda x: x not in stop_words, newtext.split())))\n\n        return newtext\n</code></pre>"},{"location":"doc/data/preprocess/#semantics.data.data_preprocessing.PREPROCESS.forward","title":"<code>forward(text, remove_punctuation=True, remove_numbers=True, lowercase=True, lemmatize=True, remove_stopwords=True)</code>","text":"<p>This function preprocesses text data.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to be preprocessed.</p> required <code>remove_punctuation</code> <code>bool</code> <p>Whether to remove punctuation. Defaults to True.</p> <code>True</code> <code>remove_numbers</code> <code>bool</code> <p>Whether to remove numbers. Defaults to True.</p> <code>True</code> <code>lowercase</code> <code>bool</code> <p>Whether to lowercase. Defaults to True.</p> <code>True</code> <code>lemmatize</code> <code>bool</code> <p>Whether to lemmatize. Defaults to True.</p> <code>True</code> <code>remove_stopwords</code> <code>bool</code> <p>Whether to remove stopwords. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>newtext</code> <code>str</code> <p>Preprocessed text.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from semantics.data.data_preprocessing import PREPROCESS\n&gt;&gt;&gt; text = 'This is a test. 1234'\n&gt;&gt;&gt; print('Original text: ', text)\n&gt;&gt;&gt; print('Preprocessed text: ', PREPROCESS().forward(text, remove_punctuation=True, remove_numbers=True, lowercase=True, lemmatize=True, remove_stopwords=True))\nOriginal text:  This is a test. 1234\nPreprocessed text:  test\n</code></pre> Source code in <code>semantics/data/data_preprocessing.py</code> <pre><code>def forward(self,\n            text,\n            remove_punctuation: bool = True,\n            remove_numbers: bool = True,\n            lowercase: bool = True,\n            lemmatize: bool = True,\n            remove_stopwords: bool = True,            \n    ):\n\n    \"\"\"\n    This function preprocesses text data.\n\n    Args:\n        text (str): Text to be preprocessed.\n        remove_punctuation (bool): Whether to remove punctuation. Defaults to True.\n        remove_numbers (bool): Whether to remove numbers. Defaults to True.\n        lowercase (bool): Whether to lowercase. Defaults to True.\n        lemmatize (bool): Whether to lemmatize. Defaults to True.\n        remove_stopwords (bool): Whether to remove stopwords. Defaults to True.\n\n    Returns:\n        newtext (str): Preprocessed text.\n\n    Examples:\n        &gt;&gt;&gt; from semantics.data.data_preprocessing import PREPROCESS\n        &gt;&gt;&gt; text = 'This is a test. 1234'\n        &gt;&gt;&gt; print('Original text: ', text)\n        &gt;&gt;&gt; print('Preprocessed text: ', PREPROCESS().forward(text, remove_punctuation=True, remove_numbers=True, lowercase=True, lemmatize=True, remove_stopwords=True))\n        Original text:  This is a test. 1234\n        Preprocessed text:  test\n    \"\"\"\n\n    newtext = re.sub('\\n', ' ', text) # Remove ordinary linebreaks (there shouldn't be, so this might be redundant)\n\n    if remove_punctuation:\n        newtext = re.sub(r'[^a-zA-Z0-9\\s\\.]', '', str(newtext)) # Remove anything that is not a space, a letter, a dot, or a number\n\n    if remove_numbers:\n        newtext = re.sub(r'[0-9]', '', str(newtext)) # Remove numbers\n\n    if lowercase:\n        newtext = str(newtext).lower() # Lowercase\n\n    if lemmatize:\n        from nltk.stem import WordNetLemmatizer\n        from nltk.corpus import wordnet \n\n        lemmatizer = WordNetLemmatizer()\n        newtext = ' '.join(list(map(lambda x: lemmatizer.lemmatize(x, \"n\"), newtext.split())))\n        # newtext = ' '.join(list(map(lambda x: lemmatizer.lemmatize(x, \"v\"), newtext.split())))\n\n    if remove_stopwords:\n        from nltk.corpus import stopwords\n        stop_words = set(stopwords.words('english'))\n        newtext = ' '.join(list(filter(lambda x: x not in stop_words, newtext.split())))\n\n    return newtext\n</code></pre>"},{"location":"doc/feature_extraction/bert/","title":"BERT","text":""},{"location":"doc/feature_extraction/bert/#semantics.feature_extraction.bert.BertEmbeddings","title":"<code>BertEmbeddings</code>","text":"<p>This class is used to infer the vector embeddings of a word from a sentence.</p>"},{"location":"doc/feature_extraction/bert/#semantics.feature_extraction.bert.BertEmbeddings--methods","title":"Methods","text":"<pre><code>infer_vector(doc:str, main_word:str)\n    This method is used to infer the vector embeddings of a word from a sentence.\n_bert_case_preparation()\n    This method is used to prepare the BERT model for the inference.\n</code></pre> Source code in <code>semantics/feature_extraction/bert.py</code> <pre><code>class BertEmbeddings:\n    \"\"\"\n    This class is used to infer the vector embeddings of a word from a sentence.\n\n    Methods\n    -------\n        infer_vector(doc:str, main_word:str)\n            This method is used to infer the vector embeddings of a word from a sentence.\n        _bert_case_preparation()\n            This method is used to prepare the BERT model for the inference.\n    \"\"\"\n    def __init__(\n        self,\n        pretrained_model_path:Union[str, Path] = None,\n    ):\n        self.model_path = pretrained_model_path\n        if pretrained_model_path is not None:\n            if not os.path.exists(pretrained_model_path):\n                raise ValueError(\n                    f'The path {pretrained_model_path} does not exist'\n                )\n            self.model_path = Path(pretrained_model_path)\n\n        self._tokens = []\n        self.model = None\n        self.vocab = False\n        self.lematizer = None\n\n        lg.set_verbosity_error()\n        self._bert_case_preparation()\n\n    @property\n    def tokens(self):\n        return self._tokens\n\n    def _bert_case_preparation(self) -&gt; None:\n        \"\"\"\n        This method is used to prepare the BERT model for the inference.\n        \"\"\"\n        model_path = self.model_path if self.model_path is not None else 'bert-base-uncased'\n        self.bert_tokenizer = BertTokenizer.from_pretrained(model_path)\n        self.model = BertModel.from_pretrained(\n            model_path,\n            output_hidden_states = True,\n        )\n        self.model.eval()\n        self.vocab = True\n\n    def infer_vector(self, doc:str, main_word:str):\n        \"\"\"\n        This method is used to infer the vector embeddings of a word from a sentence.\n        Args:\n            doc: Document to process\n            main_word: Main work to extract the vector embeddings for.\n\n        Returns: torch.Tensor\n\n        \"\"\"\n        if not self.vocab:\n            raise ValueError(\n                f'The Embedding model {self.model.__class__.__name__} has not been initialized'\n            )\n        marked_text = \"[CLS] \" + doc + \" [SEP]\"\n        tokens = self.bert_tokenizer.tokenize(marked_text)\n        try:\n            main_token_id = tokens.index(main_word.lower())\n            idx = self.bert_tokenizer.convert_tokens_to_ids(tokens)\n            segment_id = [1] * len(tokens)\n\n            self.tokens_tensor = torch.tensor([idx])\n            self.segments_tensors = torch.tensor([segment_id])\n\n            with torch.no_grad():\n                outputs = self.model(self.tokens_tensor, self.segments_tensors)\n                hidden_states = outputs[2]\n\n            return hidden_states[-2][0][main_token_id]\n\n        except ValueError:\n            raise ValueError(\n                f'The word: \"{main_word}\" does not exist in the list of tokens: {tokens} from {doc}'\n            )\n</code></pre>"},{"location":"doc/feature_extraction/bert/#semantics.feature_extraction.bert.BertEmbeddings.infer_vector","title":"<code>infer_vector(doc, main_word)</code>","text":"<p>This method is used to infer the vector embeddings of a word from a sentence. Args:     doc: Document to process     main_word: Main work to extract the vector embeddings for.</p> <p>Returns: torch.Tensor</p> Source code in <code>semantics/feature_extraction/bert.py</code> <pre><code>def infer_vector(self, doc:str, main_word:str):\n    \"\"\"\n    This method is used to infer the vector embeddings of a word from a sentence.\n    Args:\n        doc: Document to process\n        main_word: Main work to extract the vector embeddings for.\n\n    Returns: torch.Tensor\n\n    \"\"\"\n    if not self.vocab:\n        raise ValueError(\n            f'The Embedding model {self.model.__class__.__name__} has not been initialized'\n        )\n    marked_text = \"[CLS] \" + doc + \" [SEP]\"\n    tokens = self.bert_tokenizer.tokenize(marked_text)\n    try:\n        main_token_id = tokens.index(main_word.lower())\n        idx = self.bert_tokenizer.convert_tokens_to_ids(tokens)\n        segment_id = [1] * len(tokens)\n\n        self.tokens_tensor = torch.tensor([idx])\n        self.segments_tensors = torch.tensor([segment_id])\n\n        with torch.no_grad():\n            outputs = self.model(self.tokens_tensor, self.segments_tensors)\n            hidden_states = outputs[2]\n\n        return hidden_states[-2][0][main_token_id]\n\n    except ValueError:\n        raise ValueError(\n            f'The word: \"{main_word}\" does not exist in the list of tokens: {tokens} from {doc}'\n        )\n</code></pre>"},{"location":"doc/feature_extraction/roberta/","title":"RoBERTa","text":""},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.CustomDataset","title":"<code>CustomDataset</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>This class is used to create a custom dataset for the Roberta model. It inherits from torch.utils.data.Dataset.</p>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.CustomDataset--methods","title":"Methods","text":"<pre><code>__init__(data: List[str], tokenizer, max_length=128, truncation=True, padding=True)\n    The constructor for the CustomDataset class.\n__len__()\n    This method is used to get the length of the dataset.\n__getitem__(idx)\n    This method is used to get the item at a specific index.\n</code></pre> Source code in <code>semantics/feature_extraction/roberta.py</code> <pre><code>class CustomDataset(Dataset):\n    \"\"\"\n    This class is used to create a custom dataset for the Roberta model. It inherits from torch.utils.data.Dataset.\n\n    Methods\n    -------\n        __init__(data: List[str], tokenizer, max_length=128, truncation=True, padding=True)\n            The constructor for the CustomDataset class.\n        __len__()\n            This method is used to get the length of the dataset.\n        __getitem__(idx)\n            This method is used to get the item at a specific index.  \n    \"\"\"\n    def __init__(\n            self, \n            data: List[str], \n            tokenizer, \n            max_length=128,\n            truncation=True,\n            padding= \"max_length\",\n            ):\n        \"\"\"\n        Args:\n            data (List[str]): List of strings to create a dataset from.\n            tokenizer: Tokenizer to tokenize the data with.\n            max_length (int): Maximum length of the input sequence. Defaults to 128.\n            truncation (bool): Whether to truncate the input sequence to max_length or not. Defaults to True.\n            padding (str): Whether to pad the input sequence to max_length or not. Defaults to \"max_length\".\n\n        Attributes:\n            tokenizer: Tokenizer to tokenize the data with.\n            max_length (int): Maximum length of the input sequence. Defaults to 128.\n            tokenized_data (dict): Dictionary containing the input_ids, attention_mask, and labels. \n        \"\"\"\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.tokenized_data = tokenizer(data, truncation=truncation, padding=padding, max_length=max_length)\n\n    def __len__(self):\n        return len(self.tokenized_data.input_ids)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Retrieves the item at the specified index.\n\n        Parameters:\n            idx (int): Index of the item to retrieve.\n\n        Returns:\n            tokenized_data (dict): Dictionary containing the input_ids, attention_mask, and labels.\n        \"\"\"\n        # Get the tokenized inputs at the specified index\n        input_ids = self.tokenized_data.input_ids[idx]\n        attention_mask = self.tokenized_data.attention_mask[idx]\n\n        # Return a dictionary containing input_ids, attention_mask, and labels (if applicable)\n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask\n            # Add 'labels': labels if you have labels for your data\n        }\n</code></pre>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.CustomDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Retrieves the item at the specified index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the item to retrieve.</p> required <p>Returns:</p> Name Type Description <code>tokenized_data</code> <code>dict</code> <p>Dictionary containing the input_ids, attention_mask, and labels.</p> Source code in <code>semantics/feature_extraction/roberta.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"\n    Retrieves the item at the specified index.\n\n    Parameters:\n        idx (int): Index of the item to retrieve.\n\n    Returns:\n        tokenized_data (dict): Dictionary containing the input_ids, attention_mask, and labels.\n    \"\"\"\n    # Get the tokenized inputs at the specified index\n    input_ids = self.tokenized_data.input_ids[idx]\n    attention_mask = self.tokenized_data.attention_mask[idx]\n\n    # Return a dictionary containing input_ids, attention_mask, and labels (if applicable)\n    return {\n        'input_ids': input_ids,\n        'attention_mask': attention_mask\n        # Add 'labels': labels if you have labels for your data\n    }\n</code></pre>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.CustomDataset.__init__","title":"<code>__init__(data, tokenizer, max_length=128, truncation=True, padding='max_length')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[str]</code> <p>List of strings to create a dataset from.</p> required <code>tokenizer</code> <p>Tokenizer to tokenize the data with.</p> required <code>max_length</code> <code>int</code> <p>Maximum length of the input sequence. Defaults to 128.</p> <code>128</code> <code>truncation</code> <code>bool</code> <p>Whether to truncate the input sequence to max_length or not. Defaults to True.</p> <code>True</code> <code>padding</code> <code>str</code> <p>Whether to pad the input sequence to max_length or not. Defaults to \"max_length\".</p> <code>'max_length'</code> <p>Attributes:</p> Name Type Description <code>tokenizer</code> <p>Tokenizer to tokenize the data with.</p> <code>max_length</code> <code>int</code> <p>Maximum length of the input sequence. Defaults to 128.</p> <code>tokenized_data</code> <code>dict</code> <p>Dictionary containing the input_ids, attention_mask, and labels.</p> Source code in <code>semantics/feature_extraction/roberta.py</code> <pre><code>def __init__(\n        self, \n        data: List[str], \n        tokenizer, \n        max_length=128,\n        truncation=True,\n        padding= \"max_length\",\n        ):\n    \"\"\"\n    Args:\n        data (List[str]): List of strings to create a dataset from.\n        tokenizer: Tokenizer to tokenize the data with.\n        max_length (int): Maximum length of the input sequence. Defaults to 128.\n        truncation (bool): Whether to truncate the input sequence to max_length or not. Defaults to True.\n        padding (str): Whether to pad the input sequence to max_length or not. Defaults to \"max_length\".\n\n    Attributes:\n        tokenizer: Tokenizer to tokenize the data with.\n        max_length (int): Maximum length of the input sequence. Defaults to 128.\n        tokenized_data (dict): Dictionary containing the input_ids, attention_mask, and labels. \n    \"\"\"\n    self.tokenizer = tokenizer\n    self.max_length = max_length\n    self.tokenized_data = tokenizer(data, truncation=truncation, padding=padding, max_length=max_length)\n</code></pre>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaEmbedding","title":"<code>RobertaEmbedding</code>","text":"<p>This class is used to infer vector embeddings from a document.</p>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaEmbedding--methods","title":"Methods","text":"<pre><code>__init__(pretrained_model_path:Union[str, Path] = None)\n    The constructor for the VectorEmbeddings class.\n_roberta_case_preparation()\n    This method is used to prepare the Roberta model for the inference.\ninfer_vector(doc:str, main_word:str)\n    This method is used to infer the vector embeddings of a word from a document.\ninfer_mask_logits(doc:str)\n    This method is used to infer the logits of a word from a document.\n</code></pre> Source code in <code>semantics/feature_extraction/roberta.py</code> <pre><code>class RobertaEmbedding:\n    \"\"\"\n    This class is used to infer vector embeddings from a document.\n\n    Methods\n    -------\n        __init__(pretrained_model_path:Union[str, Path] = None)\n            The constructor for the VectorEmbeddings class.\n        _roberta_case_preparation()\n            This method is used to prepare the Roberta model for the inference.\n        infer_vector(doc:str, main_word:str)\n            This method is used to infer the vector embeddings of a word from a document.\n        infer_mask_logits(doc:str)\n            This method is used to infer the logits of a word from a document.\n    \"\"\"\n    def __init__(\n        self,\n        pretrained_model_path: Optional[Union[str, Path]] = None,\n    ):\n        \"\"\"\n        Args:\n            pretrained_model_path (str, Path, None): Path to the pretrained model. Defaults to None.\n\n        Attributes:\n            model_path (str, Path, None): Path to the pretrained model. Defaults to None.\n            model (transformers.RobertaModel): RobertaModel object to infer vector embeddings from.\n            MLM (transformers.RobertaForMaskedLM): RobertaForMaskedLM object to infer vector embeddings from.\n            tokenizer (transformers.RobertaTokenizer): Tokenizer to tokenize the data with.\n            max_length (int): Maximum length of the input sequence. Defaults to 128.\n            vocab (bool): Whether the model has been initialized or not.\n        \"\"\"\n        self.model_path = pretrained_model_path\n        if pretrained_model_path is not None:\n            if not os.path.exists(pretrained_model_path):\n                raise ValueError(\n                    f'The path {pretrained_model_path} does not exist'\n                )\n            self.model_path = Path(pretrained_model_path)\n\n        self._tokens = []\n        self.model = None\n        self.vocab = False\n\n        lg.set_verbosity_error()\n        self._roberta_case_preparation()\n\n    @property\n    def tokens(self):\n        return self._tokens\n\n    def _roberta_case_preparation(self) -&gt; None:\n        \"\"\"\n        This method is used to prepare the BERT model for the inference.\n        \"\"\"\n        model_path = self.model_path if self.model_path is not None else 'roberta-base'\n        self.tokenizer = RobertaTokenizer.from_pretrained(model_path)\n        self.model = RobertaModel.from_pretrained(\n            model_path, \n            output_hidden_states=True\n            )\n        self.MLM = RobertaForMaskedLM.from_pretrained(\n            model_path\n        )\n        # self.model_max_length = self.model.config.max_position_embeddings\n        # self.mlm_max_length = self.MLM.config.max_position_embeddings\n        self.model.eval()\n        self.MLM.eval()\n        self.vocab = True\n\n    def infer_vector(self, doc:str, main_word:str) -&gt; torch.Tensor:\n        \"\"\"\n        This method is used to infer the vector embeddings of a word from a document.\n\n        Args:\n            doc (str): Document to process\n            main_word (str): Main work to extract the vector embeddings for.\n\n        Returns: \n            embeddings (torch.Tensor): Tensor of stacked embeddings of shape (num_embeddings, embedding_size) where num_embeddings is the number of times the main_word appears in the doc.\n\n        Examples:\n            &gt;&gt;&gt; model = RobertaEmbedding()\n            &gt;&gt;&gt; model.infer_vector(doc=\"The brown fox jumps over the lazy dog\", main_word=\"fox\")\n            tensor([[-0.2182, ..., -0.1709],\n                    ...,\n                    [-0.2182, ..., -0.1706]])\n        \"\"\"\n        if not self.vocab:\n            raise ValueError(\n                f'The Embedding model {self.model.__class__.__name__} has not been initialized'\n            )\n\n\n        input_ids = self.tokenizer(doc, return_tensors=\"pt\", max_length=512, truncation=True).input_ids\n        token = self.tokenizer.encode(main_word, add_special_tokens=False)[0]\n\n        word_token_index = torch.where(input_ids == token)[1]\n        emb = []\n\n        try:\n            with torch.no_grad():\n                embeddings = self.model(input_ids).last_hidden_state\n\n            emb = [embeddings[0, idx] for idx in word_token_index]\n            return torch.stack(emb)\n\n        except:\n            print(f'The word: \"{main_word}\" does not exist in the list of tokens')\n            return torch.tensor(np.array(emb))\n\n\n\n\n    def infer_mask_logits(self, doc:str) -&gt; torch.Tensor:\n        \"\"\"\n        This method is used to infer the logits of the mask token in a document.\n\n        Args:\n            doc (str): Document to process where the mask token is present.\n\n        Returns: \n            logits (torch.Tensor): Tensor of stacked logits of shape (num_embeddings, logits_size) where num_embeddings is the number of times the mask token appears in the doc withing the max_length.\n\n        Examples:\n            &gt;&gt;&gt; model = RobertaEmbedding()\n            &gt;&gt;&gt; model.infer_mask_logits(doc=\"The brown fox &lt;mask&gt; over the lazy dog\")\n            tensor([[-2.1816e-01,  ..., -1.7064e-01],\n                    ...,\n                    [-2.1816e-01, ..., -1.7093e-01]])\n        \"\"\"\n\n        if not self.vocab:\n            raise ValueError(\n                f'The Embedding model {self.MLM.__class__.__name__} has not been initialized'\n            )\n\n        input_ids = self.tokenizer(doc, return_tensors=\"pt\", max_length= 512, truncation=True).input_ids\n        mask_token_index = torch.where(input_ids == self.tokenizer.mask_token_id)[1]\n        l = []\n        try:\n            with torch.no_grad():\n                logits = self.MLM(input_ids).logits\n\n            l = [logits[0, idx] for idx in mask_token_index]\n            return torch.stack(l) if len(l) &gt; 0 else torch.empty(0)\n\n        except IndexError:\n            raise ValueError(f'The mask falls outside of the max length of {512}, please use a smaller document')\n</code></pre>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaEmbedding.__init__","title":"<code>__init__(pretrained_model_path=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pretrained_model_path</code> <code>(str, Path, None)</code> <p>Path to the pretrained model. Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>model_path</code> <code>(str, Path, None)</code> <p>Path to the pretrained model. Defaults to None.</p> <code>model</code> <code>RobertaModel</code> <p>RobertaModel object to infer vector embeddings from.</p> <code>MLM</code> <code>RobertaForMaskedLM</code> <p>RobertaForMaskedLM object to infer vector embeddings from.</p> <code>tokenizer</code> <code>RobertaTokenizer</code> <p>Tokenizer to tokenize the data with.</p> <code>max_length</code> <code>int</code> <p>Maximum length of the input sequence. Defaults to 128.</p> <code>vocab</code> <code>bool</code> <p>Whether the model has been initialized or not.</p> Source code in <code>semantics/feature_extraction/roberta.py</code> <pre><code>def __init__(\n    self,\n    pretrained_model_path: Optional[Union[str, Path]] = None,\n):\n    \"\"\"\n    Args:\n        pretrained_model_path (str, Path, None): Path to the pretrained model. Defaults to None.\n\n    Attributes:\n        model_path (str, Path, None): Path to the pretrained model. Defaults to None.\n        model (transformers.RobertaModel): RobertaModel object to infer vector embeddings from.\n        MLM (transformers.RobertaForMaskedLM): RobertaForMaskedLM object to infer vector embeddings from.\n        tokenizer (transformers.RobertaTokenizer): Tokenizer to tokenize the data with.\n        max_length (int): Maximum length of the input sequence. Defaults to 128.\n        vocab (bool): Whether the model has been initialized or not.\n    \"\"\"\n    self.model_path = pretrained_model_path\n    if pretrained_model_path is not None:\n        if not os.path.exists(pretrained_model_path):\n            raise ValueError(\n                f'The path {pretrained_model_path} does not exist'\n            )\n        self.model_path = Path(pretrained_model_path)\n\n    self._tokens = []\n    self.model = None\n    self.vocab = False\n\n    lg.set_verbosity_error()\n    self._roberta_case_preparation()\n</code></pre>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaEmbedding.infer_mask_logits","title":"<code>infer_mask_logits(doc)</code>","text":"<p>This method is used to infer the logits of the mask token in a document.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>str</code> <p>Document to process where the mask token is present.</p> required <p>Returns:</p> Name Type Description <code>logits</code> <code>Tensor</code> <p>Tensor of stacked logits of shape (num_embeddings, logits_size) where num_embeddings is the number of times the mask token appears in the doc withing the max_length.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = RobertaEmbedding()\n&gt;&gt;&gt; model.infer_mask_logits(doc=\"The brown fox &lt;mask&gt; over the lazy dog\")\ntensor([[-2.1816e-01,  ..., -1.7064e-01],\n        ...,\n        [-2.1816e-01, ..., -1.7093e-01]])\n</code></pre> Source code in <code>semantics/feature_extraction/roberta.py</code> <pre><code>def infer_mask_logits(self, doc:str) -&gt; torch.Tensor:\n    \"\"\"\n    This method is used to infer the logits of the mask token in a document.\n\n    Args:\n        doc (str): Document to process where the mask token is present.\n\n    Returns: \n        logits (torch.Tensor): Tensor of stacked logits of shape (num_embeddings, logits_size) where num_embeddings is the number of times the mask token appears in the doc withing the max_length.\n\n    Examples:\n        &gt;&gt;&gt; model = RobertaEmbedding()\n        &gt;&gt;&gt; model.infer_mask_logits(doc=\"The brown fox &lt;mask&gt; over the lazy dog\")\n        tensor([[-2.1816e-01,  ..., -1.7064e-01],\n                ...,\n                [-2.1816e-01, ..., -1.7093e-01]])\n    \"\"\"\n\n    if not self.vocab:\n        raise ValueError(\n            f'The Embedding model {self.MLM.__class__.__name__} has not been initialized'\n        )\n\n    input_ids = self.tokenizer(doc, return_tensors=\"pt\", max_length= 512, truncation=True).input_ids\n    mask_token_index = torch.where(input_ids == self.tokenizer.mask_token_id)[1]\n    l = []\n    try:\n        with torch.no_grad():\n            logits = self.MLM(input_ids).logits\n\n        l = [logits[0, idx] for idx in mask_token_index]\n        return torch.stack(l) if len(l) &gt; 0 else torch.empty(0)\n\n    except IndexError:\n        raise ValueError(f'The mask falls outside of the max length of {512}, please use a smaller document')\n</code></pre>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaEmbedding.infer_vector","title":"<code>infer_vector(doc, main_word)</code>","text":"<p>This method is used to infer the vector embeddings of a word from a document.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>str</code> <p>Document to process</p> required <code>main_word</code> <code>str</code> <p>Main work to extract the vector embeddings for.</p> required <p>Returns:</p> Name Type Description <code>embeddings</code> <code>Tensor</code> <p>Tensor of stacked embeddings of shape (num_embeddings, embedding_size) where num_embeddings is the number of times the main_word appears in the doc.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = RobertaEmbedding()\n&gt;&gt;&gt; model.infer_vector(doc=\"The brown fox jumps over the lazy dog\", main_word=\"fox\")\ntensor([[-0.2182, ..., -0.1709],\n        ...,\n        [-0.2182, ..., -0.1706]])\n</code></pre> Source code in <code>semantics/feature_extraction/roberta.py</code> <pre><code>def infer_vector(self, doc:str, main_word:str) -&gt; torch.Tensor:\n    \"\"\"\n    This method is used to infer the vector embeddings of a word from a document.\n\n    Args:\n        doc (str): Document to process\n        main_word (str): Main work to extract the vector embeddings for.\n\n    Returns: \n        embeddings (torch.Tensor): Tensor of stacked embeddings of shape (num_embeddings, embedding_size) where num_embeddings is the number of times the main_word appears in the doc.\n\n    Examples:\n        &gt;&gt;&gt; model = RobertaEmbedding()\n        &gt;&gt;&gt; model.infer_vector(doc=\"The brown fox jumps over the lazy dog\", main_word=\"fox\")\n        tensor([[-0.2182, ..., -0.1709],\n                ...,\n                [-0.2182, ..., -0.1706]])\n    \"\"\"\n    if not self.vocab:\n        raise ValueError(\n            f'The Embedding model {self.model.__class__.__name__} has not been initialized'\n        )\n\n\n    input_ids = self.tokenizer(doc, return_tensors=\"pt\", max_length=512, truncation=True).input_ids\n    token = self.tokenizer.encode(main_word, add_special_tokens=False)[0]\n\n    word_token_index = torch.where(input_ids == token)[1]\n    emb = []\n\n    try:\n        with torch.no_grad():\n            embeddings = self.model(input_ids).last_hidden_state\n\n        emb = [embeddings[0, idx] for idx in word_token_index]\n        return torch.stack(emb)\n\n    except:\n        print(f'The word: \"{main_word}\" does not exist in the list of tokens')\n        return torch.tensor(np.array(emb))\n</code></pre>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaInference","title":"<code>RobertaInference</code>","text":"<p>Wrapper class for the RobertaEmbedding class for inference.</p>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaInference--methods","title":"Methods","text":"<pre><code>__init__(pretrained_model_path:Union[str, Path] = None)\n    The constructor for the VectorEmbeddings class.\n_roberta_case_preparation()\n    This method is used to prepare the Roberta model for the inference.\nget_embedding(word:str, doc: Optional[Union[str, List[str]]] = None,, mask:bool=False)\n    This method is used to infer the vector embeddings of a word from a document.\nget_top_k_words(word:str, doc:str, k:int=3)\n    This method is used to infer the vector embeddings of a word from a document.\n</code></pre> Source code in <code>semantics/feature_extraction/roberta.py</code> <pre><code>class RobertaInference:\n    \"\"\"\n    Wrapper class for the RobertaEmbedding class for inference.\n\n    Methods\n    -------\n        __init__(pretrained_model_path:Union[str, Path] = None)\n            The constructor for the VectorEmbeddings class.\n        _roberta_case_preparation()\n            This method is used to prepare the Roberta model for the inference.\n        get_embedding(word:str, doc: Optional[Union[str, List[str]]] = None,, mask:bool=False)\n            This method is used to infer the vector embeddings of a word from a document.\n        get_top_k_words(word:str, doc:str, k:int=3)\n            This method is used to infer the vector embeddings of a word from a document.\n    \"\"\"\n\n    def __init__(\n            self,\n            pretrained_model_path:Union[str, Path] = None,\n    ):\n        \"\"\"\n        Args:\n            pretrained_model_path (str, Path, None): Path to the pretrained model. Defaults to None.\n\n        Attributes:\n            model_path (str, Path, None): Path to the pretrained model. Defaults to None.\n            word_vectorizor (RobertaEmbedding): RobertaEmbedding object to infer vector embeddings from.\n            vocab (bool): Whether the model has been initialized or not.\n        \"\"\"\n        self.model_path = pretrained_model_path\n        if pretrained_model_path is not None:\n            if not os.path.exists(pretrained_model_path):\n                raise ValueError(\n                    f'The path {pretrained_model_path} does not exist'\n                )\n            self.model_path = Path(pretrained_model_path)\n\n        self.word_vectorizor = None\n        self.vocab = False\n\n\n        lg.set_verbosity_error()\n        self._roberta_case_preparation()\n\n\n    def _roberta_case_preparation(self) -&gt; None:\n        \"\"\"\n        This method is used to prepare the Roberta model for the inference.\n        \"\"\"\n        model_path = self.model_path if self.model_path is not None else 'roberta-base'\n        self.tokenizer = RobertaTokenizer.from_pretrained(model_path)\n        self.word_vectorizor = RobertaEmbedding(pretrained_model_path=model_path)\n        self.vocab = True\n\n\n    def get_embedding(\n            self,\n            main_word : str, \n            doc: Optional[Union[str, List[str]]] = None,\n            mask : bool = False\n            ) -&gt; torch.Tensor:\n\n        \"\"\"\n        This method is used to infer the vector embeddings of a word from a document.\n\n        Args:\n            main_word (str): Word to get the vector embeddings for\n            doc (str, List[str], None): Documents to get the vector embeddings of the main_word from. If None, the document is the main_word itself. Defaults to None.\n            mask: Whether to mask the main_word in the documents or not. Defaults to False.\n\n        Returns: \n            embeddings (torch.Tensor): Tensor of stacked embeddings of shape (num_embeddings, embedding_size) where num_embeddings is the number of times the main_word appears in the doc, depending on the mask parameter.\n\n        Examples:\n            &gt;&gt;&gt; model = RobertaInference()\n            &gt;&gt;&gt; model.get_embedding(main_word=\"office\", doc=\"The brown office is very big\", mask=False)\n            tensor([[-0.2182, ..., -0.1709],\n                    ...,\n                    [-0.2182, ..., -0.1706]])\n        \"\"\"\n\n        if not self.vocab:\n            raise ValueError(\n                f'The Embedding model {self.model.__class__.__name__} has not been initialized'\n            )\n\n        if doc is None:\n            doc = ' ' + main_word.strip() + ' '\n\n        if mask:\n            doc = doc.replace(main_word, self.tokenizer.mask_token)\n            main_word = self.tokenizer.mask_token\n\n        else:\n            main_word = ' ' + main_word.strip()\n\n        embeddings = self.word_vectorizor.infer_vector(doc=doc, main_word=main_word)\n        return embeddings\n\n    def get_top_k_words(\n            self,\n            main_word : str,\n            doc: str,\n            k: int = 3\n            ) -&gt; List[str]:\n        \"\"\"\n        This method is used to infer the vector embeddings of a main_word from a document.\n        Args:\n            main_word: Word to mask\n            doc: Document to infer the top k words of the main_word from\n            k: Number of top words to return\n\n        Returns:\n            top_k_words (List[str]): List of top k words\n\n        Examples:\n            &gt;&gt;&gt; model = RobertaInference()\n            &gt;&gt;&gt; model.get_top_k_words(main_word=\"office\", doc=\"The brown office is very big\")\n            ['room', 'eye', 'bear']\n        \"\"\"\n        if not self.vocab:\n            raise ValueError(\n                f'The Embedding model {self.model.__class__.__name__} has not been initialized'\n            )\n\n        masked_doc = doc.replace(main_word, '&lt;mask&gt;')\n        try:\n            logits = self.word_vectorizor.infer_mask_logits(doc=masked_doc)\n            top_k = []\n\n            for logit_set in logits:\n                top_k_tokens = torch.topk(logit_set, k).indices\n                top_k_words = [self.tokenizer.decode(token.item()).strip() for token in top_k_tokens]\n\n                top_k.extend(top_k_words)\n\n            return top_k\n\n        except ValueError:\n            print(f'The word: \"{main_word}\" does not exist in the list of tokens')\n            return []\n</code></pre>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaInference.__init__","title":"<code>__init__(pretrained_model_path=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pretrained_model_path</code> <code>(str, Path, None)</code> <p>Path to the pretrained model. Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>model_path</code> <code>(str, Path, None)</code> <p>Path to the pretrained model. Defaults to None.</p> <code>word_vectorizor</code> <code>RobertaEmbedding</code> <p>RobertaEmbedding object to infer vector embeddings from.</p> <code>vocab</code> <code>bool</code> <p>Whether the model has been initialized or not.</p> Source code in <code>semantics/feature_extraction/roberta.py</code> <pre><code>def __init__(\n        self,\n        pretrained_model_path:Union[str, Path] = None,\n):\n    \"\"\"\n    Args:\n        pretrained_model_path (str, Path, None): Path to the pretrained model. Defaults to None.\n\n    Attributes:\n        model_path (str, Path, None): Path to the pretrained model. Defaults to None.\n        word_vectorizor (RobertaEmbedding): RobertaEmbedding object to infer vector embeddings from.\n        vocab (bool): Whether the model has been initialized or not.\n    \"\"\"\n    self.model_path = pretrained_model_path\n    if pretrained_model_path is not None:\n        if not os.path.exists(pretrained_model_path):\n            raise ValueError(\n                f'The path {pretrained_model_path} does not exist'\n            )\n        self.model_path = Path(pretrained_model_path)\n\n    self.word_vectorizor = None\n    self.vocab = False\n\n\n    lg.set_verbosity_error()\n    self._roberta_case_preparation()\n</code></pre>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaInference.get_embedding","title":"<code>get_embedding(main_word, doc=None, mask=False)</code>","text":"<p>This method is used to infer the vector embeddings of a word from a document.</p> <p>Parameters:</p> Name Type Description Default <code>main_word</code> <code>str</code> <p>Word to get the vector embeddings for</p> required <code>doc</code> <code>(str, List[str], None)</code> <p>Documents to get the vector embeddings of the main_word from. If None, the document is the main_word itself. Defaults to None.</p> <code>None</code> <code>mask</code> <code>bool</code> <p>Whether to mask the main_word in the documents or not. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>embeddings</code> <code>Tensor</code> <p>Tensor of stacked embeddings of shape (num_embeddings, embedding_size) where num_embeddings is the number of times the main_word appears in the doc, depending on the mask parameter.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = RobertaInference()\n&gt;&gt;&gt; model.get_embedding(main_word=\"office\", doc=\"The brown office is very big\", mask=False)\ntensor([[-0.2182, ..., -0.1709],\n        ...,\n        [-0.2182, ..., -0.1706]])\n</code></pre> Source code in <code>semantics/feature_extraction/roberta.py</code> <pre><code>def get_embedding(\n        self,\n        main_word : str, \n        doc: Optional[Union[str, List[str]]] = None,\n        mask : bool = False\n        ) -&gt; torch.Tensor:\n\n    \"\"\"\n    This method is used to infer the vector embeddings of a word from a document.\n\n    Args:\n        main_word (str): Word to get the vector embeddings for\n        doc (str, List[str], None): Documents to get the vector embeddings of the main_word from. If None, the document is the main_word itself. Defaults to None.\n        mask: Whether to mask the main_word in the documents or not. Defaults to False.\n\n    Returns: \n        embeddings (torch.Tensor): Tensor of stacked embeddings of shape (num_embeddings, embedding_size) where num_embeddings is the number of times the main_word appears in the doc, depending on the mask parameter.\n\n    Examples:\n        &gt;&gt;&gt; model = RobertaInference()\n        &gt;&gt;&gt; model.get_embedding(main_word=\"office\", doc=\"The brown office is very big\", mask=False)\n        tensor([[-0.2182, ..., -0.1709],\n                ...,\n                [-0.2182, ..., -0.1706]])\n    \"\"\"\n\n    if not self.vocab:\n        raise ValueError(\n            f'The Embedding model {self.model.__class__.__name__} has not been initialized'\n        )\n\n    if doc is None:\n        doc = ' ' + main_word.strip() + ' '\n\n    if mask:\n        doc = doc.replace(main_word, self.tokenizer.mask_token)\n        main_word = self.tokenizer.mask_token\n\n    else:\n        main_word = ' ' + main_word.strip()\n\n    embeddings = self.word_vectorizor.infer_vector(doc=doc, main_word=main_word)\n    return embeddings\n</code></pre>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaInference.get_top_k_words","title":"<code>get_top_k_words(main_word, doc, k=3)</code>","text":"<p>This method is used to infer the vector embeddings of a main_word from a document. Args:     main_word: Word to mask     doc: Document to infer the top k words of the main_word from     k: Number of top words to return</p> <p>Returns:</p> Name Type Description <code>top_k_words</code> <code>List[str]</code> <p>List of top k words</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = RobertaInference()\n&gt;&gt;&gt; model.get_top_k_words(main_word=\"office\", doc=\"The brown office is very big\")\n['room', 'eye', 'bear']\n</code></pre> Source code in <code>semantics/feature_extraction/roberta.py</code> <pre><code>def get_top_k_words(\n        self,\n        main_word : str,\n        doc: str,\n        k: int = 3\n        ) -&gt; List[str]:\n    \"\"\"\n    This method is used to infer the vector embeddings of a main_word from a document.\n    Args:\n        main_word: Word to mask\n        doc: Document to infer the top k words of the main_word from\n        k: Number of top words to return\n\n    Returns:\n        top_k_words (List[str]): List of top k words\n\n    Examples:\n        &gt;&gt;&gt; model = RobertaInference()\n        &gt;&gt;&gt; model.get_top_k_words(main_word=\"office\", doc=\"The brown office is very big\")\n        ['room', 'eye', 'bear']\n    \"\"\"\n    if not self.vocab:\n        raise ValueError(\n            f'The Embedding model {self.model.__class__.__name__} has not been initialized'\n        )\n\n    masked_doc = doc.replace(main_word, '&lt;mask&gt;')\n    try:\n        logits = self.word_vectorizor.infer_mask_logits(doc=masked_doc)\n        top_k = []\n\n        for logit_set in logits:\n            top_k_tokens = torch.topk(logit_set, k).indices\n            top_k_words = [self.tokenizer.decode(token.item()).strip() for token in top_k_tokens]\n\n            top_k.extend(top_k_words)\n\n        return top_k\n\n    except ValueError:\n        print(f'The word: \"{main_word}\" does not exist in the list of tokens')\n        return []\n</code></pre>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaTrainer","title":"<code>RobertaTrainer</code>","text":"<p>This class is used to train a Roberta model.</p>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaTrainer--methods","title":"Methods","text":"<pre><code>__init__(model_name=\"roberta-base\", max_length=128, mlm_probability=0.15, batch_size=4, learning_rate=1e-5, epochs=3, warmup_steps=500, split_ratio=0.8)\n    The constructor for the RobertaTrainer class.\nprepare_dataset(data)\n    This method is used to prepare the dataset for training.\ntrain(data, output_dir: Union[str, Path] = None)\n    This method is used to train the model.\n</code></pre> Source code in <code>semantics/feature_extraction/roberta.py</code> <pre><code>class RobertaTrainer:\n    \"\"\"\n    This class is used to train a Roberta model.\n\n    Methods\n    -------\n        __init__(model_name=\"roberta-base\", max_length=128, mlm_probability=0.15, batch_size=4, learning_rate=1e-5, epochs=3, warmup_steps=500, split_ratio=0.8)\n            The constructor for the RobertaTrainer class.\n        prepare_dataset(data)\n            This method is used to prepare the dataset for training.\n        train(data, output_dir: Union[str, Path] = None)\n            This method is used to train the model.\n    \"\"\"\n    def __init__(\n            self, \n            model_name: str = \"roberta-base\", \n            max_length: int = 128, \n            mlm_probability: float = 0.15, \n            batch_size: int = 4, \n            learning_rate: float = 1e-5, \n            epochs: int = 3, \n            warmup_steps: int = 500, \n            split_ratio: float = 0.8, \n            truncation: bool = True, \n            padding: str = \"max_length\"\n            ):\n\n        \"\"\"\n        Args:\n            model_name (str): Name of the model to train. Defaults to \"roberta-base\".\n            max_length (int): Maximum length of the input sequence. Defaults to 128.\n            mlm_probability (float): Probability of masking tokens in the input sequence. Defaults to 0.15.\n            batch_size (int): Size of the batch. Defaults to 4.\n            learning_rate (float): Learning rate of the optimizer. Defaults to 1e-5.\n            epochs (int): Number of epochs to train the model for. Defaults to 3.\n            warmup_steps (int): Number of warmup steps for the learning rate scheduler. Defaults to 500.\n            split_ratio (float): Ratio to split the data into train and test. Defaults to 0.8.\n            truncation (bool): Whether to truncate the input sequence to max_length or not. Defaults to True.\n            padding (str): Whether to pad the input sequence to max_length or not. Defaults to \"max_length\".\n\n        Attributes:\n            tokenizer (transformers.RobertaTokenizer): Tokenizer to tokenize the data with.\n            model (transformers.RobertaForMaskedLM): Model to train.\n            data_collator (transformers.DataCollatorForLanguageModeling): DataCollatorForLanguageModeling object to collate the data.\n            split_ratio (float): Ratio to split the data into train and test. Defaults to 0.8.\n            truncation (bool): Whether to truncate the input sequence to max_length or not. Defaults to True.\n            padding (str): Whether to pad the input sequence to max_length or not. Defaults to \"max_length\".\n            max_length (int): Maximum length of the input sequence. Defaults to 128.\n            batch_size (int): Size of the batch. Defaults to 4.\n            learning_rate (float): Learning rate of the optimizer. Defaults to 1e-5.\n            epochs (int): Number of epochs to train the model for. Defaults to 3.\n            warmup_steps (int): Number of warmup steps for the learning rate scheduler. Defaults to 500.\n            accelerator (accelerate.Accelerator): Accelerator object to distribute the training across multiple GPUs.\n        \"\"\"\n\n\n        self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n        self.model = RobertaForMaskedLM.from_pretrained(model_name)\n\n        self.data_collator = DataCollatorForLanguageModeling(\n            tokenizer=self.tokenizer, \n            mlm=True, \n            mlm_probability=mlm_probability\n            )\n\n        self.split_ratio = split_ratio\n        self.truncation = truncation\n        self.padding = padding\n        self.max_length = max_length\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.warmup_steps = warmup_steps\n        self.accelerator = Accelerator()\n\n    def prepare_dataset(self, data: List[str]):\n        \"\"\"\n        This method is used to prepare the dataset for training.\n        Args:\n            data: List of strings to train the model on.\n\n        Returns:\n            train_loader (torch.utils.data.DataLoader): DataLoader object containing the training data.\n            dataset (CustomDataset): CustomDataset object containing the training data.\n        \"\"\"\n        dataset = CustomDataset(\n            data, \n            self.tokenizer, \n            max_length=self.max_length,\n            truncation=self.truncation,\n            padding=self.padding\n            )\n\n        train_loader = DataLoader(\n            dataset, \n            batch_size=self.batch_size, \n            shuffle=True, \n            collate_fn=self.data_collator\n            )\n\n        return train_loader, dataset\n\n    def train(\n            self, \n            data: List[str],\n            output_dir: Optional[Union[str, Path]] = None\n            ) -&gt; None:\n        \"\"\"\n        This method is used to train the model.\n\n        Args:\n            data (List[str]): List of strings to train the model on.\n            output_dir (str, Path, None): Path to save the model to. Defaults to None.\n\n\n        Examples:\n            &gt;&gt;&gt; model = RobertaTrainer(epoch=3)\n            &gt;&gt;&gt; model.train(data=[\"The brown fox jumps over the lazy dog\", \"The brown fox jumps over the lazy dog\", \"Hello world!\"], output_dir=\"../../output/MLM_roberta\")\n            Epoch: 0 | Loss: 1.1637206077575684 | Perplexity: 3.2020153999328613\n            Epoch: 1 | Loss: 0.6941609382629395 | Perplexity: 2.0011680126190186\n            Epoch: 2 | Loss: 0.4749067425727844 | Perplexity: 1.608262062072754  \n        \"\"\"\n\n        train_data, test_data = train_test_split(\n            data, \n            test_ratio=1 - self.split_ratio, \n            random_seed=42\n            )\n\n        train_loader, _ = self.prepare_dataset(train_data)\n        test_loader, _ = self.prepare_dataset(test_data)\n\n        optimizer = optim.AdamW(\n            self.model.parameters(), \n            lr=self.learning_rate\n            )\n\n        model, optimizer, train_loader, test_loader = self.accelerator.prepare(\n            self.model, \n            optimizer, \n            train_loader, \n            test_loader\n            )\n\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, \n            num_warmup_steps=self.warmup_steps, \n            num_training_steps=len(train_loader) * self.epochs\n            )\n\n        progress_bar = tqdm.tqdm(\n            range(len(train_loader) * self.epochs), \n            desc=\"Training\", \n            dynamic_ncols=True\n            )\n\n        for epoch in range(self.epochs):\n            self.model.train()\n\n            for batch in train_loader:\n                outputs = self.model(**batch)\n                loss = outputs.loss\n                self.accelerator.backward(loss)\n                optimizer.step()\n                scheduler.step()  # Update learning rate scheduler\n                optimizer.zero_grad()\n                progress_bar.update(1)\n\n            self.model.eval()\n            losses = []\n            for step, batch in enumerate(test_loader):\n                with torch.no_grad():\n                    outputs = self.model(**batch)\n\n                loss = outputs.loss\n                losses.append(self.accelerator.gather(loss.repeat(self.batch_size)))\n\n            losses = torch.cat(losses)\n            losses = losses[:len(test_data)]\n\n            try:\n                perplexity = math.exp(torch.mean(losses))\n            except OverflowError:\n                perplexity = float(\"inf\")\n            print(f\"Epoch: {epoch} | Loss: {torch.mean(losses)} | Perplexity: {perplexity}\")\n\n            # Save model\n            if output_dir is not None:\n                self.accelerator.wait_for_everyone()\n                unwrapped_model = self.accelerator.unwrap_model(model)\n                unwrapped_model.save_pretrained(output_dir, save_function=self.accelerator.save)\n                if self.accelerator.is_main_process:\n                    self.tokenizer.save_pretrained(output_dir)\n</code></pre>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaTrainer.__init__","title":"<code>__init__(model_name='roberta-base', max_length=128, mlm_probability=0.15, batch_size=4, learning_rate=1e-05, epochs=3, warmup_steps=500, split_ratio=0.8, truncation=True, padding='max_length')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to train. Defaults to \"roberta-base\".</p> <code>'roberta-base'</code> <code>max_length</code> <code>int</code> <p>Maximum length of the input sequence. Defaults to 128.</p> <code>128</code> <code>mlm_probability</code> <code>float</code> <p>Probability of masking tokens in the input sequence. Defaults to 0.15.</p> <code>0.15</code> <code>batch_size</code> <code>int</code> <p>Size of the batch. Defaults to 4.</p> <code>4</code> <code>learning_rate</code> <code>float</code> <p>Learning rate of the optimizer. Defaults to 1e-5.</p> <code>1e-05</code> <code>epochs</code> <code>int</code> <p>Number of epochs to train the model for. Defaults to 3.</p> <code>3</code> <code>warmup_steps</code> <code>int</code> <p>Number of warmup steps for the learning rate scheduler. Defaults to 500.</p> <code>500</code> <code>split_ratio</code> <code>float</code> <p>Ratio to split the data into train and test. Defaults to 0.8.</p> <code>0.8</code> <code>truncation</code> <code>bool</code> <p>Whether to truncate the input sequence to max_length or not. Defaults to True.</p> <code>True</code> <code>padding</code> <code>str</code> <p>Whether to pad the input sequence to max_length or not. Defaults to \"max_length\".</p> <code>'max_length'</code> <p>Attributes:</p> Name Type Description <code>tokenizer</code> <code>RobertaTokenizer</code> <p>Tokenizer to tokenize the data with.</p> <code>model</code> <code>RobertaForMaskedLM</code> <p>Model to train.</p> <code>data_collator</code> <code>DataCollatorForLanguageModeling</code> <p>DataCollatorForLanguageModeling object to collate the data.</p> <code>split_ratio</code> <code>float</code> <p>Ratio to split the data into train and test. Defaults to 0.8.</p> <code>truncation</code> <code>bool</code> <p>Whether to truncate the input sequence to max_length or not. Defaults to True.</p> <code>padding</code> <code>str</code> <p>Whether to pad the input sequence to max_length or not. Defaults to \"max_length\".</p> <code>max_length</code> <code>int</code> <p>Maximum length of the input sequence. Defaults to 128.</p> <code>batch_size</code> <code>int</code> <p>Size of the batch. Defaults to 4.</p> <code>learning_rate</code> <code>float</code> <p>Learning rate of the optimizer. Defaults to 1e-5.</p> <code>epochs</code> <code>int</code> <p>Number of epochs to train the model for. Defaults to 3.</p> <code>warmup_steps</code> <code>int</code> <p>Number of warmup steps for the learning rate scheduler. Defaults to 500.</p> <code>accelerator</code> <code>Accelerator</code> <p>Accelerator object to distribute the training across multiple GPUs.</p> Source code in <code>semantics/feature_extraction/roberta.py</code> <pre><code>def __init__(\n        self, \n        model_name: str = \"roberta-base\", \n        max_length: int = 128, \n        mlm_probability: float = 0.15, \n        batch_size: int = 4, \n        learning_rate: float = 1e-5, \n        epochs: int = 3, \n        warmup_steps: int = 500, \n        split_ratio: float = 0.8, \n        truncation: bool = True, \n        padding: str = \"max_length\"\n        ):\n\n    \"\"\"\n    Args:\n        model_name (str): Name of the model to train. Defaults to \"roberta-base\".\n        max_length (int): Maximum length of the input sequence. Defaults to 128.\n        mlm_probability (float): Probability of masking tokens in the input sequence. Defaults to 0.15.\n        batch_size (int): Size of the batch. Defaults to 4.\n        learning_rate (float): Learning rate of the optimizer. Defaults to 1e-5.\n        epochs (int): Number of epochs to train the model for. Defaults to 3.\n        warmup_steps (int): Number of warmup steps for the learning rate scheduler. Defaults to 500.\n        split_ratio (float): Ratio to split the data into train and test. Defaults to 0.8.\n        truncation (bool): Whether to truncate the input sequence to max_length or not. Defaults to True.\n        padding (str): Whether to pad the input sequence to max_length or not. Defaults to \"max_length\".\n\n    Attributes:\n        tokenizer (transformers.RobertaTokenizer): Tokenizer to tokenize the data with.\n        model (transformers.RobertaForMaskedLM): Model to train.\n        data_collator (transformers.DataCollatorForLanguageModeling): DataCollatorForLanguageModeling object to collate the data.\n        split_ratio (float): Ratio to split the data into train and test. Defaults to 0.8.\n        truncation (bool): Whether to truncate the input sequence to max_length or not. Defaults to True.\n        padding (str): Whether to pad the input sequence to max_length or not. Defaults to \"max_length\".\n        max_length (int): Maximum length of the input sequence. Defaults to 128.\n        batch_size (int): Size of the batch. Defaults to 4.\n        learning_rate (float): Learning rate of the optimizer. Defaults to 1e-5.\n        epochs (int): Number of epochs to train the model for. Defaults to 3.\n        warmup_steps (int): Number of warmup steps for the learning rate scheduler. Defaults to 500.\n        accelerator (accelerate.Accelerator): Accelerator object to distribute the training across multiple GPUs.\n    \"\"\"\n\n\n    self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n    self.model = RobertaForMaskedLM.from_pretrained(model_name)\n\n    self.data_collator = DataCollatorForLanguageModeling(\n        tokenizer=self.tokenizer, \n        mlm=True, \n        mlm_probability=mlm_probability\n        )\n\n    self.split_ratio = split_ratio\n    self.truncation = truncation\n    self.padding = padding\n    self.max_length = max_length\n    self.batch_size = batch_size\n    self.learning_rate = learning_rate\n    self.epochs = epochs\n    self.warmup_steps = warmup_steps\n    self.accelerator = Accelerator()\n</code></pre>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaTrainer.prepare_dataset","title":"<code>prepare_dataset(data)</code>","text":"<p>This method is used to prepare the dataset for training. Args:     data: List of strings to train the model on.</p> <p>Returns:</p> Name Type Description <code>train_loader</code> <code>DataLoader</code> <p>DataLoader object containing the training data.</p> <code>dataset</code> <code>CustomDataset</code> <p>CustomDataset object containing the training data.</p> Source code in <code>semantics/feature_extraction/roberta.py</code> <pre><code>def prepare_dataset(self, data: List[str]):\n    \"\"\"\n    This method is used to prepare the dataset for training.\n    Args:\n        data: List of strings to train the model on.\n\n    Returns:\n        train_loader (torch.utils.data.DataLoader): DataLoader object containing the training data.\n        dataset (CustomDataset): CustomDataset object containing the training data.\n    \"\"\"\n    dataset = CustomDataset(\n        data, \n        self.tokenizer, \n        max_length=self.max_length,\n        truncation=self.truncation,\n        padding=self.padding\n        )\n\n    train_loader = DataLoader(\n        dataset, \n        batch_size=self.batch_size, \n        shuffle=True, \n        collate_fn=self.data_collator\n        )\n\n    return train_loader, dataset\n</code></pre>"},{"location":"doc/feature_extraction/roberta/#semantics.feature_extraction.roberta.RobertaTrainer.train","title":"<code>train(data, output_dir=None)</code>","text":"<p>This method is used to train the model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[str]</code> <p>List of strings to train the model on.</p> required <code>output_dir</code> <code>(str, Path, None)</code> <p>Path to save the model to. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = RobertaTrainer(epoch=3)\n&gt;&gt;&gt; model.train(data=[\"The brown fox jumps over the lazy dog\", \"The brown fox jumps over the lazy dog\", \"Hello world!\"], output_dir=\"../../output/MLM_roberta\")\nEpoch: 0 | Loss: 1.1637206077575684 | Perplexity: 3.2020153999328613\nEpoch: 1 | Loss: 0.6941609382629395 | Perplexity: 2.0011680126190186\nEpoch: 2 | Loss: 0.4749067425727844 | Perplexity: 1.608262062072754\n</code></pre> Source code in <code>semantics/feature_extraction/roberta.py</code> <pre><code>def train(\n        self, \n        data: List[str],\n        output_dir: Optional[Union[str, Path]] = None\n        ) -&gt; None:\n    \"\"\"\n    This method is used to train the model.\n\n    Args:\n        data (List[str]): List of strings to train the model on.\n        output_dir (str, Path, None): Path to save the model to. Defaults to None.\n\n\n    Examples:\n        &gt;&gt;&gt; model = RobertaTrainer(epoch=3)\n        &gt;&gt;&gt; model.train(data=[\"The brown fox jumps over the lazy dog\", \"The brown fox jumps over the lazy dog\", \"Hello world!\"], output_dir=\"../../output/MLM_roberta\")\n        Epoch: 0 | Loss: 1.1637206077575684 | Perplexity: 3.2020153999328613\n        Epoch: 1 | Loss: 0.6941609382629395 | Perplexity: 2.0011680126190186\n        Epoch: 2 | Loss: 0.4749067425727844 | Perplexity: 1.608262062072754  \n    \"\"\"\n\n    train_data, test_data = train_test_split(\n        data, \n        test_ratio=1 - self.split_ratio, \n        random_seed=42\n        )\n\n    train_loader, _ = self.prepare_dataset(train_data)\n    test_loader, _ = self.prepare_dataset(test_data)\n\n    optimizer = optim.AdamW(\n        self.model.parameters(), \n        lr=self.learning_rate\n        )\n\n    model, optimizer, train_loader, test_loader = self.accelerator.prepare(\n        self.model, \n        optimizer, \n        train_loader, \n        test_loader\n        )\n\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=self.warmup_steps, \n        num_training_steps=len(train_loader) * self.epochs\n        )\n\n    progress_bar = tqdm.tqdm(\n        range(len(train_loader) * self.epochs), \n        desc=\"Training\", \n        dynamic_ncols=True\n        )\n\n    for epoch in range(self.epochs):\n        self.model.train()\n\n        for batch in train_loader:\n            outputs = self.model(**batch)\n            loss = outputs.loss\n            self.accelerator.backward(loss)\n            optimizer.step()\n            scheduler.step()  # Update learning rate scheduler\n            optimizer.zero_grad()\n            progress_bar.update(1)\n\n        self.model.eval()\n        losses = []\n        for step, batch in enumerate(test_loader):\n            with torch.no_grad():\n                outputs = self.model(**batch)\n\n            loss = outputs.loss\n            losses.append(self.accelerator.gather(loss.repeat(self.batch_size)))\n\n        losses = torch.cat(losses)\n        losses = losses[:len(test_data)]\n\n        try:\n            perplexity = math.exp(torch.mean(losses))\n        except OverflowError:\n            perplexity = float(\"inf\")\n        print(f\"Epoch: {epoch} | Loss: {torch.mean(losses)} | Perplexity: {perplexity}\")\n\n        # Save model\n        if output_dir is not None:\n            self.accelerator.wait_for_everyone()\n            unwrapped_model = self.accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(output_dir, save_function=self.accelerator.save)\n            if self.accelerator.is_main_process:\n                self.tokenizer.save_pretrained(output_dir)\n</code></pre>"},{"location":"doc/feature_extraction/roberta/#references","title":"References","text":"<ol> <li>torch.utils.data</li> </ol>"},{"location":"doc/feature_extraction/word2vec/","title":"Word2Vec","text":""},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecAlign","title":"<code>Word2VecAlign</code>","text":"<p>Wrapper class for gensim.models.Word2Vec to align Word2Vec models.</p>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecAlign--methods","title":"Methods","text":"<pre><code>__init__(model_paths)\n    Initialize the Word2VecAlign object with a list of paths to the Word2Vec models.\nload_models()\n    Load the models\nalign_models(reference_index, output_dir, method)\n    Align the models\n</code></pre> Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>class Word2VecAlign:\n    \"\"\"\n    Wrapper class for gensim.models.Word2Vec to align Word2Vec models.\n\n    Methods\n    -------\n        __init__(model_paths)\n            Initialize the Word2VecAlign object with a list of paths to the Word2Vec models.\n        load_models()\n            Load the models\n        align_models(reference_index, output_dir, method)\n            Align the models\n    \"\"\"\n    def __init__(\n            self, \n            model_paths: List[str],\n\n            ):\n        \"\"\"\n        Args:\n            model_paths (List[str]): List of paths to the models \n\n        Attributes:\n            model_paths (List[str]): List of paths to the models \n            reference_model (gensim.models.Word2Vec): The reference model\n            models (List[gensim.models.Word2Vec]): List of models\n            model_names (List[str]): List of model names\n            aligned_models (List[gensim.models.Word2Vec]): List of aligned models     \n        \"\"\"\n        self.model_paths = model_paths\n        self.reference_model = None\n        self.models = []\n        self.model_names = [Path(model_path).stem for model_path in model_paths]\n        self.aligned_models = []\n\n        self.load_models()\n\n    def load_models(self) -&gt; None:\n        \"\"\"\n        Load the models\n        \"\"\"\n        for model_path in self.model_paths:\n            self.models.append(Word2Vec.load(model_path))\n\n    def align_models(\n            self,\n            reference_index: int = -1,\n            output_dir: Optional[str] = None,\n            method: str = \"procrustes\",\n            ) -&gt; List[Word2Vec]:\n        \"\"\"\n        Align the models\n\n        Args: \n            reference_index (int, optional): Index of the reference model, by default -1\n            output_dir (str, optional): Path to save the aligned models, by default None\n            method (str, optional): Alignment method, by default \"procrustes\"\n\n        Returns:\n            aligned_models (List[gensim.models.Word2Vec]): List of aligned models\n\n        Examples:\n            &gt;&gt;&gt; from semantics.feature_extraction.word2vec import Word2VecAlign\n            &gt;&gt;&gt; model_paths = ['model1.model', 'model2.model']\n            &gt;&gt;&gt; Word2VecAlign(model_paths).align_models(reference_index=0, output_dir='aligned_models')\n            &gt;&gt;&gt; print('Aligned models: ', Word2VecAlign(model_paths).aligned_models)\n            Aligned models:  [Word2Vec(vocab=5, vector_size=100, alpha=0.025), Word2Vec(vocab=5, vector_size=100, alpha=0.025)]\n        \"\"\"\n\n        if method != \"procrustes\":\n            raise NotImplementedError(\"Only procrustes alignment is implemented. Please use method='procrustes'\")\n\n\n        self.reference_model = self.models[reference_index]\n        self.reference_model.save(f\"{output_dir}/{self.model_names[reference_index]}_aligned.model\")\n        self.aligned_models.append(self.reference_model)\n        self.models.pop(reference_index)\n\n        for i, model in enumerate(self.models):\n            aligned_model = smart_procrustes_align_gensim(self.reference_model,model)\n            aligned_model.save(f\"{output_dir}/{self.model_names[i]}_aligned.model\")\n            self.aligned_models.append(aligned_model)\n\n        return self.aligned_models\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecAlign.__init__","title":"<code>__init__(model_paths)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_paths</code> <code>List[str]</code> <p>List of paths to the models </p> required <p>Attributes:</p> Name Type Description <code>model_paths</code> <code>List[str]</code> <p>List of paths to the models </p> <code>reference_model</code> <code>Word2Vec</code> <p>The reference model</p> <code>models</code> <code>List[Word2Vec]</code> <p>List of models</p> <code>model_names</code> <code>List[str]</code> <p>List of model names</p> <code>aligned_models</code> <code>List[Word2Vec]</code> <p>List of aligned models</p> Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>def __init__(\n        self, \n        model_paths: List[str],\n\n        ):\n    \"\"\"\n    Args:\n        model_paths (List[str]): List of paths to the models \n\n    Attributes:\n        model_paths (List[str]): List of paths to the models \n        reference_model (gensim.models.Word2Vec): The reference model\n        models (List[gensim.models.Word2Vec]): List of models\n        model_names (List[str]): List of model names\n        aligned_models (List[gensim.models.Word2Vec]): List of aligned models     \n    \"\"\"\n    self.model_paths = model_paths\n    self.reference_model = None\n    self.models = []\n    self.model_names = [Path(model_path).stem for model_path in model_paths]\n    self.aligned_models = []\n\n    self.load_models()\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecAlign.align_models","title":"<code>align_models(reference_index=-1, output_dir=None, method='procrustes')</code>","text":"<p>Align the models</p> <p>Parameters:</p> Name Type Description Default <code>reference_index</code> <code>int</code> <p>Index of the reference model, by default -1</p> <code>-1</code> <code>output_dir</code> <code>str</code> <p>Path to save the aligned models, by default None</p> <code>None</code> <code>method</code> <code>str</code> <p>Alignment method, by default \"procrustes\"</p> <code>'procrustes'</code> <p>Returns:</p> Name Type Description <code>aligned_models</code> <code>List[Word2Vec]</code> <p>List of aligned models</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from semantics.feature_extraction.word2vec import Word2VecAlign\n&gt;&gt;&gt; model_paths = ['model1.model', 'model2.model']\n&gt;&gt;&gt; Word2VecAlign(model_paths).align_models(reference_index=0, output_dir='aligned_models')\n&gt;&gt;&gt; print('Aligned models: ', Word2VecAlign(model_paths).aligned_models)\nAligned models:  [Word2Vec(vocab=5, vector_size=100, alpha=0.025), Word2Vec(vocab=5, vector_size=100, alpha=0.025)]\n</code></pre> Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>def align_models(\n        self,\n        reference_index: int = -1,\n        output_dir: Optional[str] = None,\n        method: str = \"procrustes\",\n        ) -&gt; List[Word2Vec]:\n    \"\"\"\n    Align the models\n\n    Args: \n        reference_index (int, optional): Index of the reference model, by default -1\n        output_dir (str, optional): Path to save the aligned models, by default None\n        method (str, optional): Alignment method, by default \"procrustes\"\n\n    Returns:\n        aligned_models (List[gensim.models.Word2Vec]): List of aligned models\n\n    Examples:\n        &gt;&gt;&gt; from semantics.feature_extraction.word2vec import Word2VecAlign\n        &gt;&gt;&gt; model_paths = ['model1.model', 'model2.model']\n        &gt;&gt;&gt; Word2VecAlign(model_paths).align_models(reference_index=0, output_dir='aligned_models')\n        &gt;&gt;&gt; print('Aligned models: ', Word2VecAlign(model_paths).aligned_models)\n        Aligned models:  [Word2Vec(vocab=5, vector_size=100, alpha=0.025), Word2Vec(vocab=5, vector_size=100, alpha=0.025)]\n    \"\"\"\n\n    if method != \"procrustes\":\n        raise NotImplementedError(\"Only procrustes alignment is implemented. Please use method='procrustes'\")\n\n\n    self.reference_model = self.models[reference_index]\n    self.reference_model.save(f\"{output_dir}/{self.model_names[reference_index]}_aligned.model\")\n    self.aligned_models.append(self.reference_model)\n    self.models.pop(reference_index)\n\n    for i, model in enumerate(self.models):\n        aligned_model = smart_procrustes_align_gensim(self.reference_model,model)\n        aligned_model.save(f\"{output_dir}/{self.model_names[i]}_aligned.model\")\n        self.aligned_models.append(aligned_model)\n\n    return self.aligned_models\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecAlign.load_models","title":"<code>load_models()</code>","text":"<p>Load the models</p> Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>def load_models(self) -&gt; None:\n    \"\"\"\n    Load the models\n    \"\"\"\n    for model_path in self.model_paths:\n        self.models.append(Word2Vec.load(model_path))\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecEmbeddings","title":"<code>Word2VecEmbeddings</code>","text":"<p>Wrapper class for gensim.models.Word2Vec to infer word vectors.</p>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecEmbeddings--methods","title":"Methods","text":"<pre><code>__init__(pretrained_model_path)\n    Initialize the Word2VecEmbeddings object with a pretrained model.\n_word2vec_case_preparation()\n    Prepare the Word2Vec model\ninfer_vector(word, norm)\n    Infer the vector of a word\n</code></pre> Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>class Word2VecEmbeddings:\n    \"\"\"\n    Wrapper class for gensim.models.Word2Vec to infer word vectors.\n\n    Methods\n    -------\n        __init__(pretrained_model_path)\n            Initialize the Word2VecEmbeddings object with a pretrained model.\n        _word2vec_case_preparation()\n            Prepare the Word2Vec model\n        infer_vector(word, norm)\n            Infer the vector of a word\n    \"\"\"\n    def __init__(\n            self,\n            pretrained_model_path: Optional[str] = None,\n            ):\n        \"\"\"\n        Args: \n            pretrained_model_path (str, optional): Path to a pretrained model, by default None\n\n        Attributes:\n            model_path (str, optional): Path to a pretrained model, by default None\n            model (gensim.models.Word2Vec): The Word2Vec model\n            vocab (bool): Whether the model has been initialized\n        \"\"\"\n        self.model_path = pretrained_model_path\n        if pretrained_model_path is not None:\n            if not os.path.exists(pretrained_model_path):\n                raise ValueError(\n                    f\"Model path {pretrained_model_path} does not exist.\"\n                )\n            self.model_path = pretrained_model_path\n\n        self.model = None\n        self.vocab = False\n\n        self._word2vec_case_preparation()\n\n    def _word2vec_case_preparation(self) -&gt; None:\n        \"\"\"\n        Initialize the Word2Vec model\n        \"\"\"\n        if self.model_path is None:\n            self.model = Word2Vec()\n        else:\n            self.model = Word2Vec.load(self.model_path)\n        self.vocab = True\n\n    def infer_vector(self, word:str, norm = False) -&gt; List[float]:\n        \"\"\"\n        Infer the vector of a word\n\n        Args:\n            word (str): The word to infer the embedding vector of\n            norm (bool, optional): Whether to normalize the vector, by default False\n\n        Returns:\n            embedding (List[float]): The embedding vector of the word\n        \"\"\"\n        if not self.vocab:\n            raise ValueError(\n                f'The Embedding model {self.model.__class__.__name__} has not been initialized'\n            )\n        return self.model.wv.get_vector(word, norm = norm)\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecEmbeddings.__init__","title":"<code>__init__(pretrained_model_path=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pretrained_model_path</code> <code>str</code> <p>Path to a pretrained model, by default None</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>model_path</code> <code>str</code> <p>Path to a pretrained model, by default None</p> <code>model</code> <code>Word2Vec</code> <p>The Word2Vec model</p> <code>vocab</code> <code>bool</code> <p>Whether the model has been initialized</p> Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>def __init__(\n        self,\n        pretrained_model_path: Optional[str] = None,\n        ):\n    \"\"\"\n    Args: \n        pretrained_model_path (str, optional): Path to a pretrained model, by default None\n\n    Attributes:\n        model_path (str, optional): Path to a pretrained model, by default None\n        model (gensim.models.Word2Vec): The Word2Vec model\n        vocab (bool): Whether the model has been initialized\n    \"\"\"\n    self.model_path = pretrained_model_path\n    if pretrained_model_path is not None:\n        if not os.path.exists(pretrained_model_path):\n            raise ValueError(\n                f\"Model path {pretrained_model_path} does not exist.\"\n            )\n        self.model_path = pretrained_model_path\n\n    self.model = None\n    self.vocab = False\n\n    self._word2vec_case_preparation()\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecEmbeddings.infer_vector","title":"<code>infer_vector(word, norm=False)</code>","text":"<p>Infer the vector of a word</p> <p>Parameters:</p> Name Type Description Default <code>word</code> <code>str</code> <p>The word to infer the embedding vector of</p> required <code>norm</code> <code>bool</code> <p>Whether to normalize the vector, by default False</p> <code>False</code> <p>Returns:</p> Name Type Description <code>embedding</code> <code>List[float]</code> <p>The embedding vector of the word</p> Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>def infer_vector(self, word:str, norm = False) -&gt; List[float]:\n    \"\"\"\n    Infer the vector of a word\n\n    Args:\n        word (str): The word to infer the embedding vector of\n        norm (bool, optional): Whether to normalize the vector, by default False\n\n    Returns:\n        embedding (List[float]): The embedding vector of the word\n    \"\"\"\n    if not self.vocab:\n        raise ValueError(\n            f'The Embedding model {self.model.__class__.__name__} has not been initialized'\n        )\n    return self.model.wv.get_vector(word, norm = norm)\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecInference","title":"<code>Word2VecInference</code>","text":"<p>Wrapper class for gensim.models.Word2Vec for Inference.</p>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecInference--methods","title":"Methods","text":"<pre><code>__init__(pretrained_model_path)\n    Initialize the Word2VecInference object with a pretrained model.\nget_embedding(word, norm)\n    Infer the vector of a word\nget_similarity(word1, word2)\n    Get the cosine similarity between two words\nget_top_k_words(word, k)\n    Get the top k most similar words to a word in the vocabulary of the model.\n</code></pre> Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>class Word2VecInference:\n    \"\"\"\n    Wrapper class for gensim.models.Word2Vec for Inference.\n\n    Methods\n    -------\n        __init__(pretrained_model_path)\n            Initialize the Word2VecInference object with a pretrained model.\n        get_embedding(word, norm)\n            Infer the vector of a word\n        get_similarity(word1, word2)\n            Get the cosine similarity between two words\n        get_top_k_words(word, k)\n            Get the top k most similar words to a word in the vocabulary of the model.\n    \"\"\"\n    def __init__(\n            self,\n            pretrained_model_path: Optional[str] = None,\n            ):\n        \"\"\"\n        Args:\n            pretrained_model_path (str, optional): Path to a pretrained model, by default None  \n\n        Attributes:\n            word_vectorizor (Word2VecEmbeddings): The Word2VecEmbeddings object\n        \"\"\"\n        self.word_vectorizor = Word2VecEmbeddings(pretrained_model_path)\n\n    def get_embedding(self, word:str, norm: bool = False) -&gt; List[float]:\n        \"\"\"\n        Infer the vector of a word\n\n        Args:\n            word (str): The word to infer the embedding vector of\n            norm (bool, optional): Whether to normalize the vector, by default False\n\n        Returns:\n            embedding (List[float]): The embedding vector of the word\n\n        Examples:\n            &gt;&gt;&gt; from semantics.feature_extraction.word2vec import Word2VecInference\n            &gt;&gt;&gt; Word2VecInference('model.model').get_embedding('test', norm=False)\n            array([-0.00460768, -0.00460768, ..., -0.00460768, -0.00460768])\n        \"\"\"\n        return self.word_vectorizor.infer_vector(word= word, norm = norm)\n\n    def get_similarity(self, word1: str, word2: str) -&gt; float:\n        \"\"\"\n        Get the cosine similarity between two words' embedding vectors\n\n        Args:\n            word1 (str): The first word\n            word2 (str): The second word\n\n        Returns:\n            similarity (float): The cosine similarity between the two words\n\n        Examples:\n            &gt;&gt;&gt; from semantics.feature_extraction.word2vec import Word2VecInference\n            &gt;&gt;&gt; Word2VecInference('model.model').get_similarity('test', 'another')\n            0.99999994\n        \"\"\"\n        return self.word_vectorizor.model.wv.similarity(word1, word2)\n\n    def get_top_k_words(\n            self,\n            word: str,\n            k: int = 10,\n            ):\n        \"\"\"\n        Get the top k most similar words to a word in the vocabulary of the model. Default k = 10\n\n        Args:\n            word (str): The word to get the top k most similar words of\n            k (int, optional): The number of words to return, by default 10\n\n        Returns:\n            topk (Tuple[List[str], List[float]]): Tuple of lists of the top k most similar words and their cosine similarities\n\n        Examples:\n            &gt;&gt;&gt; from semantics.feature_extraction.word2vec import Word2VecInference\n            &gt;&gt;&gt; Word2VecInference('model.model').get_top_k_words('test', k=1)\n            (['another'], [0.9999999403953552])\n        \"\"\"\n\n        try:\n            sims = self.word_vectorizor.model.wv.most_similar(\n                word,\n                topn=k\n                )\n            return tuple(map(list, zip(*sims)))\n\n        except KeyError:\n            print(f\"The word {word} in the input is not in the model vocabulary.\")\n            return [], []\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecInference.__init__","title":"<code>__init__(pretrained_model_path=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pretrained_model_path</code> <code>str</code> <p>Path to a pretrained model, by default None  </p> <code>None</code> <p>Attributes:</p> Name Type Description <code>word_vectorizor</code> <code>Word2VecEmbeddings</code> <p>The Word2VecEmbeddings object</p> Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>def __init__(\n        self,\n        pretrained_model_path: Optional[str] = None,\n        ):\n    \"\"\"\n    Args:\n        pretrained_model_path (str, optional): Path to a pretrained model, by default None  \n\n    Attributes:\n        word_vectorizor (Word2VecEmbeddings): The Word2VecEmbeddings object\n    \"\"\"\n    self.word_vectorizor = Word2VecEmbeddings(pretrained_model_path)\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecInference.get_embedding","title":"<code>get_embedding(word, norm=False)</code>","text":"<p>Infer the vector of a word</p> <p>Parameters:</p> Name Type Description Default <code>word</code> <code>str</code> <p>The word to infer the embedding vector of</p> required <code>norm</code> <code>bool</code> <p>Whether to normalize the vector, by default False</p> <code>False</code> <p>Returns:</p> Name Type Description <code>embedding</code> <code>List[float]</code> <p>The embedding vector of the word</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from semantics.feature_extraction.word2vec import Word2VecInference\n&gt;&gt;&gt; Word2VecInference('model.model').get_embedding('test', norm=False)\narray([-0.00460768, -0.00460768, ..., -0.00460768, -0.00460768])\n</code></pre> Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>def get_embedding(self, word:str, norm: bool = False) -&gt; List[float]:\n    \"\"\"\n    Infer the vector of a word\n\n    Args:\n        word (str): The word to infer the embedding vector of\n        norm (bool, optional): Whether to normalize the vector, by default False\n\n    Returns:\n        embedding (List[float]): The embedding vector of the word\n\n    Examples:\n        &gt;&gt;&gt; from semantics.feature_extraction.word2vec import Word2VecInference\n        &gt;&gt;&gt; Word2VecInference('model.model').get_embedding('test', norm=False)\n        array([-0.00460768, -0.00460768, ..., -0.00460768, -0.00460768])\n    \"\"\"\n    return self.word_vectorizor.infer_vector(word= word, norm = norm)\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecInference.get_similarity","title":"<code>get_similarity(word1, word2)</code>","text":"<p>Get the cosine similarity between two words' embedding vectors</p> <p>Parameters:</p> Name Type Description Default <code>word1</code> <code>str</code> <p>The first word</p> required <code>word2</code> <code>str</code> <p>The second word</p> required <p>Returns:</p> Name Type Description <code>similarity</code> <code>float</code> <p>The cosine similarity between the two words</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from semantics.feature_extraction.word2vec import Word2VecInference\n&gt;&gt;&gt; Word2VecInference('model.model').get_similarity('test', 'another')\n0.99999994\n</code></pre> Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>def get_similarity(self, word1: str, word2: str) -&gt; float:\n    \"\"\"\n    Get the cosine similarity between two words' embedding vectors\n\n    Args:\n        word1 (str): The first word\n        word2 (str): The second word\n\n    Returns:\n        similarity (float): The cosine similarity between the two words\n\n    Examples:\n        &gt;&gt;&gt; from semantics.feature_extraction.word2vec import Word2VecInference\n        &gt;&gt;&gt; Word2VecInference('model.model').get_similarity('test', 'another')\n        0.99999994\n    \"\"\"\n    return self.word_vectorizor.model.wv.similarity(word1, word2)\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecInference.get_top_k_words","title":"<code>get_top_k_words(word, k=10)</code>","text":"<p>Get the top k most similar words to a word in the vocabulary of the model. Default k = 10</p> <p>Parameters:</p> Name Type Description Default <code>word</code> <code>str</code> <p>The word to get the top k most similar words of</p> required <code>k</code> <code>int</code> <p>The number of words to return, by default 10</p> <code>10</code> <p>Returns:</p> Name Type Description <code>topk</code> <code>Tuple[List[str], List[float]]</code> <p>Tuple of lists of the top k most similar words and their cosine similarities</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from semantics.feature_extraction.word2vec import Word2VecInference\n&gt;&gt;&gt; Word2VecInference('model.model').get_top_k_words('test', k=1)\n(['another'], [0.9999999403953552])\n</code></pre> Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>def get_top_k_words(\n        self,\n        word: str,\n        k: int = 10,\n        ):\n    \"\"\"\n    Get the top k most similar words to a word in the vocabulary of the model. Default k = 10\n\n    Args:\n        word (str): The word to get the top k most similar words of\n        k (int, optional): The number of words to return, by default 10\n\n    Returns:\n        topk (Tuple[List[str], List[float]]): Tuple of lists of the top k most similar words and their cosine similarities\n\n    Examples:\n        &gt;&gt;&gt; from semantics.feature_extraction.word2vec import Word2VecInference\n        &gt;&gt;&gt; Word2VecInference('model.model').get_top_k_words('test', k=1)\n        (['another'], [0.9999999403953552])\n    \"\"\"\n\n    try:\n        sims = self.word_vectorizor.model.wv.most_similar(\n            word,\n            topn=k\n            )\n        return tuple(map(list, zip(*sims)))\n\n    except KeyError:\n        print(f\"The word {word} in the input is not in the model vocabulary.\")\n        return [], []\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecTrainer","title":"<code>Word2VecTrainer</code>","text":"<p>Wrapper class for gensim.models.Word2Vec to train a Word2Vec model.</p>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecTrainer--methods","title":"Methods","text":"<pre><code>__init__(model_path, min_count, window, negative, ns_exponent, vector_size, workers, sg, **kwargs)\n    Initialize the Word2Vec model\ntrain(data, output_path, epochs, start_alpha, end_alpha, compute_loss, **kwargs)\n    Train the Word2Vec model on the given data\n</code></pre> Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>class Word2VecTrainer:  \n    \"\"\"\n    Wrapper class for gensim.models.Word2Vec to train a Word2Vec model.\n\n    Methods\n    -------\n        __init__(model_path, min_count, window, negative, ns_exponent, vector_size, workers, sg, **kwargs)\n            Initialize the Word2Vec model\n        train(data, output_path, epochs, start_alpha, end_alpha, compute_loss, **kwargs)\n            Train the Word2Vec model on the given data\n    \"\"\"  \n\n    def __init__(\n            self,\n            model_path: Optional[str] = None,\n            min_count=0,\n            window=15,\n            negative=5,\n            ns_exponent=0.75,\n            vector_size=100,\n            workers=1,\n            sg=1,\n            **kwargs\n            ):\n        \"\"\"\n        Args:\n            model_path (str, optional): Path to a pretrained model, by default None.\n            min_count (int, optional): Ignores all words with total frequency lower than this, by default 0\n            window (int, optional): The maximum distance between the current and predicted word within a sentence, by default 15\n            negative (int, optional): If &gt; 0, negative sampling will be used, by default 5\n            ns_exponent (float, optional): The exponent used to shape the negative sampling distribution, by default 0.75\n            vector_size (int, optional): Dimensionality of the word vectors, by default 100\n            workers (int, optional): Number of worker threads to train the model, by default 1\n            sg (int, optional): Training algorithm: 1 for skip-gram; otherwise CBOW, by default 1\n            **kwargs (optional): Additional arguments to pass to the gensim.models.Word2Vec constructor\n\n        Attributes:\n            model (gensim.models.Word2Vec): The Word2Vec model\n        \"\"\"\n\n        if model_path:\n            self.model = Word2Vec.load(model_path)\n        else:\n            self.model = Word2Vec(\n                    min_count=min_count,\n                    window=window,\n                    negative=negative,\n                    ns_exponent=ns_exponent,\n                    vector_size=vector_size,\n                    workers=workers,\n                    sg=sg,\n                    **kwargs\n                    )\n\n    def train(\n            self, \n            data: List[str],\n            output_dir: Optional[Union[str, Path]] = None,\n            epochs=5,\n            start_alpha=0.025,\n            end_alpha=0.0001,\n            compute_loss=True,\n            **kwargs\n            ):\n        \"\"\"\n        Train the Word2Vec model on the given data\n\n        Args:\n            data (List[str]): List of documents\n            output_dir (Union[str, Path], None): Path to save the trained model, by default None\n            epochs (int, optional): Number of epochs, by default 5\n            start_alpha (float, optional): Learning rate, by default 0.025\n            end_alpha (float, optional): Minimum learning rate, by default 0.0001\n            compute_loss (bool, optional): Whether to compute the loss, by default True\n            **kwargs : optional\n\n        Examples:\n            &gt;&gt;&gt; from semantics.feature_extraction.word2vec import Word2VecTrainer\n            &gt;&gt;&gt; texts = ['This is a test.', 'This is another test.', 'This is a third test.']\n            &gt;&gt;&gt; Word2VecTrainer().train(texts, epochs=1)\n            &gt;&gt;&gt; print('Trained model: ', Word2VecTrainer().model)\n            Trained model:  Word2Vec(vocab=5, vector_size=100, alpha=0.025)\n        \"\"\"\n        self.model.build_vocab(data)\n        total_examples = self.model.corpus_count\n        self.model.train(\n                data,\n                total_examples=total_examples,\n                epochs=epochs,\n                start_alpha=start_alpha,\n                end_alpha=end_alpha,\n                compute_loss=compute_loss,\n                **kwargs\n                )\n        if output_dir:\n            self.model.save(output_dir)\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecTrainer.__init__","title":"<code>__init__(model_path=None, min_count=0, window=15, negative=5, ns_exponent=0.75, vector_size=100, workers=1, sg=1, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to a pretrained model, by default None.</p> <code>None</code> <code>min_count</code> <code>int</code> <p>Ignores all words with total frequency lower than this, by default 0</p> <code>0</code> <code>window</code> <code>int</code> <p>The maximum distance between the current and predicted word within a sentence, by default 15</p> <code>15</code> <code>negative</code> <code>int</code> <p>If &gt; 0, negative sampling will be used, by default 5</p> <code>5</code> <code>ns_exponent</code> <code>float</code> <p>The exponent used to shape the negative sampling distribution, by default 0.75</p> <code>0.75</code> <code>vector_size</code> <code>int</code> <p>Dimensionality of the word vectors, by default 100</p> <code>100</code> <code>workers</code> <code>int</code> <p>Number of worker threads to train the model, by default 1</p> <code>1</code> <code>sg</code> <code>int</code> <p>Training algorithm: 1 for skip-gram; otherwise CBOW, by default 1</p> <code>1</code> <code>**kwargs</code> <code>optional</code> <p>Additional arguments to pass to the gensim.models.Word2Vec constructor</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>model</code> <code>Word2Vec</code> <p>The Word2Vec model</p> Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>def __init__(\n        self,\n        model_path: Optional[str] = None,\n        min_count=0,\n        window=15,\n        negative=5,\n        ns_exponent=0.75,\n        vector_size=100,\n        workers=1,\n        sg=1,\n        **kwargs\n        ):\n    \"\"\"\n    Args:\n        model_path (str, optional): Path to a pretrained model, by default None.\n        min_count (int, optional): Ignores all words with total frequency lower than this, by default 0\n        window (int, optional): The maximum distance between the current and predicted word within a sentence, by default 15\n        negative (int, optional): If &gt; 0, negative sampling will be used, by default 5\n        ns_exponent (float, optional): The exponent used to shape the negative sampling distribution, by default 0.75\n        vector_size (int, optional): Dimensionality of the word vectors, by default 100\n        workers (int, optional): Number of worker threads to train the model, by default 1\n        sg (int, optional): Training algorithm: 1 for skip-gram; otherwise CBOW, by default 1\n        **kwargs (optional): Additional arguments to pass to the gensim.models.Word2Vec constructor\n\n    Attributes:\n        model (gensim.models.Word2Vec): The Word2Vec model\n    \"\"\"\n\n    if model_path:\n        self.model = Word2Vec.load(model_path)\n    else:\n        self.model = Word2Vec(\n                min_count=min_count,\n                window=window,\n                negative=negative,\n                ns_exponent=ns_exponent,\n                vector_size=vector_size,\n                workers=workers,\n                sg=sg,\n                **kwargs\n                )\n</code></pre>"},{"location":"doc/feature_extraction/word2vec/#semantics.feature_extraction.word2vec.Word2VecTrainer.train","title":"<code>train(data, output_dir=None, epochs=5, start_alpha=0.025, end_alpha=0.0001, compute_loss=True, **kwargs)</code>","text":"<p>Train the Word2Vec model on the given data</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[str]</code> <p>List of documents</p> required <code>output_dir</code> <code>(Union[str, Path], None)</code> <p>Path to save the trained model, by default None</p> <code>None</code> <code>epochs</code> <code>int</code> <p>Number of epochs, by default 5</p> <code>5</code> <code>start_alpha</code> <code>float</code> <p>Learning rate, by default 0.025</p> <code>0.025</code> <code>end_alpha</code> <code>float</code> <p>Minimum learning rate, by default 0.0001</p> <code>0.0001</code> <code>compute_loss</code> <code>bool</code> <p>Whether to compute the loss, by default True</p> <code>True</code> <code>**kwargs</code> <p>optional</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from semantics.feature_extraction.word2vec import Word2VecTrainer\n&gt;&gt;&gt; texts = ['This is a test.', 'This is another test.', 'This is a third test.']\n&gt;&gt;&gt; Word2VecTrainer().train(texts, epochs=1)\n&gt;&gt;&gt; print('Trained model: ', Word2VecTrainer().model)\nTrained model:  Word2Vec(vocab=5, vector_size=100, alpha=0.025)\n</code></pre> Source code in <code>semantics/feature_extraction/word2vec.py</code> <pre><code>def train(\n        self, \n        data: List[str],\n        output_dir: Optional[Union[str, Path]] = None,\n        epochs=5,\n        start_alpha=0.025,\n        end_alpha=0.0001,\n        compute_loss=True,\n        **kwargs\n        ):\n    \"\"\"\n    Train the Word2Vec model on the given data\n\n    Args:\n        data (List[str]): List of documents\n        output_dir (Union[str, Path], None): Path to save the trained model, by default None\n        epochs (int, optional): Number of epochs, by default 5\n        start_alpha (float, optional): Learning rate, by default 0.025\n        end_alpha (float, optional): Minimum learning rate, by default 0.0001\n        compute_loss (bool, optional): Whether to compute the loss, by default True\n        **kwargs : optional\n\n    Examples:\n        &gt;&gt;&gt; from semantics.feature_extraction.word2vec import Word2VecTrainer\n        &gt;&gt;&gt; texts = ['This is a test.', 'This is another test.', 'This is a third test.']\n        &gt;&gt;&gt; Word2VecTrainer().train(texts, epochs=1)\n        &gt;&gt;&gt; print('Trained model: ', Word2VecTrainer().model)\n        Trained model:  Word2Vec(vocab=5, vector_size=100, alpha=0.025)\n    \"\"\"\n    self.model.build_vocab(data)\n    total_examples = self.model.corpus_count\n    self.model.train(\n            data,\n            total_examples=total_examples,\n            epochs=epochs,\n            start_alpha=start_alpha,\n            end_alpha=end_alpha,\n            compute_loss=compute_loss,\n            **kwargs\n            )\n    if output_dir:\n        self.model.save(output_dir)\n</code></pre>"},{"location":"doc/graph/edges/","title":"Edges","text":"<p>This class is used to get the edges of the word graph.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Dict[int, str], node_features: np.ndarray, node_embeddings: np.ndarray) The constructor of the Edges class.</p> <code>get_similarity</code> <p>int, emb2: int) -&gt; float This method is used to get the similarity between two nodes.</p> <code>get_pmi</code> <p>List[str], word1: str, word2: str) -&gt; float This method is used to get the PMI between two nodes.</p> <code>get_edge_features</code> <p>List[str]) This method is used to get the edge features of the word graph.</p> Source code in <code>semantics/graphs/temporal_graph.py</code> <pre><code>class Edges:\n    \"\"\"\n    This class is used to get the edges of the word graph.\n\n    methods:\n        __init__(self, word_ids: Dict[int, str], node_features: np.ndarray, node_embeddings: np.ndarray)\n            The constructor of the Edges class.\n        get_similarity(self, emb1: int, emb2: int) -&gt; float\n            This method is used to get the similarity between two nodes.\n        get_pmi(self, data: List[str], word1: str, word2: str) -&gt; float\n            This method is used to get the PMI between two nodes.\n        get_edge_features(self, dataset: List[str])\n            This method is used to get the edge features of the word graph.\n    \"\"\"\n    def __init__(\n            self,\n            index_to_key: Dict[int, str],\n            node_features: np.ndarray,\n            node_embeddings: np.ndarray,\n        ):\n        \"\"\"\n        Args:\n            index_to_key (Dict[int, str]): the index of the nodes of the word graph. The keys are the indices of the nodes and the values are the words of the nodes.\n            node_features (np.ndarray): the features of the nodes of the word graph of shape (num_nodes, 3) where num_nodes is the number of nodes in the graph.\n            node_embeddings (np.ndarray): the embeddings of the nodes of the word graph from the MLM model, of shape (num_nodes, 768).\n        \"\"\"\n\n        self.index_to_key = index_to_key\n        self.node_features = node_features\n        self.node_embeddings = node_embeddings\n\n\n    def get_similarity(self, emb1: int, emb2: int) -&gt; float:\n        \"\"\"\n        This method is used to get the similarity between two nodes.\n\n        Args:\n            emb1 (int): the first index of the embedding in node_embeddings\n            emb2 (int): the second index of the embedding in node_embeddings\n\n        Returns:\n            similarity (float): the similarity between the two embeddings\n        \"\"\"\n        # np.dot(node1, node2) / (np.linalg.norm(node1) * np.linalg.norm(node2))\n        return torch.cosine_similarity(\n            torch.tensor(self.node_embeddings[emb1]).reshape(1,-1), \n            torch.tensor(self.node_embeddings[emb2]).reshape(1,-1)\n            ).item()\n\n\n    def get_pmi(self, data: List[str], word1: str, word2: str) -&gt; float:\n        \"\"\"\n        This method is used to get the PMI between two nodes.\n\n        Args:\n            word1 (str): the first node (word)\n            word2 (str): the second node (word)\n\n        Returns:\n            pmi (float): the PMI between the two words in the dataset\n        \"\"\"\n        # Replace these methods with actual methods to get the word count, \n        # the co-occurrence count, and the total count.\n        word1_count = count_occurence(data, word1)\n        word2_count = count_occurence(data, word2)\n        co_occurrence_count = count_occurence(data, [word1, word2])\n        total_count = count_occurence(data)\n\n        # Calculate probabilities\n        p_word1 = word1_count / total_count\n        p_word2 = word2_count / total_count\n        p_co_occurrence = co_occurrence_count / total_count\n\n        # Calculate PMI\n        pmi = log(p_co_occurrence / (p_word1 * p_word2), 2) if p_co_occurrence &gt; 0 else 0\n        return pmi\n\n    def get_edge_features(self, dataset: List[str], sim_threshold: float = 0.5):\n        \"\"\"\n        This method is used to get the edge features of the word graph.\n\n        Args:\n            dataset (List[str]): the dataset to get the edge features from\n            sim_threshold (float): the similarity threshold to create an edge between two nodes. Default: 0.5.\n\n        Returns:\n            edge_index (np.ndarray): the edge index of the word graph of shape (2, num_edges) where num_edges is the number of edges in the graph. The first row contains the indices of the first node of the edge and the second row contains the indices of the second node of the edge. An edge is created if the similarity between the two nodes is greater than sim_threshold.\n            edge_features (np.ndarray): the edge features of the word graph of shape (num_edges, 3) where num_edges is the number of edges in the graph. The features are:\n\n                - edge_type: the type of the edge (target-similar (1), target-context(2), similar-similar(3), similar-context(4), context-context(5), self-loop(0))\n\n                - similarity: the similarity between node embeddings in the current snapshot\n\n                - PMI: the PMI between nodes in the current snapshot\n\n        \"\"\"\n        edge_index_1 = []\n        edge_index_2 = []\n        edge_types = []\n        similarities = []\n        pmis = []\n        for word_idx1 in range(self.node_features.shape[0]):\n            for word_idx2 in range(word_idx1, self.node_features.shape[0]):\n                if word_idx1 == word_idx2:\n                    edge_type = 0\n                elif self.node_features[word_idx1][0] == 0 and self.node_features[word_idx2][0] == 1:\n                    edge_type = 1\n                elif self.node_features[word_idx1][0] == 0 and self.node_features[word_idx2][0] == 2:\n                    edge_type = 2\n                elif self.node_features[word_idx1][0] == 1 and self.node_features[word_idx2][0] == 1:\n                    edge_type = 3\n                elif self.node_features[word_idx1][0] == 1 and self.node_features[word_idx2][0] == 2:\n                    edge_type = 4\n                elif self.node_features[word_idx1][0] == 2 and self.node_features[word_idx2][0] == 2:\n                    edge_type = 5\n\n                similarity = self.get_similarity(word_idx1, word_idx2)\n                pmi = self.get_pmi(dataset, self.index_to_key[word_idx1], self.index_to_key[word_idx2])\n\n                if similarity &gt; sim_threshold:\n                    edge_index_1.append(word_idx1)\n                    edge_index_2.append(word_idx2)\n                    edge_types.append(edge_type)\n                    similarities.append(similarity)\n                    pmis.append(pmi)\n\n        edge_index = np.stack([edge_index_1, edge_index_2])\n        edge_features = np.stack([edge_types, similarities, pmis]).T\n\n        return edge_index, edge_features\n</code></pre>"},{"location":"doc/graph/edges/#semantics.graphs.temporal_graph.Edges.__init__","title":"<code>__init__(index_to_key, node_features, node_embeddings)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>index_to_key</code> <code>Dict[int, str]</code> <p>the index of the nodes of the word graph. The keys are the indices of the nodes and the values are the words of the nodes.</p> required <code>node_features</code> <code>ndarray</code> <p>the features of the nodes of the word graph of shape (num_nodes, 3) where num_nodes is the number of nodes in the graph.</p> required <code>node_embeddings</code> <code>ndarray</code> <p>the embeddings of the nodes of the word graph from the MLM model, of shape (num_nodes, 768).</p> required Source code in <code>semantics/graphs/temporal_graph.py</code> <pre><code>def __init__(\n        self,\n        index_to_key: Dict[int, str],\n        node_features: np.ndarray,\n        node_embeddings: np.ndarray,\n    ):\n    \"\"\"\n    Args:\n        index_to_key (Dict[int, str]): the index of the nodes of the word graph. The keys are the indices of the nodes and the values are the words of the nodes.\n        node_features (np.ndarray): the features of the nodes of the word graph of shape (num_nodes, 3) where num_nodes is the number of nodes in the graph.\n        node_embeddings (np.ndarray): the embeddings of the nodes of the word graph from the MLM model, of shape (num_nodes, 768).\n    \"\"\"\n\n    self.index_to_key = index_to_key\n    self.node_features = node_features\n    self.node_embeddings = node_embeddings\n</code></pre>"},{"location":"doc/graph/edges/#semantics.graphs.temporal_graph.Edges.get_edge_features","title":"<code>get_edge_features(dataset, sim_threshold=0.5)</code>","text":"<p>This method is used to get the edge features of the word graph.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>List[str]</code> <p>the dataset to get the edge features from</p> required <code>sim_threshold</code> <code>float</code> <p>the similarity threshold to create an edge between two nodes. Default: 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>edge_index</code> <code>ndarray</code> <p>the edge index of the word graph of shape (2, num_edges) where num_edges is the number of edges in the graph. The first row contains the indices of the first node of the edge and the second row contains the indices of the second node of the edge. An edge is created if the similarity between the two nodes is greater than sim_threshold.</p> <code>edge_features</code> <code>ndarray</code> <p>the edge features of the word graph of shape (num_edges, 3) where num_edges is the number of edges in the graph. The features are:</p> <ul> <li> <p>edge_type: the type of the edge (target-similar (1), target-context(2), similar-similar(3), similar-context(4), context-context(5), self-loop(0))</p> </li> <li> <p>similarity: the similarity between node embeddings in the current snapshot</p> </li> <li> <p>PMI: the PMI between nodes in the current snapshot</p> </li> </ul> Source code in <code>semantics/graphs/temporal_graph.py</code> <pre><code>def get_edge_features(self, dataset: List[str], sim_threshold: float = 0.5):\n    \"\"\"\n    This method is used to get the edge features of the word graph.\n\n    Args:\n        dataset (List[str]): the dataset to get the edge features from\n        sim_threshold (float): the similarity threshold to create an edge between two nodes. Default: 0.5.\n\n    Returns:\n        edge_index (np.ndarray): the edge index of the word graph of shape (2, num_edges) where num_edges is the number of edges in the graph. The first row contains the indices of the first node of the edge and the second row contains the indices of the second node of the edge. An edge is created if the similarity between the two nodes is greater than sim_threshold.\n        edge_features (np.ndarray): the edge features of the word graph of shape (num_edges, 3) where num_edges is the number of edges in the graph. The features are:\n\n            - edge_type: the type of the edge (target-similar (1), target-context(2), similar-similar(3), similar-context(4), context-context(5), self-loop(0))\n\n            - similarity: the similarity between node embeddings in the current snapshot\n\n            - PMI: the PMI between nodes in the current snapshot\n\n    \"\"\"\n    edge_index_1 = []\n    edge_index_2 = []\n    edge_types = []\n    similarities = []\n    pmis = []\n    for word_idx1 in range(self.node_features.shape[0]):\n        for word_idx2 in range(word_idx1, self.node_features.shape[0]):\n            if word_idx1 == word_idx2:\n                edge_type = 0\n            elif self.node_features[word_idx1][0] == 0 and self.node_features[word_idx2][0] == 1:\n                edge_type = 1\n            elif self.node_features[word_idx1][0] == 0 and self.node_features[word_idx2][0] == 2:\n                edge_type = 2\n            elif self.node_features[word_idx1][0] == 1 and self.node_features[word_idx2][0] == 1:\n                edge_type = 3\n            elif self.node_features[word_idx1][0] == 1 and self.node_features[word_idx2][0] == 2:\n                edge_type = 4\n            elif self.node_features[word_idx1][0] == 2 and self.node_features[word_idx2][0] == 2:\n                edge_type = 5\n\n            similarity = self.get_similarity(word_idx1, word_idx2)\n            pmi = self.get_pmi(dataset, self.index_to_key[word_idx1], self.index_to_key[word_idx2])\n\n            if similarity &gt; sim_threshold:\n                edge_index_1.append(word_idx1)\n                edge_index_2.append(word_idx2)\n                edge_types.append(edge_type)\n                similarities.append(similarity)\n                pmis.append(pmi)\n\n    edge_index = np.stack([edge_index_1, edge_index_2])\n    edge_features = np.stack([edge_types, similarities, pmis]).T\n\n    return edge_index, edge_features\n</code></pre>"},{"location":"doc/graph/edges/#semantics.graphs.temporal_graph.Edges.get_pmi","title":"<code>get_pmi(data, word1, word2)</code>","text":"<p>This method is used to get the PMI between two nodes.</p> <p>Parameters:</p> Name Type Description Default <code>word1</code> <code>str</code> <p>the first node (word)</p> required <code>word2</code> <code>str</code> <p>the second node (word)</p> required <p>Returns:</p> Name Type Description <code>pmi</code> <code>float</code> <p>the PMI between the two words in the dataset</p> Source code in <code>semantics/graphs/temporal_graph.py</code> <pre><code>def get_pmi(self, data: List[str], word1: str, word2: str) -&gt; float:\n    \"\"\"\n    This method is used to get the PMI between two nodes.\n\n    Args:\n        word1 (str): the first node (word)\n        word2 (str): the second node (word)\n\n    Returns:\n        pmi (float): the PMI between the two words in the dataset\n    \"\"\"\n    # Replace these methods with actual methods to get the word count, \n    # the co-occurrence count, and the total count.\n    word1_count = count_occurence(data, word1)\n    word2_count = count_occurence(data, word2)\n    co_occurrence_count = count_occurence(data, [word1, word2])\n    total_count = count_occurence(data)\n\n    # Calculate probabilities\n    p_word1 = word1_count / total_count\n    p_word2 = word2_count / total_count\n    p_co_occurrence = co_occurrence_count / total_count\n\n    # Calculate PMI\n    pmi = log(p_co_occurrence / (p_word1 * p_word2), 2) if p_co_occurrence &gt; 0 else 0\n    return pmi\n</code></pre>"},{"location":"doc/graph/edges/#semantics.graphs.temporal_graph.Edges.get_similarity","title":"<code>get_similarity(emb1, emb2)</code>","text":"<p>This method is used to get the similarity between two nodes.</p> <p>Parameters:</p> Name Type Description Default <code>emb1</code> <code>int</code> <p>the first index of the embedding in node_embeddings</p> required <code>emb2</code> <code>int</code> <p>the second index of the embedding in node_embeddings</p> required <p>Returns:</p> Name Type Description <code>similarity</code> <code>float</code> <p>the similarity between the two embeddings</p> Source code in <code>semantics/graphs/temporal_graph.py</code> <pre><code>def get_similarity(self, emb1: int, emb2: int) -&gt; float:\n    \"\"\"\n    This method is used to get the similarity between two nodes.\n\n    Args:\n        emb1 (int): the first index of the embedding in node_embeddings\n        emb2 (int): the second index of the embedding in node_embeddings\n\n    Returns:\n        similarity (float): the similarity between the two embeddings\n    \"\"\"\n    # np.dot(node1, node2) / (np.linalg.norm(node1) * np.linalg.norm(node2))\n    return torch.cosine_similarity(\n        torch.tensor(self.node_embeddings[emb1]).reshape(1,-1), \n        torch.tensor(self.node_embeddings[emb2]).reshape(1,-1)\n        ).item()\n</code></pre>"},{"location":"doc/graph/nodes/","title":"Nodes","text":"<p>This class is used to get the nodes of the word graph.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>str, dataset: List[str], level: int, k: int, c: int, word2vec_model_path: str, mlm_model_path: str, mlm_model_type: str = 'roberta') The constructor of the Nodes class.</p> <code>get_similar_nodes</code> <p>str) -&gt; List[str] This method is used to get the similar nodes of a word.</p> <code>get_context_nodes</code> <p>str) -&gt; List[str] This method is used to get the context nodes of a word.</p> <code>get_node_features</code> <p>Dict[str, List[str]]) This method is used to get the features of the nodes of the word graph.</p> Source code in <code>semantics/graphs/temporal_graph.py</code> <pre><code>class Nodes:\n    \"\"\"\n    This class is used to get the nodes of the word graph.\n\n    methods:\n        __init__(self, target_word: str, dataset: List[str], level: int, k: int, c: int, word2vec_model_path: str, mlm_model_path: str, mlm_model_type: str = 'roberta')\n            The constructor of the Nodes class.\n        get_similar_nodes(self, word: str) -&gt; List[str]\n            This method is used to get the similar nodes of a word.\n        get_context_nodes(self, word: str) -&gt; List[str]\n            This method is used to get the context nodes of a word.\n        get_nodes(self) -&gt; Dict[str, List[str]]\n            This method is used to get the nodes of the word graph.\n        get_node_features(self, nodes: Dict[str, List[str]])\n            This method is used to get the features of the nodes of the word graph.\n    \"\"\"\n    def __init__(\n            self,\n            target_word: str,\n            dataset: List[str],\n            level: int,\n            k: int,\n            c: int,\n            word2vec_model: Word2VecInference,\n            mlm_model: Union[RobertaInference, BertInference]\n            ):\n\n        \"\"\"\n        Args:\n            target_word (str): the word to get the nodes for\n            dataset (List[str]): the sentences to get the nodes from\n            level (int): the level of the graph to get\n            k (int): the number of similar nodes to get for each occurrence of the target word\n            c (int): the number of context nodes to get for the target word\n            word2vec_model (Word2VecInference): the word2vec model's Inference class\n            mlm_model (RobertaInference, BertInference): the MLM model's Inference class\n        \"\"\"\n\n        self.target_word = target_word\n        self.dataset = dataset\n        self.k = k\n        self.c = c\n        self.level = level\n        self.word2vec = word2vec_model\n        self.mlm = mlm_model\n\n\n    def get_similar_nodes(self, word: str, keep_k: int = 50) -&gt; List[str]:\n        \"\"\"\n        This method is used to get the similar nodes of a word using the MLM model.\n\n        Args:\n            word (str): the word to get the similar nodes for\n            keep_k (int): the number of similar nodes to keep for each occurrence of the word. Default: 50.\n\n        Returns:\n            similar_nodes (List[str]): the list of similar nodes of the word\n        \"\"\"\n        print(f'Getting the similar nodes of the word: ')\n        progress_bar = tqdm.tqdm(\n            range(len(self.dataset)), \n            desc=f\"{word}\", \n            dynamic_ncols=True\n            )\n        similar_nodes = []\n        for sentence in self.dataset:\n            similar_nodes += self.mlm.get_top_k_words(word, sentence, self.k)\n            progress_bar.update(1)\n\n        similar_nodes = list(set(similar_nodes))\n        similar_nodes = most_frequent(similar_nodes, keep_k)\n        print(f'{len(similar_nodes)} similar nodes found.', '\\n')\n        return similar_nodes\n\n    def get_context_nodes(self, word: str) -&gt; List[str]:\n        \"\"\"\n        This method is used to get the context nodes of a word using the word2vec model.\n\n        Args:\n            word (str): the word to get the context nodes for\n\n        Returns:\n            context_nodes (List[str]): the list of context nodes of the word\n        \"\"\"\n        print(f'Getting the context nodes of the word \"{word}\" ...')\n        context_nodes, _ = self.word2vec.get_top_k_words(word, self.c)\n\n        print(f'{len(list(set(context_nodes)))} context nodes found.', '\\n')\n        return list(set(context_nodes))\n\n    def get_nodes(self) -&gt; Dict[str, List[str]]:\n        \"\"\"\n        This method is used to get the nodes of the word graph (similar nodes, context nodes, and target node).\n\n        Returns:\n            nodes (Dict[str, List[str]]): the nodes of the word graph\n        \"\"\"\n        nodes = {'target_node': [], 'similar_nodes': [], 'context_nodes': []}\n        for level in range(self.level):\n            print(f'Getting the nodes of level {level} ...')\n            if level == 0:\n                similar_nodes = self.get_similar_nodes(self.target_word, keep_k= 5)\n                context_nodes = self.get_context_nodes(self.target_word)\n\n                nodes['similar_nodes'].append(similar_nodes)\n                nodes['context_nodes'].append(context_nodes)\n                nodes['target_node'].append([self.target_word])\n\n            else:\n                similar_nodes = []\n                context_nodes = []\n                for word in nodes['similar_nodes'][level-1]:\n                    similar_nodes += self.get_similar_nodes(word, keep_k= 5)\n                    context_nodes += self.get_context_nodes(word)\n\n\n                for word in nodes['context_nodes'][level-1]:\n                    similar_nodes += self.get_similar_nodes(word, keep_k= 5)\n                    context_nodes += self.get_context_nodes(word)\n\n                nodes['similar_nodes'].append(similar_nodes)\n                nodes['context_nodes'].append(context_nodes)          \n        return nodes\n\n    def get_node_features(self, nodes: Dict[str, List[str]]):\n        \"\"\"\n        This method is used to get the features of the nodes of the word graph.\n\n        Args:\n            nodes (Dict[str, List[str]]): the nodes of the word graph\n\n        Returns:\n            index (Dict[str, Dict[int, str]]): the index of the nodes of the word graph. The index contains the 'index_to_key' and 'key_to_index' mapping dictionaries. Example: in the index_to_key dictionary {0: target_word}, and in the key_to_index dictionary {target_word: 0}.\n            node_features (np.ndarray): the features of the nodes of the word graph of shape (num_nodes, 3) where num_nodes is the number of nodes in the graph. The features are:\n\n                - node_type: the type of the node (target: 0, similar: 1, context: 2).\n\n                - node_level: the level of the node in the graph. The target node is level 0.\n\n                - frequency: the frequency of the word node in the dataset.\n            embeddings (np.ndarray): the embeddings of the nodes of the word graph from the MLM model, of shape (num_nodes, 768).\n\n        Examples:\n            &gt;&gt;&gt; word2vec = Word2VecInference('word2vec.model')\n            &gt;&gt;&gt; mlm = RobertaInference('MLM_roberta')\n            &gt;&gt;&gt; n = Nodes(target_word='sentence', dataset=['this is a sentence', 'this is another sentence'], level=3, k=2, c=2, word2vec_model = word2vec, mlm_model = mlm)\n            &gt;&gt;&gt; nodes = n.get_nodes()\n            &gt;&gt;&gt; index, node_features, embeddings = n.get_node_features(nodes)\n            &gt;&gt;&gt; print(index)\n            {'index_to_key': {0: 'sentence', 1: 'this', 2: 'is', 3: 'a', 4: 'another'}, 'key_to_index': {'sentence': 0, 'this': 1, 'is': 2, 'a': 3, 'another': 4}\n            &gt;&gt;&gt; print(node_features)\n            [[0, 0, 2], [1, 1, 2], [1, 1, 2], [1, 1, 2], [2, 1, 2]]\n            &gt;&gt;&gt; print(embeddings.shape)\n            (5, 768)\n        \"\"\"\n        index_to_key = {}\n        key_to_index = {}\n        node_types = []\n        node_levels = []\n        frequencies = []\n        embeddings = []\n        count = 0\n        for node_type in ['target_node', 'similar_nodes', 'context_nodes']:\n            for level in range(len(nodes[node_type])):\n                for node in nodes[node_type][level]:\n                    index_to_key[count] = node\n                    key_to_index[node] = count\n                    count += 1 \n                    if node_type == 'target_node':\n                        node_types.append(0)\n                    elif node_type == 'similar_nodes':\n                        node_types.append(1)\n                    else:\n                        node_types.append(2)\n                    node_levels.append(level)\n                    frequencies.append(count_occurence(self.dataset, node))\n                    embeddings.append(self.mlm.get_embedding(main_word=node).mean(axis=0))\n\n        embeddings = np.array(embeddings)\n        node_features = np.stack([node_types, node_levels, frequencies]).T\n        # node_features = np.concatenate((node_features, embeddings), axis=1)\n\n        index = {'index_to_key': index_to_key, 'key_to_index': key_to_index}\n        return index, node_features, embeddings\n</code></pre>"},{"location":"doc/graph/nodes/#semantics.graphs.temporal_graph.Nodes.__init__","title":"<code>__init__(target_word, dataset, level, k, c, word2vec_model, mlm_model)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>target_word</code> <code>str</code> <p>the word to get the nodes for</p> required <code>dataset</code> <code>List[str]</code> <p>the sentences to get the nodes from</p> required <code>level</code> <code>int</code> <p>the level of the graph to get</p> required <code>k</code> <code>int</code> <p>the number of similar nodes to get for each occurrence of the target word</p> required <code>c</code> <code>int</code> <p>the number of context nodes to get for the target word</p> required <code>word2vec_model</code> <code>Word2VecInference</code> <p>the word2vec model's Inference class</p> required <code>mlm_model</code> <code>(RobertaInference, BertInference)</code> <p>the MLM model's Inference class</p> required Source code in <code>semantics/graphs/temporal_graph.py</code> <pre><code>def __init__(\n        self,\n        target_word: str,\n        dataset: List[str],\n        level: int,\n        k: int,\n        c: int,\n        word2vec_model: Word2VecInference,\n        mlm_model: Union[RobertaInference, BertInference]\n        ):\n\n    \"\"\"\n    Args:\n        target_word (str): the word to get the nodes for\n        dataset (List[str]): the sentences to get the nodes from\n        level (int): the level of the graph to get\n        k (int): the number of similar nodes to get for each occurrence of the target word\n        c (int): the number of context nodes to get for the target word\n        word2vec_model (Word2VecInference): the word2vec model's Inference class\n        mlm_model (RobertaInference, BertInference): the MLM model's Inference class\n    \"\"\"\n\n    self.target_word = target_word\n    self.dataset = dataset\n    self.k = k\n    self.c = c\n    self.level = level\n    self.word2vec = word2vec_model\n    self.mlm = mlm_model\n</code></pre>"},{"location":"doc/graph/nodes/#semantics.graphs.temporal_graph.Nodes.get_context_nodes","title":"<code>get_context_nodes(word)</code>","text":"<p>This method is used to get the context nodes of a word using the word2vec model.</p> <p>Parameters:</p> Name Type Description Default <code>word</code> <code>str</code> <p>the word to get the context nodes for</p> required <p>Returns:</p> Name Type Description <code>context_nodes</code> <code>List[str]</code> <p>the list of context nodes of the word</p> Source code in <code>semantics/graphs/temporal_graph.py</code> <pre><code>def get_context_nodes(self, word: str) -&gt; List[str]:\n    \"\"\"\n    This method is used to get the context nodes of a word using the word2vec model.\n\n    Args:\n        word (str): the word to get the context nodes for\n\n    Returns:\n        context_nodes (List[str]): the list of context nodes of the word\n    \"\"\"\n    print(f'Getting the context nodes of the word \"{word}\" ...')\n    context_nodes, _ = self.word2vec.get_top_k_words(word, self.c)\n\n    print(f'{len(list(set(context_nodes)))} context nodes found.', '\\n')\n    return list(set(context_nodes))\n</code></pre>"},{"location":"doc/graph/nodes/#semantics.graphs.temporal_graph.Nodes.get_node_features","title":"<code>get_node_features(nodes)</code>","text":"<p>This method is used to get the features of the nodes of the word graph.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>Dict[str, List[str]]</code> <p>the nodes of the word graph</p> required <p>Returns:</p> Name Type Description <code>index</code> <code>Dict[str, Dict[int, str]]</code> <p>the index of the nodes of the word graph. The index contains the 'index_to_key' and 'key_to_index' mapping dictionaries. Example: in the index_to_key dictionary {0: target_word}, and in the key_to_index dictionary {target_word: 0}.</p> <code>node_features</code> <code>ndarray</code> <p>the features of the nodes of the word graph of shape (num_nodes, 3) where num_nodes is the number of nodes in the graph. The features are:</p> <ul> <li> <p>node_type: the type of the node (target: 0, similar: 1, context: 2).</p> </li> <li> <p>node_level: the level of the node in the graph. The target node is level 0.</p> </li> <li> <p>frequency: the frequency of the word node in the dataset.</p> </li> </ul> <code>embeddings</code> <code>ndarray</code> <p>the embeddings of the nodes of the word graph from the MLM model, of shape (num_nodes, 768).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; word2vec = Word2VecInference('word2vec.model')\n&gt;&gt;&gt; mlm = RobertaInference('MLM_roberta')\n&gt;&gt;&gt; n = Nodes(target_word='sentence', dataset=['this is a sentence', 'this is another sentence'], level=3, k=2, c=2, word2vec_model = word2vec, mlm_model = mlm)\n&gt;&gt;&gt; nodes = n.get_nodes()\n&gt;&gt;&gt; index, node_features, embeddings = n.get_node_features(nodes)\n&gt;&gt;&gt; print(index)\n{'index_to_key': {0: 'sentence', 1: 'this', 2: 'is', 3: 'a', 4: 'another'}, 'key_to_index': {'sentence': 0, 'this': 1, 'is': 2, 'a': 3, 'another': 4}\n&gt;&gt;&gt; print(node_features)\n[[0, 0, 2], [1, 1, 2], [1, 1, 2], [1, 1, 2], [2, 1, 2]]\n&gt;&gt;&gt; print(embeddings.shape)\n(5, 768)\n</code></pre> Source code in <code>semantics/graphs/temporal_graph.py</code> <pre><code>def get_node_features(self, nodes: Dict[str, List[str]]):\n    \"\"\"\n    This method is used to get the features of the nodes of the word graph.\n\n    Args:\n        nodes (Dict[str, List[str]]): the nodes of the word graph\n\n    Returns:\n        index (Dict[str, Dict[int, str]]): the index of the nodes of the word graph. The index contains the 'index_to_key' and 'key_to_index' mapping dictionaries. Example: in the index_to_key dictionary {0: target_word}, and in the key_to_index dictionary {target_word: 0}.\n        node_features (np.ndarray): the features of the nodes of the word graph of shape (num_nodes, 3) where num_nodes is the number of nodes in the graph. The features are:\n\n            - node_type: the type of the node (target: 0, similar: 1, context: 2).\n\n            - node_level: the level of the node in the graph. The target node is level 0.\n\n            - frequency: the frequency of the word node in the dataset.\n        embeddings (np.ndarray): the embeddings of the nodes of the word graph from the MLM model, of shape (num_nodes, 768).\n\n    Examples:\n        &gt;&gt;&gt; word2vec = Word2VecInference('word2vec.model')\n        &gt;&gt;&gt; mlm = RobertaInference('MLM_roberta')\n        &gt;&gt;&gt; n = Nodes(target_word='sentence', dataset=['this is a sentence', 'this is another sentence'], level=3, k=2, c=2, word2vec_model = word2vec, mlm_model = mlm)\n        &gt;&gt;&gt; nodes = n.get_nodes()\n        &gt;&gt;&gt; index, node_features, embeddings = n.get_node_features(nodes)\n        &gt;&gt;&gt; print(index)\n        {'index_to_key': {0: 'sentence', 1: 'this', 2: 'is', 3: 'a', 4: 'another'}, 'key_to_index': {'sentence': 0, 'this': 1, 'is': 2, 'a': 3, 'another': 4}\n        &gt;&gt;&gt; print(node_features)\n        [[0, 0, 2], [1, 1, 2], [1, 1, 2], [1, 1, 2], [2, 1, 2]]\n        &gt;&gt;&gt; print(embeddings.shape)\n        (5, 768)\n    \"\"\"\n    index_to_key = {}\n    key_to_index = {}\n    node_types = []\n    node_levels = []\n    frequencies = []\n    embeddings = []\n    count = 0\n    for node_type in ['target_node', 'similar_nodes', 'context_nodes']:\n        for level in range(len(nodes[node_type])):\n            for node in nodes[node_type][level]:\n                index_to_key[count] = node\n                key_to_index[node] = count\n                count += 1 \n                if node_type == 'target_node':\n                    node_types.append(0)\n                elif node_type == 'similar_nodes':\n                    node_types.append(1)\n                else:\n                    node_types.append(2)\n                node_levels.append(level)\n                frequencies.append(count_occurence(self.dataset, node))\n                embeddings.append(self.mlm.get_embedding(main_word=node).mean(axis=0))\n\n    embeddings = np.array(embeddings)\n    node_features = np.stack([node_types, node_levels, frequencies]).T\n    # node_features = np.concatenate((node_features, embeddings), axis=1)\n\n    index = {'index_to_key': index_to_key, 'key_to_index': key_to_index}\n    return index, node_features, embeddings\n</code></pre>"},{"location":"doc/graph/nodes/#semantics.graphs.temporal_graph.Nodes.get_nodes","title":"<code>get_nodes()</code>","text":"<p>This method is used to get the nodes of the word graph (similar nodes, context nodes, and target node).</p> <p>Returns:</p> Name Type Description <code>nodes</code> <code>Dict[str, List[str]]</code> <p>the nodes of the word graph</p> Source code in <code>semantics/graphs/temporal_graph.py</code> <pre><code>def get_nodes(self) -&gt; Dict[str, List[str]]:\n    \"\"\"\n    This method is used to get the nodes of the word graph (similar nodes, context nodes, and target node).\n\n    Returns:\n        nodes (Dict[str, List[str]]): the nodes of the word graph\n    \"\"\"\n    nodes = {'target_node': [], 'similar_nodes': [], 'context_nodes': []}\n    for level in range(self.level):\n        print(f'Getting the nodes of level {level} ...')\n        if level == 0:\n            similar_nodes = self.get_similar_nodes(self.target_word, keep_k= 5)\n            context_nodes = self.get_context_nodes(self.target_word)\n\n            nodes['similar_nodes'].append(similar_nodes)\n            nodes['context_nodes'].append(context_nodes)\n            nodes['target_node'].append([self.target_word])\n\n        else:\n            similar_nodes = []\n            context_nodes = []\n            for word in nodes['similar_nodes'][level-1]:\n                similar_nodes += self.get_similar_nodes(word, keep_k= 5)\n                context_nodes += self.get_context_nodes(word)\n\n\n            for word in nodes['context_nodes'][level-1]:\n                similar_nodes += self.get_similar_nodes(word, keep_k= 5)\n                context_nodes += self.get_context_nodes(word)\n\n            nodes['similar_nodes'].append(similar_nodes)\n            nodes['context_nodes'].append(context_nodes)          \n    return nodes\n</code></pre>"},{"location":"doc/graph/nodes/#semantics.graphs.temporal_graph.Nodes.get_similar_nodes","title":"<code>get_similar_nodes(word, keep_k=50)</code>","text":"<p>This method is used to get the similar nodes of a word using the MLM model.</p> <p>Parameters:</p> Name Type Description Default <code>word</code> <code>str</code> <p>the word to get the similar nodes for</p> required <code>keep_k</code> <code>int</code> <p>the number of similar nodes to keep for each occurrence of the word. Default: 50.</p> <code>50</code> <p>Returns:</p> Name Type Description <code>similar_nodes</code> <code>List[str]</code> <p>the list of similar nodes of the word</p> Source code in <code>semantics/graphs/temporal_graph.py</code> <pre><code>def get_similar_nodes(self, word: str, keep_k: int = 50) -&gt; List[str]:\n    \"\"\"\n    This method is used to get the similar nodes of a word using the MLM model.\n\n    Args:\n        word (str): the word to get the similar nodes for\n        keep_k (int): the number of similar nodes to keep for each occurrence of the word. Default: 50.\n\n    Returns:\n        similar_nodes (List[str]): the list of similar nodes of the word\n    \"\"\"\n    print(f'Getting the similar nodes of the word: ')\n    progress_bar = tqdm.tqdm(\n        range(len(self.dataset)), \n        desc=f\"{word}\", \n        dynamic_ncols=True\n        )\n    similar_nodes = []\n    for sentence in self.dataset:\n        similar_nodes += self.mlm.get_top_k_words(word, sentence, self.k)\n        progress_bar.update(1)\n\n    similar_nodes = list(set(similar_nodes))\n    similar_nodes = most_frequent(similar_nodes, keep_k)\n    print(f'{len(similar_nodes)} similar nodes found.', '\\n')\n    return similar_nodes\n</code></pre>"},{"location":"doc/graph/temporal_graph/","title":"Temporal Graph","text":"<p>This class is used to get the temporal graph of a word.</p> <p>Methods:</p> Name Description <code>add_graph</code> <p>str, level: int, k: int, c: int, dataset: List[str], word2vec_model: Word2VecInference, mlm_model: Union[RobertaInference, BertInference]) This method is used to add a snapshot to the temporal graph.</p> <code>get_aligned_graph</code> <p>dict, previous_graph: dict) -&gt; (dict, dict) This method is used to align the nodes of the current snapshot with the nodes of the previous snapshot.</p> <code>label_previous_graph</code> <p>dict, previous_graph: dict, label_feature_idx: int = 1) -&gt; (np.ndarray, np.ndarray) This method is used to label the edges of the previous snapshot with the edge feature values in the current snapshot.</p> Source code in <code>semantics/graphs/temporal_graph.py</code> <pre><code>class TemporalGraph:\n    \"\"\"\n    This class is used to get the temporal graph of a word.\n\n    methods:\n        __init__(self)\n            The constructor of the TemporalGraph class.\n        __getitem__(self, idx)\n            Retrieves the snapshot at the specified index.\n        add_graph(self, target_word: str, level: int, k: int, c: int, dataset: List[str], word2vec_model: Word2VecInference, mlm_model: Union[RobertaInference, BertInference])\n            This method is used to add a snapshot to the temporal graph.\n        construct_graph(self, current_index, current_node_feature_matrix, current_embeddings, current_edge_index, current_edge_feature_matrix)\n            This method is used to construct the temporal graph.\n        get_aligned_graph(self, current_graph: dict, previous_graph: dict) -&gt; (dict, dict)\n            This method is used to align the nodes of the current snapshot with the nodes of the previous snapshot.\n        label_previous_graph(self, current_graph: dict, previous_graph: dict, label_feature_idx: int = 1) -&gt; (np.ndarray, np.ndarray)\n            This method is used to label the edges of the previous snapshot with the edge feature values in the current snapshot.\n    \"\"\"\n    def __init__(\n            self\n            ):\n\n        \"\"\"\n        Attributes:\n            snapshots (List[dict]): the snapshots of the temporal graph. Each snapshot is a dictionary containing the index of the nodes of the snapshot.\n            xs (List[np.ndarray]): the features of the nodes of the temporal graph.\n            edge_indices (List[np.ndarray]): the edge index of the temporal graph.\n            edge_features (List[np.ndarray]): the edge features of the temporal graph.\n            ys (List[np.ndarray]): the labels of the edges of the temporal graph.\n            y_indices (List[np.ndarray]): the indices of the labels of the edges of the temporal graph.\n\n        \"\"\"\n\n        self.snapshots = []\n        self.xs = []\n        self.edge_indices = []\n        self.edge_features = []\n        self.ys = []\n        self.y_indices = []\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Retrieves the snapshot at the specified index.\n\n        Parameters:\n            idx (int): Index of the item to retrieve.\n\n        Returns:\n            snapshot (dict): the graph data at the specified index.\n            node_features (np.ndarray): the features of the nodes of the graph at the specified index.\n            edge_index (np.ndarray): the edge index of the graph at the specified index.\n            edge_feature (np.ndarray): the edge features of the graph at the specified index.\n            labels (np.ndarray): the labels of the edges of the graph at the specified index.\n            labels_mask (np.ndarray): the indices of the labels of the edges of the graph at the specified index.\n        \"\"\"\n        # Get the tokenized inputs at the specified index\n        snapshot = self.snapshots[idx]\n        node_features = self.xs[idx]\n        edge_index = self.edge_indices[idx]\n        edge_feature = self.edge_features[idx]\n        labels = self.ys[idx]\n        labels_mask = self.y_indices[idx]\n\n        return snapshot, node_features, edge_index, edge_feature, labels, labels_mask\n\n\n    def add_graph(\n            self,\n            target_word: str, \n            level: int, \n            k: int, \n            c: int,\n            dataset: List[str], \n            word2vec_model: Word2VecInference, \n            mlm_model: Union[RobertaInference, BertInference]\n            ) -&gt; None:\n        \"\"\"\n        This method is used to add a snapshot to the temporal graph.\n\n        Args:\n            target_word (str): the word to get the nodes for\n            level (int): the level of the graph to get\n            k (int): the number of similar nodes to get for each occurrence of the target word\n            c (int): the number of context nodes to get for the target word\n            dataset (List[str]): the sentences to get the nodes from\n            word2vec_model (Word2VecInference): the word2vec model's Inference class\n            mlm_model (RobertaInference, BertInference): the MLM model's Inference class\n\n        Examples:\n            &gt;&gt;&gt; word2vec = Word2VecInference('word2vec.model')\n            &gt;&gt;&gt; mlm = RobertaInference('MLM_roberta')\n            &gt;&gt;&gt; tg = TemporalGraph()\n            &gt;&gt;&gt; tg.add_graph(target_word='sentence', level=3, k=2, c=2, dataset=['this is a sentence', 'this is another sentence'], word2vec_model = word2vec, mlm_model = mlm)\n            &gt;&gt;&gt; snapshot, node_features, edge_index, edge_feature, _, _= tg[0]\n            &gt;&gt;&gt; print(snapshot)\n            {'index_to_key': {0: 'sentence', 1: 'this', 2: 'is', 3: 'a', 4: 'another'}, 'key_to_index': {'sentence': 0, 'this': 1, 'is': 2, 'a': 3, 'another': 4}\n            &gt;&gt;&gt; print(node_features)\n            [[0, 0, 2, ...], [1, 1, 2, ...], [1, 1, 2, ...], [1, 1, 2, ...], [2, 1, 2, ...]]\n            &gt;&gt;&gt; print(edge_index)\n            [[0, 0, 0, 1, 1, 2, 2, 3, 3, 4], [1, 2, 4, 1, 3, 1, 4, 1, 4, 4]]\n            &gt;&gt;&gt; print(edge_feature)\n            [[0, 0.9999999403953552, 0.0], [0, 0.9999999403953552, 0.0], [0, 0.9999999403953552, 0.0], [0, 0.9999999403953552, 0.0], [0, 0.9999999403953552, 0.0], [1, 0.9999999403953552, 0.0], [1, 0.9999999403953552, 0.0], [1, 0.9999999403953552, 0.0], [1, 0.9999999403953552, 0.0], [1, 0.9999999403953552, 0.0]]\n\n            &gt;&gt;&gt; tg.add_graph(target_word='sentence', level=3, k=2, c=2, dataset=['this is a sentence', 'this is another sentence', 'this is a third sentence'], word2vec_model = word2vec, mlm_model = mlm)\n            &gt;&gt;&gt; _, _, _, _, labels, label_mask= tg[0]\n            &gt;&gt;&gt; print(labels)\n            [[0.9999, 1., 0.0001]]\n            &gt;&gt;&gt; print(label_mask)\n            [[0, 1, 0], [1, 2, 4]]\n        \"\"\"\n\n        print(f'Adding the nodes of the word graph for the word \"{target_word}\"...')\n        nodes = Nodes(\n            target_word= target_word,\n            dataset=dataset,\n            level= level,\n            k= k,\n            c= c,\n            word2vec_model = word2vec_model,\n            mlm_model = mlm_model\n            )\n\n        nds = nodes.get_nodes()\n        print('Getting their features...', '\\n')\n        index, node_feature_matrix, embeddings = nodes.get_node_features(nds)\n        print(f'Adding the edges of the word graph for the word \"{target_word}\"...')\n        edges = Edges(\n            index_to_key=index['index_to_key'],\n            node_features=node_feature_matrix,\n            node_embeddings=embeddings\n        )\n        edge_index, edge_feature_matrix = edges.get_edge_features(dataset)\n\n        print('Constructing the temporal graph...', '\\n')\n        self.construct_graph(\n            current_index=index,\n            current_node_feature_matrix=node_feature_matrix,\n            current_embeddings=embeddings,\n            current_edge_index=edge_index,\n            current_edge_feature_matrix=edge_feature_matrix\n        )\n\n\n    def construct_graph(\n            self, \n            current_index, \n            current_node_feature_matrix, \n            current_embeddings, \n            current_edge_index, \n            current_edge_feature_matrix\n            ):\n\n        \"\"\"\n        This method is used to construct the temporal graph.\n\n        Args:\n            current_index (dict): the index of the nodes of the current snapshot.\n            current_node_feature_matrix (np.ndarray): the features of the nodes of the current snapshot.\n            current_embeddings (np.ndarray): the embeddings of the nodes of the current snapshot.\n            current_edge_index (np.ndarray): the edge index of the current snapshot.\n            current_edge_feature_matrix (np.ndarray): the edge features of the current snapshot.\n        \"\"\"\n\n        if len(self.snapshots) == 0:\n            print('Adding the first snapshot to the temporal graph...', '\\n')\n            self.snapshots.append(current_index)\n            self.xs.append(np.concatenate((current_node_feature_matrix, current_embeddings), axis=1))\n            self.edge_indices.append(current_edge_index)\n            self.edge_features.append(current_edge_feature_matrix)\n            self.ys.append([])\n            self.y_indices.append([])\n\n        else:\n            print(f'Adding the {len(self.snapshots)} snapshot to the temporal graph...', '\\n')\n            previous_index, previous_node_features, previous_edge_index, previous_edge_feature, _, _= self[-1]\n            current_node_features = np.concatenate((current_node_feature_matrix, current_embeddings), axis=1)\n\n            previous_graph = {\n                'index': previous_index,\n                'node_features': previous_node_features,\n                'edge_index': previous_edge_index,\n                'edge_features': previous_edge_feature\n            }\n\n            current_graph = {\n                'index': current_index,\n                'node_features': current_node_features,\n                'edge_index': current_edge_index,\n                'edge_features': current_edge_feature_matrix\n            }\n\n            print('Aligning the nodes of the current snapshot with the nodes of the previous snapshot...', '\\n')\n            aligned_previous_graph, aligned_current_graph = self.get_aligned_graph(current_graph, previous_graph)\n            print('Labeling the edges of the previous snapshot with the edge feature values in the current snapshot...', '\\n')\n            previous_labels, previous_label_mask = self.label_previous_graph(current_graph, previous_graph)\n\n            self.snapshots[-1] = aligned_previous_graph['index']\n            self.xs[-1] = aligned_previous_graph['node_features']\n            self.edge_indices[-1] = aligned_previous_graph['edge_index']\n            self.edge_features[-1] = aligned_previous_graph['edge_features']\n            self.ys[-1] = previous_labels\n            self.y_indices[-1] = previous_label_mask\n\n            self.snapshots.append(aligned_current_graph['index'])\n            self.xs.append(aligned_current_graph['node_features'])\n            self.edge_indices.append(aligned_current_graph['edge_index'])\n            self.edge_features.append(aligned_current_graph['edge_features'])\n            self.ys.append([])\n            self.y_indices.append([])\n\n\n\n    def get_aligned_graph(\n            self, \n            current_graph: dict, \n            previous_graph: dict\n            ) -&gt; (dict, dict):\n\n        \"\"\"\n        This method is used to align the nodes of the current snapshot with the nodes of the previous snapshot.\n\n        Args:\n            current_graph (dict): the current snapshot of the temporal graph to align with the previous snapshot.\n            previous_graph (dict): the previous snapshot of the temporal graph to align with the current snapshot.\n\n        Returns:\n            aligned_previous_graph (dict): the aligned previous snapshot of the temporal graph.\n            aligned_current_graph (dict): the aligned current snapshot of the temporal graph.\n        \"\"\"\n\n        current_index = current_graph['index']\n        previous_index = previous_graph['index']\n\n        if current_index == previous_index:\n            return current_graph\n\n        current_words = set(current_index['key_to_index'].keys())\n        previous_words = set(previous_index['key_to_index'].keys())\n\n        dynamic_graph = current_words != previous_words\n\n        if not dynamic_graph:\n            index_mapping = {current_index['key_to_index'][key]: previous_index['key_to_index'][key] for key in current_index['key_to_index']}\n\n            reordered_node_feature_matrix = np.zeros_like(current_graph['node_features'])\n            for current_idx, previous_idx in index_mapping.items():\n                reordered_node_feature_matrix[previous_idx] = current_graph['node_features'][current_idx]\n\n\n            updated_edge_index = np.zeros_like(current_graph['edge_index'])\n            for i in range(current_graph['edge_index'].shape[1]):\n                updated_edge_index[0, i] = index_mapping.get(current_graph['edge_index'][0, i], -1)\n                updated_edge_index[1, i] = index_mapping.get(current_graph['edge_index'][1, i], -1)\n            # Remove edges where one of the nodes does not exist anymore (indicated by -1)\n            updated_edge_index = updated_edge_index[:, ~(updated_edge_index == -1).any(axis=0)]\n\n            aligned_current_graph = {\n                'index': previous_graph['index'],\n                'node_features': reordered_node_feature_matrix,\n                'edge_index': updated_edge_index,\n                'edge_features': current_graph['edge_features']\n            }\n            return previous_graph, aligned_current_graph\n\n\n        else:\n            all_words = current_words | previous_words\n            unified_dict = {word: idx for idx, word in enumerate(all_words)}\n            unified_dict_reverse = {idx: word for idx, word in enumerate(all_words)}\n            reordered_index = {'index_to_key': unified_dict_reverse, 'key_to_index': unified_dict}\n\n            reordered_previous_node_feature_matrix = np.zeros((len(unified_dict), previous_graph['node_features'].shape[1]))\n            for word, index in previous_index['key_to_index'].items():\n                if word in unified_dict:\n                    reordered_previous_node_feature_matrix[unified_dict[word]] = previous_graph['node_features'][index]\n\n\n            reordered_current_node_feature_matrix = np.zeros((len(unified_dict), current_graph['node_features'].shape[1]))\n            for word, index in current_index['key_to_index'].items():\n                if word in unified_dict:\n                    reordered_current_node_feature_matrix[unified_dict[word]] = current_graph['node_features'][index]\n\n\n            # Mapping old indices to new indices for the previous dictionary\n            previous_index_mapping = {old_index: unified_dict[word] for word, old_index in previous_index['key_to_index'].items()}\n            updated_previous_edge_index = np.array(previous_graph['edge_index'])\n            for i in range(previous_graph['edge_index'].shape[1]):\n                updated_previous_edge_index[0, i] = previous_index_mapping.get(previous_graph['edge_index'][0, i], -1)\n                updated_previous_edge_index[1, i] = previous_index_mapping.get(previous_graph['edge_index'][1, i], -1)\n            # Remove edges where one of the nodes does not exist anymore (indicated by -1)\n            updated_previous_edge_index = updated_previous_edge_index[:, ~(updated_previous_edge_index == -1).any(axis=0)]\n\n            # Mapping old indices to new indices for the current dictionary\n            current_index_mapping = {old_index: unified_dict[word] for word, old_index in current_index['key_to_index'].items()}\n            updated_current_edge_index = np.array(current_graph['edge_index'])\n            for i in range(current_graph['edge_index'].shape[1]):\n                updated_current_edge_index[0, i] = current_index_mapping.get(current_graph['edge_index'][0, i], -1)\n                updated_current_edge_index[1, i] = current_index_mapping.get(current_graph['edge_index'][1, i], -1)\n            # Remove edges where one of the nodes does not exist anymore (indicated by -1)\n            updated_current_edge_index = updated_current_edge_index[:, ~(updated_current_edge_index == -1).any(axis=0)]\n\n            aligned_previous_graph = {\n                'index': reordered_index,\n                'node_features': reordered_previous_node_feature_matrix,\n                'edge_index': updated_previous_edge_index,\n                'edge_features': previous_graph['edge_features']\n            }\n\n            aligned_current_graph = {\n                'index': reordered_index,\n                'node_features': reordered_current_node_feature_matrix,\n                'edge_index': updated_current_edge_index,\n                'edge_features': current_graph['edge_features']\n            }\n\n            return aligned_previous_graph, aligned_current_graph\n\n\n\n    def label_previous_graph(\n            self,\n            current_graph: dict,\n            previous_graph: dict,\n            label_feature_idx: int = 1\n            ) -&gt; (np.ndarray, np.ndarray):\n        \"\"\"\n        This method is used to label the edges of the previous snapshot with the edge feature values in the current snapshot.\n\n        Args:\n            current_graph (dict): the current snapshot of the temporal graph to use for labeling the previous snapshot.\n            previous_graph (dict): the previous snapshot of the temporal graph to label.\n            label_feature_idx (int): the index of the feature to use as labels. Default: 1.\n\n        Returns:\n            labels (np.ndarray): the labels of the edges of the graph at the specified index.\n            labels_mask (np.ndarray): the indices of the labels of the edges of the graph at the specified index.\n        \"\"\"\n\n        current_edge_index = current_graph['edge_index']\n        current_edge_features = current_graph['edge_features']\n\n        previous_edge_index = previous_graph['edge_index']\n\n        previous_edges = [tuple(edge) for edge in previous_edge_index.T]\n        current_edges  = [tuple(edge) for edge in current_edge_index.T]\n\n        labels = []\n\n        label_mask_1 = []\n        label_mask_2 = []\n\n        for i, previous_edge in enumerate(previous_edges):\n            if previous_edge in current_edges:\n                label_mask_1.append(previous_edge[0])\n                label_mask_2.append(previous_edge[1])\n\n                current_index = current_edges.index(previous_edge)\n                labels.append(current_edge_features[current_index][label_feature_idx])\n\n        label_mask = np.stack([label_mask_1, label_mask_2])\n        labels = np.array(labels)\n\n        return labels, label_mask\n</code></pre>"},{"location":"doc/graph/temporal_graph/#semantics.graphs.temporal_graph.TemporalGraph.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Retrieves the snapshot at the specified index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the item to retrieve.</p> required <p>Returns:</p> Name Type Description <code>snapshot</code> <code>dict</code> <p>the graph data at the specified index.</p> <code>node_features</code> <code>ndarray</code> <p>the features of the nodes of the graph at the specified index.</p> <code>edge_index</code> <code>ndarray</code> <p>the edge index of the graph at the specified index.</p> <code>edge_feature</code> <code>ndarray</code> <p>the edge features of the graph at the specified index.</p> <code>labels</code> <code>ndarray</code> <p>the labels of the edges of the graph at the specified index.</p> <code>labels_mask</code> <code>ndarray</code> <p>the indices of the labels of the edges of the graph at the specified index.</p> Source code in <code>semantics/graphs/temporal_graph.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"\n    Retrieves the snapshot at the specified index.\n\n    Parameters:\n        idx (int): Index of the item to retrieve.\n\n    Returns:\n        snapshot (dict): the graph data at the specified index.\n        node_features (np.ndarray): the features of the nodes of the graph at the specified index.\n        edge_index (np.ndarray): the edge index of the graph at the specified index.\n        edge_feature (np.ndarray): the edge features of the graph at the specified index.\n        labels (np.ndarray): the labels of the edges of the graph at the specified index.\n        labels_mask (np.ndarray): the indices of the labels of the edges of the graph at the specified index.\n    \"\"\"\n    # Get the tokenized inputs at the specified index\n    snapshot = self.snapshots[idx]\n    node_features = self.xs[idx]\n    edge_index = self.edge_indices[idx]\n    edge_feature = self.edge_features[idx]\n    labels = self.ys[idx]\n    labels_mask = self.y_indices[idx]\n\n    return snapshot, node_features, edge_index, edge_feature, labels, labels_mask\n</code></pre>"},{"location":"doc/graph/temporal_graph/#semantics.graphs.temporal_graph.TemporalGraph.__init__","title":"<code>__init__()</code>","text":"<p>Attributes:</p> Name Type Description <code>snapshots</code> <code>List[dict]</code> <p>the snapshots of the temporal graph. Each snapshot is a dictionary containing the index of the nodes of the snapshot.</p> <code>xs</code> <code>List[ndarray]</code> <p>the features of the nodes of the temporal graph.</p> <code>edge_indices</code> <code>List[ndarray]</code> <p>the edge index of the temporal graph.</p> <code>edge_features</code> <code>List[ndarray]</code> <p>the edge features of the temporal graph.</p> <code>ys</code> <code>List[ndarray]</code> <p>the labels of the edges of the temporal graph.</p> <code>y_indices</code> <code>List[ndarray]</code> <p>the indices of the labels of the edges of the temporal graph.</p> Source code in <code>semantics/graphs/temporal_graph.py</code> <pre><code>def __init__(\n        self\n        ):\n\n    \"\"\"\n    Attributes:\n        snapshots (List[dict]): the snapshots of the temporal graph. Each snapshot is a dictionary containing the index of the nodes of the snapshot.\n        xs (List[np.ndarray]): the features of the nodes of the temporal graph.\n        edge_indices (List[np.ndarray]): the edge index of the temporal graph.\n        edge_features (List[np.ndarray]): the edge features of the temporal graph.\n        ys (List[np.ndarray]): the labels of the edges of the temporal graph.\n        y_indices (List[np.ndarray]): the indices of the labels of the edges of the temporal graph.\n\n    \"\"\"\n\n    self.snapshots = []\n    self.xs = []\n    self.edge_indices = []\n    self.edge_features = []\n    self.ys = []\n    self.y_indices = []\n</code></pre>"},{"location":"doc/graph/temporal_graph/#semantics.graphs.temporal_graph.TemporalGraph.add_graph","title":"<code>add_graph(target_word, level, k, c, dataset, word2vec_model, mlm_model)</code>","text":"<p>This method is used to add a snapshot to the temporal graph.</p> <p>Parameters:</p> Name Type Description Default <code>target_word</code> <code>str</code> <p>the word to get the nodes for</p> required <code>level</code> <code>int</code> <p>the level of the graph to get</p> required <code>k</code> <code>int</code> <p>the number of similar nodes to get for each occurrence of the target word</p> required <code>c</code> <code>int</code> <p>the number of context nodes to get for the target word</p> required <code>dataset</code> <code>List[str]</code> <p>the sentences to get the nodes from</p> required <code>word2vec_model</code> <code>Word2VecInference</code> <p>the word2vec model's Inference class</p> required <code>mlm_model</code> <code>(RobertaInference, BertInference)</code> <p>the MLM model's Inference class</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; word2vec = Word2VecInference('word2vec.model')\n&gt;&gt;&gt; mlm = RobertaInference('MLM_roberta')\n&gt;&gt;&gt; tg = TemporalGraph()\n&gt;&gt;&gt; tg.add_graph(target_word='sentence', level=3, k=2, c=2, dataset=['this is a sentence', 'this is another sentence'], word2vec_model = word2vec, mlm_model = mlm)\n&gt;&gt;&gt; snapshot, node_features, edge_index, edge_feature, _, _= tg[0]\n&gt;&gt;&gt; print(snapshot)\n{'index_to_key': {0: 'sentence', 1: 'this', 2: 'is', 3: 'a', 4: 'another'}, 'key_to_index': {'sentence': 0, 'this': 1, 'is': 2, 'a': 3, 'another': 4}\n&gt;&gt;&gt; print(node_features)\n[[0, 0, 2, ...], [1, 1, 2, ...], [1, 1, 2, ...], [1, 1, 2, ...], [2, 1, 2, ...]]\n&gt;&gt;&gt; print(edge_index)\n[[0, 0, 0, 1, 1, 2, 2, 3, 3, 4], [1, 2, 4, 1, 3, 1, 4, 1, 4, 4]]\n&gt;&gt;&gt; print(edge_feature)\n[[0, 0.9999999403953552, 0.0], [0, 0.9999999403953552, 0.0], [0, 0.9999999403953552, 0.0], [0, 0.9999999403953552, 0.0], [0, 0.9999999403953552, 0.0], [1, 0.9999999403953552, 0.0], [1, 0.9999999403953552, 0.0], [1, 0.9999999403953552, 0.0], [1, 0.9999999403953552, 0.0], [1, 0.9999999403953552, 0.0]]\n</code></pre> <pre><code>&gt;&gt;&gt; tg.add_graph(target_word='sentence', level=3, k=2, c=2, dataset=['this is a sentence', 'this is another sentence', 'this is a third sentence'], word2vec_model = word2vec, mlm_model = mlm)\n&gt;&gt;&gt; _, _, _, _, labels, label_mask= tg[0]\n&gt;&gt;&gt; print(labels)\n[[0.9999, 1., 0.0001]]\n&gt;&gt;&gt; print(label_mask)\n[[0, 1, 0], [1, 2, 4]]\n</code></pre> Source code in <code>semantics/graphs/temporal_graph.py</code> <pre><code>def add_graph(\n        self,\n        target_word: str, \n        level: int, \n        k: int, \n        c: int,\n        dataset: List[str], \n        word2vec_model: Word2VecInference, \n        mlm_model: Union[RobertaInference, BertInference]\n        ) -&gt; None:\n    \"\"\"\n    This method is used to add a snapshot to the temporal graph.\n\n    Args:\n        target_word (str): the word to get the nodes for\n        level (int): the level of the graph to get\n        k (int): the number of similar nodes to get for each occurrence of the target word\n        c (int): the number of context nodes to get for the target word\n        dataset (List[str]): the sentences to get the nodes from\n        word2vec_model (Word2VecInference): the word2vec model's Inference class\n        mlm_model (RobertaInference, BertInference): the MLM model's Inference class\n\n    Examples:\n        &gt;&gt;&gt; word2vec = Word2VecInference('word2vec.model')\n        &gt;&gt;&gt; mlm = RobertaInference('MLM_roberta')\n        &gt;&gt;&gt; tg = TemporalGraph()\n        &gt;&gt;&gt; tg.add_graph(target_word='sentence', level=3, k=2, c=2, dataset=['this is a sentence', 'this is another sentence'], word2vec_model = word2vec, mlm_model = mlm)\n        &gt;&gt;&gt; snapshot, node_features, edge_index, edge_feature, _, _= tg[0]\n        &gt;&gt;&gt; print(snapshot)\n        {'index_to_key': {0: 'sentence', 1: 'this', 2: 'is', 3: 'a', 4: 'another'}, 'key_to_index': {'sentence': 0, 'this': 1, 'is': 2, 'a': 3, 'another': 4}\n        &gt;&gt;&gt; print(node_features)\n        [[0, 0, 2, ...], [1, 1, 2, ...], [1, 1, 2, ...], [1, 1, 2, ...], [2, 1, 2, ...]]\n        &gt;&gt;&gt; print(edge_index)\n        [[0, 0, 0, 1, 1, 2, 2, 3, 3, 4], [1, 2, 4, 1, 3, 1, 4, 1, 4, 4]]\n        &gt;&gt;&gt; print(edge_feature)\n        [[0, 0.9999999403953552, 0.0], [0, 0.9999999403953552, 0.0], [0, 0.9999999403953552, 0.0], [0, 0.9999999403953552, 0.0], [0, 0.9999999403953552, 0.0], [1, 0.9999999403953552, 0.0], [1, 0.9999999403953552, 0.0], [1, 0.9999999403953552, 0.0], [1, 0.9999999403953552, 0.0], [1, 0.9999999403953552, 0.0]]\n\n        &gt;&gt;&gt; tg.add_graph(target_word='sentence', level=3, k=2, c=2, dataset=['this is a sentence', 'this is another sentence', 'this is a third sentence'], word2vec_model = word2vec, mlm_model = mlm)\n        &gt;&gt;&gt; _, _, _, _, labels, label_mask= tg[0]\n        &gt;&gt;&gt; print(labels)\n        [[0.9999, 1., 0.0001]]\n        &gt;&gt;&gt; print(label_mask)\n        [[0, 1, 0], [1, 2, 4]]\n    \"\"\"\n\n    print(f'Adding the nodes of the word graph for the word \"{target_word}\"...')\n    nodes = Nodes(\n        target_word= target_word,\n        dataset=dataset,\n        level= level,\n        k= k,\n        c= c,\n        word2vec_model = word2vec_model,\n        mlm_model = mlm_model\n        )\n\n    nds = nodes.get_nodes()\n    print('Getting their features...', '\\n')\n    index, node_feature_matrix, embeddings = nodes.get_node_features(nds)\n    print(f'Adding the edges of the word graph for the word \"{target_word}\"...')\n    edges = Edges(\n        index_to_key=index['index_to_key'],\n        node_features=node_feature_matrix,\n        node_embeddings=embeddings\n    )\n    edge_index, edge_feature_matrix = edges.get_edge_features(dataset)\n\n    print('Constructing the temporal graph...', '\\n')\n    self.construct_graph(\n        current_index=index,\n        current_node_feature_matrix=node_feature_matrix,\n        current_embeddings=embeddings,\n        current_edge_index=edge_index,\n        current_edge_feature_matrix=edge_feature_matrix\n    )\n</code></pre>"},{"location":"doc/graph/temporal_graph/#semantics.graphs.temporal_graph.TemporalGraph.construct_graph","title":"<code>construct_graph(current_index, current_node_feature_matrix, current_embeddings, current_edge_index, current_edge_feature_matrix)</code>","text":"<p>This method is used to construct the temporal graph.</p> <p>Parameters:</p> Name Type Description Default <code>current_index</code> <code>dict</code> <p>the index of the nodes of the current snapshot.</p> required <code>current_node_feature_matrix</code> <code>ndarray</code> <p>the features of the nodes of the current snapshot.</p> required <code>current_embeddings</code> <code>ndarray</code> <p>the embeddings of the nodes of the current snapshot.</p> required <code>current_edge_index</code> <code>ndarray</code> <p>the edge index of the current snapshot.</p> required <code>current_edge_feature_matrix</code> <code>ndarray</code> <p>the edge features of the current snapshot.</p> required Source code in <code>semantics/graphs/temporal_graph.py</code> <pre><code>def construct_graph(\n        self, \n        current_index, \n        current_node_feature_matrix, \n        current_embeddings, \n        current_edge_index, \n        current_edge_feature_matrix\n        ):\n\n    \"\"\"\n    This method is used to construct the temporal graph.\n\n    Args:\n        current_index (dict): the index of the nodes of the current snapshot.\n        current_node_feature_matrix (np.ndarray): the features of the nodes of the current snapshot.\n        current_embeddings (np.ndarray): the embeddings of the nodes of the current snapshot.\n        current_edge_index (np.ndarray): the edge index of the current snapshot.\n        current_edge_feature_matrix (np.ndarray): the edge features of the current snapshot.\n    \"\"\"\n\n    if len(self.snapshots) == 0:\n        print('Adding the first snapshot to the temporal graph...', '\\n')\n        self.snapshots.append(current_index)\n        self.xs.append(np.concatenate((current_node_feature_matrix, current_embeddings), axis=1))\n        self.edge_indices.append(current_edge_index)\n        self.edge_features.append(current_edge_feature_matrix)\n        self.ys.append([])\n        self.y_indices.append([])\n\n    else:\n        print(f'Adding the {len(self.snapshots)} snapshot to the temporal graph...', '\\n')\n        previous_index, previous_node_features, previous_edge_index, previous_edge_feature, _, _= self[-1]\n        current_node_features = np.concatenate((current_node_feature_matrix, current_embeddings), axis=1)\n\n        previous_graph = {\n            'index': previous_index,\n            'node_features': previous_node_features,\n            'edge_index': previous_edge_index,\n            'edge_features': previous_edge_feature\n        }\n\n        current_graph = {\n            'index': current_index,\n            'node_features': current_node_features,\n            'edge_index': current_edge_index,\n            'edge_features': current_edge_feature_matrix\n        }\n\n        print('Aligning the nodes of the current snapshot with the nodes of the previous snapshot...', '\\n')\n        aligned_previous_graph, aligned_current_graph = self.get_aligned_graph(current_graph, previous_graph)\n        print('Labeling the edges of the previous snapshot with the edge feature values in the current snapshot...', '\\n')\n        previous_labels, previous_label_mask = self.label_previous_graph(current_graph, previous_graph)\n\n        self.snapshots[-1] = aligned_previous_graph['index']\n        self.xs[-1] = aligned_previous_graph['node_features']\n        self.edge_indices[-1] = aligned_previous_graph['edge_index']\n        self.edge_features[-1] = aligned_previous_graph['edge_features']\n        self.ys[-1] = previous_labels\n        self.y_indices[-1] = previous_label_mask\n\n        self.snapshots.append(aligned_current_graph['index'])\n        self.xs.append(aligned_current_graph['node_features'])\n        self.edge_indices.append(aligned_current_graph['edge_index'])\n        self.edge_features.append(aligned_current_graph['edge_features'])\n        self.ys.append([])\n        self.y_indices.append([])\n</code></pre>"},{"location":"doc/graph/temporal_graph/#semantics.graphs.temporal_graph.TemporalGraph.get_aligned_graph","title":"<code>get_aligned_graph(current_graph, previous_graph)</code>","text":"<p>This method is used to align the nodes of the current snapshot with the nodes of the previous snapshot.</p> <p>Parameters:</p> Name Type Description Default <code>current_graph</code> <code>dict</code> <p>the current snapshot of the temporal graph to align with the previous snapshot.</p> required <code>previous_graph</code> <code>dict</code> <p>the previous snapshot of the temporal graph to align with the current snapshot.</p> required <p>Returns:</p> Name Type Description <code>aligned_previous_graph</code> <code>dict</code> <p>the aligned previous snapshot of the temporal graph.</p> <code>aligned_current_graph</code> <code>dict</code> <p>the aligned current snapshot of the temporal graph.</p> Source code in <code>semantics/graphs/temporal_graph.py</code> <pre><code>def get_aligned_graph(\n        self, \n        current_graph: dict, \n        previous_graph: dict\n        ) -&gt; (dict, dict):\n\n    \"\"\"\n    This method is used to align the nodes of the current snapshot with the nodes of the previous snapshot.\n\n    Args:\n        current_graph (dict): the current snapshot of the temporal graph to align with the previous snapshot.\n        previous_graph (dict): the previous snapshot of the temporal graph to align with the current snapshot.\n\n    Returns:\n        aligned_previous_graph (dict): the aligned previous snapshot of the temporal graph.\n        aligned_current_graph (dict): the aligned current snapshot of the temporal graph.\n    \"\"\"\n\n    current_index = current_graph['index']\n    previous_index = previous_graph['index']\n\n    if current_index == previous_index:\n        return current_graph\n\n    current_words = set(current_index['key_to_index'].keys())\n    previous_words = set(previous_index['key_to_index'].keys())\n\n    dynamic_graph = current_words != previous_words\n\n    if not dynamic_graph:\n        index_mapping = {current_index['key_to_index'][key]: previous_index['key_to_index'][key] for key in current_index['key_to_index']}\n\n        reordered_node_feature_matrix = np.zeros_like(current_graph['node_features'])\n        for current_idx, previous_idx in index_mapping.items():\n            reordered_node_feature_matrix[previous_idx] = current_graph['node_features'][current_idx]\n\n\n        updated_edge_index = np.zeros_like(current_graph['edge_index'])\n        for i in range(current_graph['edge_index'].shape[1]):\n            updated_edge_index[0, i] = index_mapping.get(current_graph['edge_index'][0, i], -1)\n            updated_edge_index[1, i] = index_mapping.get(current_graph['edge_index'][1, i], -1)\n        # Remove edges where one of the nodes does not exist anymore (indicated by -1)\n        updated_edge_index = updated_edge_index[:, ~(updated_edge_index == -1).any(axis=0)]\n\n        aligned_current_graph = {\n            'index': previous_graph['index'],\n            'node_features': reordered_node_feature_matrix,\n            'edge_index': updated_edge_index,\n            'edge_features': current_graph['edge_features']\n        }\n        return previous_graph, aligned_current_graph\n\n\n    else:\n        all_words = current_words | previous_words\n        unified_dict = {word: idx for idx, word in enumerate(all_words)}\n        unified_dict_reverse = {idx: word for idx, word in enumerate(all_words)}\n        reordered_index = {'index_to_key': unified_dict_reverse, 'key_to_index': unified_dict}\n\n        reordered_previous_node_feature_matrix = np.zeros((len(unified_dict), previous_graph['node_features'].shape[1]))\n        for word, index in previous_index['key_to_index'].items():\n            if word in unified_dict:\n                reordered_previous_node_feature_matrix[unified_dict[word]] = previous_graph['node_features'][index]\n\n\n        reordered_current_node_feature_matrix = np.zeros((len(unified_dict), current_graph['node_features'].shape[1]))\n        for word, index in current_index['key_to_index'].items():\n            if word in unified_dict:\n                reordered_current_node_feature_matrix[unified_dict[word]] = current_graph['node_features'][index]\n\n\n        # Mapping old indices to new indices for the previous dictionary\n        previous_index_mapping = {old_index: unified_dict[word] for word, old_index in previous_index['key_to_index'].items()}\n        updated_previous_edge_index = np.array(previous_graph['edge_index'])\n        for i in range(previous_graph['edge_index'].shape[1]):\n            updated_previous_edge_index[0, i] = previous_index_mapping.get(previous_graph['edge_index'][0, i], -1)\n            updated_previous_edge_index[1, i] = previous_index_mapping.get(previous_graph['edge_index'][1, i], -1)\n        # Remove edges where one of the nodes does not exist anymore (indicated by -1)\n        updated_previous_edge_index = updated_previous_edge_index[:, ~(updated_previous_edge_index == -1).any(axis=0)]\n\n        # Mapping old indices to new indices for the current dictionary\n        current_index_mapping = {old_index: unified_dict[word] for word, old_index in current_index['key_to_index'].items()}\n        updated_current_edge_index = np.array(current_graph['edge_index'])\n        for i in range(current_graph['edge_index'].shape[1]):\n            updated_current_edge_index[0, i] = current_index_mapping.get(current_graph['edge_index'][0, i], -1)\n            updated_current_edge_index[1, i] = current_index_mapping.get(current_graph['edge_index'][1, i], -1)\n        # Remove edges where one of the nodes does not exist anymore (indicated by -1)\n        updated_current_edge_index = updated_current_edge_index[:, ~(updated_current_edge_index == -1).any(axis=0)]\n\n        aligned_previous_graph = {\n            'index': reordered_index,\n            'node_features': reordered_previous_node_feature_matrix,\n            'edge_index': updated_previous_edge_index,\n            'edge_features': previous_graph['edge_features']\n        }\n\n        aligned_current_graph = {\n            'index': reordered_index,\n            'node_features': reordered_current_node_feature_matrix,\n            'edge_index': updated_current_edge_index,\n            'edge_features': current_graph['edge_features']\n        }\n\n        return aligned_previous_graph, aligned_current_graph\n</code></pre>"},{"location":"doc/graph/temporal_graph/#semantics.graphs.temporal_graph.TemporalGraph.label_previous_graph","title":"<code>label_previous_graph(current_graph, previous_graph, label_feature_idx=1)</code>","text":"<p>This method is used to label the edges of the previous snapshot with the edge feature values in the current snapshot.</p> <p>Parameters:</p> Name Type Description Default <code>current_graph</code> <code>dict</code> <p>the current snapshot of the temporal graph to use for labeling the previous snapshot.</p> required <code>previous_graph</code> <code>dict</code> <p>the previous snapshot of the temporal graph to label.</p> required <code>label_feature_idx</code> <code>int</code> <p>the index of the feature to use as labels. Default: 1.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>labels</code> <code>ndarray</code> <p>the labels of the edges of the graph at the specified index.</p> <code>labels_mask</code> <code>ndarray</code> <p>the indices of the labels of the edges of the graph at the specified index.</p> Source code in <code>semantics/graphs/temporal_graph.py</code> <pre><code>def label_previous_graph(\n        self,\n        current_graph: dict,\n        previous_graph: dict,\n        label_feature_idx: int = 1\n        ) -&gt; (np.ndarray, np.ndarray):\n    \"\"\"\n    This method is used to label the edges of the previous snapshot with the edge feature values in the current snapshot.\n\n    Args:\n        current_graph (dict): the current snapshot of the temporal graph to use for labeling the previous snapshot.\n        previous_graph (dict): the previous snapshot of the temporal graph to label.\n        label_feature_idx (int): the index of the feature to use as labels. Default: 1.\n\n    Returns:\n        labels (np.ndarray): the labels of the edges of the graph at the specified index.\n        labels_mask (np.ndarray): the indices of the labels of the edges of the graph at the specified index.\n    \"\"\"\n\n    current_edge_index = current_graph['edge_index']\n    current_edge_features = current_graph['edge_features']\n\n    previous_edge_index = previous_graph['edge_index']\n\n    previous_edges = [tuple(edge) for edge in previous_edge_index.T]\n    current_edges  = [tuple(edge) for edge in current_edge_index.T]\n\n    labels = []\n\n    label_mask_1 = []\n    label_mask_2 = []\n\n    for i, previous_edge in enumerate(previous_edges):\n        if previous_edge in current_edges:\n            label_mask_1.append(previous_edge[0])\n            label_mask_2.append(previous_edge[1])\n\n            current_index = current_edges.index(previous_edge)\n            labels.append(current_edge_features[current_index][label_feature_idx])\n\n    label_mask = np.stack([label_mask_1, label_mask_2])\n    labels = np.array(labels)\n\n    return labels, label_mask\n</code></pre>"}]}